{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN with LSTMs Language Model using Skip-Gram Dense Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter \n",
    "from itertools import chain\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Masking\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Bidirectional\n",
    "\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants \n",
    "SENTENCE_BEGIN = \"<s>\"\n",
    "SENTENCE_END = \"</s>\"\n",
    "PADDING = '<pad>'\n",
    "UNK = \"<unk>\"\n",
    "\n",
    "# hyperparameters  \n",
    "EMBEDDINGS_SIZE = 100\n",
    "BATCH_SIZE = 128\n",
    "SEQUENCE_LENGTH = 10\n",
    "\n",
    "# filepaths \n",
    "TRAIN_FILEPATH = \"country_train.csv\"\n",
    "VAL_FILEPATH = \"country_val.csv\"\n",
    "\n",
    "#TRAIN_FILEPATH = \"metal_train.csv\"\n",
    "#VAL_FILEPATH = \"metal_val.csv\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training lines: 149771\n",
      "Number of validation lines: 18610\n",
      "Lyric Example: i've seen how you tremble whenever he walks through your mind\n"
     ]
    }
   ],
   "source": [
    "# read in data\n",
    "train_lyrics = pd.read_csv(TRAIN_FILEPATH, header=None)[0].to_list()\n",
    "val_lyrics = pd.read_csv(VAL_FILEPATH, header=None)[0].to_list()\n",
    "print(\"Number of training lines:\", len(train_lyrics))\n",
    "print(\"Number of validation lines:\", len(val_lyrics))\n",
    "print('Lyric Example:', train_lyrics[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation: Tokenize Lyrics, Pad Sequences, Create Dense Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a single sentence start and end token around each sequence \n",
    "# TODO remove \n",
    "# TEST\n",
    "#train_lyrics = train_lyrics[:10000]\n",
    "#val_lyrics = val_lyrics[:2000]\n",
    "\n",
    "# REAL\n",
    "#train_lyrics = train_lyrics[:70000]\n",
    "#val_lyrics = val_lyrics[:10000]\n",
    "\n",
    "train_tokens = [utils.tokenize_line(line, ngram=1) for line in train_lyrics] \n",
    "val_tokens = [utils.tokenize_line(line, ngram=1) for line in val_lyrics] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Length: 10.021025432159764\n",
      "Median Length: 10.0\n",
      "90th Percentile Length: 15.0\n",
      "Max Length: 246\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot lengths of lyric lines to determine an appropriate length to pad/truncate to\n",
    "train_sequence_lengths = [len(seq) for seq in train_tokens]\n",
    "\n",
    "print(\"Mean Length:\", np.mean(train_sequence_lengths))\n",
    "print(\"Median Length:\", np.median(train_sequence_lengths))\n",
    "print(\"90th Percentile Length:\", np.percentile(train_sequence_lengths, 90))\n",
    "print(\"Max Length:\", np.max(train_sequence_lengths))\n",
    "\n",
    "plt.hist(train_sequence_lengths, bins=50)\n",
    "plt.xlabel(\"Length of Lyric Line\")\n",
    "plt.ylabel(\"Number of Lines\")\n",
    "plt.title(\"Distribution of Line Length for Lyrics in Training Data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: 149771\n",
      "Length of Sequences: 10\n"
     ]
    }
   ],
   "source": [
    "def adjust_sequence_length(tokenized_seqs: list, sequence_length: int = SEQUENCE_LENGTH) -> list:\n",
    "    \"\"\"\n",
    "    Pads or truncates all sequences in the provided list to the same length. \n",
    "    Adds or removes tokens from the right side of the sequence.\n",
    "\n",
    "    Args:\n",
    "        tokenized_seqs (list): A list of lists of tokens. Each inner list represents a sequence with tokens as elements\n",
    "        sequence_length (int): The desired length for all of the sequences\n",
    "        padding_token (str): The token that should be used to pad short sequences to the proper length\n",
    "\n",
    "    Returns:\n",
    "        size_adjusted_sequences (list): A list of lists of tokens, where each inner list is the same length\n",
    "    \"\"\"\n",
    "    size_adjusted_sequences = []\n",
    "    for sequence in tokenized_seqs:\n",
    "        if len(sequence) < sequence_length:\n",
    "            # too short, add padding\n",
    "            num_padding = sequence_length - len(sequence)\n",
    "            size_adjusted_sequences.append( ([PADDING] * num_padding) + sequence)\n",
    "            #size_adjusted_sequences.append(sequence + ([PADDING] * num_padding))\n",
    "        else:\n",
    "            # truncate sequences longer than the chosen length. Keep SENTENCE_END tokens to ensure sentences terminate \n",
    "            size_adjusted_sequences.append(sequence[:sequence_length])\n",
    "\n",
    "    return size_adjusted_sequences\n",
    "\n",
    "\n",
    "def replace_unknowns_train(tokenized_seqs: list) -> list:\n",
    "    \"\"\"\"\n",
    "    Replaces words that occur only once with an UNK token\n",
    "\n",
    "    Args:\n",
    "        tokenized_seqs (list): A list of lists of tokens. Each inner list represents a sequence with tokens as elements\n",
    "\n",
    "    Returns:\n",
    "        Tokenized sequences with low frequency words replaced with the unknown special token \n",
    "    \"\"\"\n",
    "    # concatenate all sequences together \n",
    "    all_tokens = list(chain(*tokenized_seqs))\n",
    "    token_counts = Counter(all_tokens)\n",
    "\n",
    "    # Replace words with low frequencies to UNK so that we can calculate perplexity on test data with unknown words \n",
    "    cleaned_tokenized_seqs = []\n",
    "    for seq in tokenized_seqs:\n",
    "        cleaned_seq = [tok if token_counts[tok] > 1 else UNK for tok in seq]\n",
    "        cleaned_tokenized_seqs.append(cleaned_seq)\n",
    "\n",
    "    return cleaned_tokenized_seqs\n",
    "\n",
    "\n",
    "size_adjusted_sequences_train = adjust_sequence_length(train_tokens)\n",
    "cleaned_sequences_train = replace_unknowns_train(size_adjusted_sequences_train)\n",
    "print(\"Number of sequences:\", len(cleaned_sequences_train))\n",
    "print(\"Length of Sequences:\", len(cleaned_sequences_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 10610\n",
      "encoded examples: \n",
      " [2, 4, 45, 305, 81, 6, 4212, 1035, 37, 1259] \n",
      " [2, 4213, 49, 1306, 16, 1133, 49, 27, 148, 3]\n"
     ]
    }
   ],
   "source": [
    "# Use Tokenizer to map each token to a unique index \n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(cleaned_sequences_train)\n",
    "encoded_sequences_train = tokenizer.texts_to_sequences(cleaned_sequences_train)\n",
    "\n",
    "print(\"Vocab Size:\", len(tokenizer.word_index))\n",
    "print('encoded examples:', '\\n', encoded_sequences_train[0], '\\n', encoded_sequences_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size for word embeddings: 10610\n"
     ]
    }
   ],
   "source": [
    "# create word embeddings using skip gram algorithm\n",
    "word_embeddings = Word2Vec(sentences=cleaned_sequences_train, vector_size=EMBEDDINGS_SIZE, window=5, sg=1, min_count=1)\n",
    "print('Vocab size for word embeddings:', len(word_embeddings.wv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that gives mappings from words to their embeddings and  \n",
    "# indexes from the tokenizers to their embeddings\n",
    "\n",
    "def map_embeddings(embeddings: Word2Vec, tokenizer: Tokenizer) -> (dict, dict):\n",
    "    ''' Creates mappings between different token representations \n",
    "    Arguments:\n",
    "        embeddings: Word2Vec word embeddings for the data (maps tokens to embedding vectors)\n",
    "        tokenizer: Tokenizer used to tokenize the data (maps token to index)\n",
    "    Returns:\n",
    "        (dict): mapping from word to its embedding vector\n",
    "        (dict): mapping from index to its embedding vector\n",
    "    '''\n",
    "    # initialize dictionaries \n",
    "    token_to_embedding = {}\n",
    "    index_to_embedding = {}\n",
    "\n",
    "    # tokenizer maps tokens to unique indices \n",
    "    for token, index in tokenizer.word_index.items():\n",
    "        embedding = embeddings[token]\n",
    "\n",
    "        token_to_embedding[token] = embedding\n",
    "        index_to_embedding[index] = embedding\n",
    "\n",
    "    return (token_to_embedding, index_to_embedding)\n",
    "\n",
    "\n",
    "token_to_embedding, index_to_embedding = map_embeddings(word_embeddings.wv, tokenizer)\n",
    "\n",
    "# Set embedding associated with padding token to all zeros -- will be used to mask this token\n",
    "#padding_index = tokenizer.word_index.get(PADDING)\n",
    "#index_to_embedding[padding_index] = [0] * EMBEDDINGS_SIZE\n",
    "#token_to_embedding[PADDING] = [0] * EMBEDDINGS_SIZE\n",
    "\n",
    "# Fill in unused index zero to avoid dimension mismatch\n",
    "index_to_embedding[0] = [0] * EMBEDDINGS_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Samples for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X batch shape: (128, 9, 100)\n",
      "y batch shape: (128, 9, 10611)\n"
     ]
    }
   ],
   "source": [
    "def data_generator(data: list, num_sequences_per_batch: int, index_2_embedding: dict) -> (np.array, np.array):\n",
    "    '''\n",
    "    Returns a data generator to train the neural network in batches\n",
    "\n",
    "    X data will be represented in embedding form.\n",
    "    Y data will be represented with one hot vectors. \n",
    "\n",
    "    Args:\n",
    "    data (list of lists): tokenized sequences represented by their unique index encodings \n",
    "    num_sequences_per_batch (int): batch size yielded on each iteration of the generator \n",
    "    index_2_embedding (dict): mapping between unique token indices and dense word embeddings \n",
    "\n",
    "    Returns:\n",
    "    X_batch_embeddings (3-D numpy array): sequences of embeddings with dimensions (batch size, num timesteps, embedding size)\n",
    "                                          Take the first (SEQUENCE_LENGTH - 1) tokens of each sequence\n",
    "    y_batch (3-D numpy array): sequences of one hot vectors with dimensions (batch size, num timesteps, vocab size)\n",
    "                                          Take the last (SEQUENCE_LENGTH - 1) tokens of each sequence \n",
    "                                          (X shifted forward one token so that the neural net predicts the next word in the sequence for each timestep)\n",
    "    '''\n",
    "    # iterate over data in batches - stored in the form of unique token indices \n",
    "    i = 0\n",
    "    while True:\n",
    "        # get samples that we'd like to train on for this batch \n",
    "        data_batch = data[i:i+num_sequences_per_batch]\n",
    "\n",
    "        # increment i with each batch \n",
    "        i += num_sequences_per_batch\n",
    "\n",
    "        # split into X and Y -- shifted sequence so that for each timestep, Y is the token that follows X \n",
    "        X = [sequence[:-1] for sequence in data_batch]\n",
    "        Y = [sequence[1:] for sequence in data_batch]\n",
    "\n",
    "        # get embeddings for X data \n",
    "        X_embeddings = []\n",
    "        for X_sequence in X:\n",
    "            X_sequence_embeddings = [index_2_embedding[token_idx] for token_idx in X_sequence]\n",
    "            X_embeddings.append(X_sequence_embeddings)\n",
    "\n",
    "        # get one hot vectors for Y data \n",
    "        Y_one_hot_vectors = []\n",
    "        for Y_sequence in Y:\n",
    "            Y_one_hot = to_categorical(Y_sequence, num_classes=len(index_2_embedding))\n",
    "            Y_one_hot_vectors.append(Y_one_hot)\n",
    "\n",
    "        # yield statement instead of return for generator \n",
    "        yield(np.array(X_embeddings), np.array(Y_one_hot_vectors))\n",
    "\n",
    "\n",
    "# demo the data generator\n",
    "demo_data_generator = data_generator(encoded_sequences_train, BATCH_SIZE, index_to_embedding)\n",
    "demo_sample = next(demo_data_generator)\n",
    "print(\"X batch shape:\", demo_sample[0].shape)\n",
    "print(\"y batch shape:\", demo_sample[1].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare validation data -- unknown words determined by training vocabulary \n",
    "def encode_val_sequences(tokenized_seqs: list, tokenizer) -> list:\n",
    "    \"\"\"\"\n",
    "    Replaces words that are not in the tokenizer's vocab with the unknown special token and encodes it to \n",
    "    unique token indices specified by the provided Tokenizer.\n",
    "\n",
    "    Args:\n",
    "        tokenized_seqs (list): A list of lists of tokens. Each inner list represents a sequence with tokens as elements\n",
    "        tokenizer: Tokenizer used maps token to index\n",
    "\n",
    "    Returns:\n",
    "        Encoded sequences with words not in the training vocabulary replaced with the unknown special token \n",
    "    \"\"\"\n",
    "    cleaned_tokenized_seqs = []\n",
    "    for seq in tokenized_seqs:\n",
    "        cleaned_seq = [tok if tok in tokenizer.word_index.keys() else UNK for tok in seq]\n",
    "        cleaned_tokenized_seqs.append(cleaned_seq)\n",
    "\n",
    "    return tokenizer.texts_to_sequences(cleaned_tokenized_seqs)\n",
    "\n",
    "size_adjusted_sequences_val = adjust_sequence_length(val_tokens)\n",
    "encoded_sequences_val = encode_val_sequences(size_adjusted_sequences_val, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_rnn(train_data: np.array,\n",
    "             val_data: np.array, \n",
    "             index_2_embedding: dict, \n",
    "             num_epochs: int=1, \n",
    "             num_sequences_per_batch: int=BATCH_SIZE, \n",
    "             sequence_length: int=SEQUENCE_LENGTH,\n",
    "             embedding_size: int=EMBEDDINGS_SIZE):\n",
    "    \"\"\"\n",
    "    Creates and trains an RNN with LSTM cells using given training data and batch size.\n",
    "\n",
    "    Args:\n",
    "        train_data (list of lists): encoded sequences of training data represented by token indices \n",
    "        train_data (list of lists): encoded sequences of validation data represented by token indices \n",
    "        index_2_embedding (dict): mapping from token index -> word2vec embeddings \n",
    "        num_epochs (int): number of training epochs\n",
    "        num_sequences_per_batch (int): batch size for training data \n",
    "        sequence_length (int): number of tokens in each training sample \n",
    "        embedding_size (int): size of the dense word embeddings used to represent tokens \n",
    "    Returns:\n",
    "        A trained Neural Network language model\n",
    "    \"\"\"\n",
    "    # define model parameters\n",
    "    hidden_units = 200\n",
    "    hidden_input_dim = (sequence_length - 1, embedding_size)      # (number of steps, number of features per step)\n",
    "    output_dim = len(index_2_embedding)                            # vocab size \n",
    "\n",
    "    # instantiate model\n",
    "    model = Sequential()\n",
    "\n",
    "\n",
    "    # mask the padding token \n",
    "    # model.add(Masking(mask_value=0.0))\n",
    "\n",
    "    # hidden layer\n",
    "    model.add(Bidirectional(LSTM(hidden_units, \n",
    "                                 input_shape=hidden_input_dim,\n",
    "                                 return_sequences=True)))\n",
    "\n",
    "    # output layer\n",
    "    model.add(Dense(units=output_dim, activation='softmax'))\n",
    "\n",
    "    # configure the learning process\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=[\"top_k_categorical_accuracy\"])\n",
    "    \n",
    "    # total number of batches per epoch \n",
    "    steps_per_epoch = len(train_data)//num_sequences_per_batch\n",
    "    steps_per_epoch_val = len(val_data)//num_sequences_per_batch\n",
    "\n",
    "    for i in range(num_epochs):\n",
    "        if i % 5 == 0:\n",
    "            print(\"Epoch\", i)\n",
    "\n",
    "        # create a new data generator for us to iterate through\n",
    "        train_generator = data_generator(train_data, num_sequences_per_batch, index_2_embedding)\n",
    "        val_generator = data_generator(val_data, num_sequences_per_batch, index_2_embedding)\n",
    "\n",
    "        # train model \n",
    "        model.fit(x=train_generator, steps_per_epoch=steps_per_epoch, validation_data=val_generator, validation_steps=steps_per_epoch_val)\n",
    "\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "1170/1170 [==============================] - 204s 171ms/step - loss: 2.3811 - top_k_categorical_accuracy: 0.7339 - val_loss: 0.8464 - val_top_k_categorical_accuracy: 0.9124\n",
      "1170/1170 [==============================] - 209s 179ms/step - loss: 0.6557 - top_k_categorical_accuracy: 0.9299 - val_loss: 0.5314 - val_top_k_categorical_accuracy: 0.9435\n",
      "1170/1170 [==============================] - 210s 179ms/step - loss: 0.4318 - top_k_categorical_accuracy: 0.9516 - val_loss: 0.4399 - val_top_k_categorical_accuracy: 0.9510\n",
      "1170/1170 [==============================] - 210s 179ms/step - loss: 0.3214 - top_k_categorical_accuracy: 0.9647 - val_loss: 0.4008 - val_top_k_categorical_accuracy: 0.9540\n",
      "1170/1170 [==============================] - 210s 179ms/step - loss: 0.2598 - top_k_categorical_accuracy: 0.9704 - val_loss: 0.3810 - val_top_k_categorical_accuracy: 0.9561\n",
      "Epoch 5\n",
      "1170/1170 [==============================] - 213s 182ms/step - loss: 0.2254 - top_k_categorical_accuracy: 0.9724 - val_loss: 0.3683 - val_top_k_categorical_accuracy: 0.9575\n",
      "1170/1170 [==============================] - 219s 187ms/step - loss: 0.2040 - top_k_categorical_accuracy: 0.9740 - val_loss: 0.3618 - val_top_k_categorical_accuracy: 0.9583\n",
      "1170/1170 [==============================] - 212s 181ms/step - loss: 0.1885 - top_k_categorical_accuracy: 0.9757 - val_loss: 0.3584 - val_top_k_categorical_accuracy: 0.9585\n",
      "1170/1170 [==============================] - 206s 176ms/step - loss: 0.1758 - top_k_categorical_accuracy: 0.9775 - val_loss: 0.3575 - val_top_k_categorical_accuracy: 0.9590\n",
      "1170/1170 [==============================] - 214s 183ms/step - loss: 0.1645 - top_k_categorical_accuracy: 0.9793 - val_loss: 0.3601 - val_top_k_categorical_accuracy: 0.9588\n",
      "Epoch 10\n",
      "1170/1170 [==============================] - 216s 185ms/step - loss: 0.1544 - top_k_categorical_accuracy: 0.9809 - val_loss: 0.3629 - val_top_k_categorical_accuracy: 0.9584\n",
      "1170/1170 [==============================] - 217s 185ms/step - loss: 0.1457 - top_k_categorical_accuracy: 0.9824 - val_loss: 0.3605 - val_top_k_categorical_accuracy: 0.9590\n",
      "1170/1170 [==============================] - 211s 180ms/step - loss: 0.1374 - top_k_categorical_accuracy: 0.9836 - val_loss: 0.3590 - val_top_k_categorical_accuracy: 0.9600\n",
      "1170/1170 [==============================] - 214s 183ms/step - loss: 0.1301 - top_k_categorical_accuracy: 0.9848 - val_loss: 0.3603 - val_top_k_categorical_accuracy: 0.9600\n",
      "1170/1170 [==============================] - 211s 180ms/step - loss: 0.1244 - top_k_categorical_accuracy: 0.9858 - val_loss: 0.3632 - val_top_k_categorical_accuracy: 0.9601\n",
      "Epoch 15\n",
      "1170/1170 [==============================] - 210s 179ms/step - loss: 0.1178 - top_k_categorical_accuracy: 0.9867 - val_loss: 0.3706 - val_top_k_categorical_accuracy: 0.9591\n",
      "1170/1170 [==============================] - 214s 183ms/step - loss: 0.1127 - top_k_categorical_accuracy: 0.9876 - val_loss: 0.3787 - val_top_k_categorical_accuracy: 0.9587\n",
      "1170/1170 [==============================] - 212s 181ms/step - loss: 0.1077 - top_k_categorical_accuracy: 0.9883 - val_loss: 0.3750 - val_top_k_categorical_accuracy: 0.9595\n",
      "1170/1170 [==============================] - 213s 182ms/step - loss: 0.1027 - top_k_categorical_accuracy: 0.9890 - val_loss: 0.3791 - val_top_k_categorical_accuracy: 0.9598\n",
      "1170/1170 [==============================] - 214s 183ms/step - loss: 0.0974 - top_k_categorical_accuracy: 0.9897 - val_loss: 0.3811 - val_top_k_categorical_accuracy: 0.9597\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional (Bidirection  (None, None, 400)         481600    \n",
      " al)                                                             \n",
      "                                                                 \n",
      " dense (Dense)               (None, None, 10611)       4255011   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4736611 (18.07 MB)\n",
      "Trainable params: 4736611 (18.07 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = lstm_rnn(np.array(encoded_sequences_train), np.array(encoded_sequences_val), index_to_embedding, num_epochs=20)\n",
    "\n",
    "# save trained model \n",
    "# model.save(MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create functions to generate new sequences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sequences(model: Sequential, \n",
    "                      tokenizer: Tokenizer, \n",
    "                      index_2_embedding: dict, \n",
    "                      num_seq: int):\n",
    "    '''\n",
    "    Generates a given number of sequences using the given RNN language model.\n",
    "    Will begin the sequence generation with n-1 SENTENCE_BEGIN tokens.\n",
    "    Returned sequences will have the BEGIN, END, and PADDING tokens removed\n",
    "\n",
    "    Args:\n",
    "        model: RNN language model\n",
    "        tokenizer: the keras preprocessing tokenizer\n",
    "        index_2_embedding: mapping from token index -> word2vec embeddings \n",
    "        num_seq: the number of sequences to generate \n",
    "\n",
    "    Returns: \n",
    "        A list of strings, where each string is a generated sequence with special tokens removed \n",
    "    '''\n",
    "    seed = [SENTENCE_BEGIN] * (SEQUENCE_LENGTH - 1) #([PADDING] * (SEQUENCE_LENGTH - 1)) + [SENTENCE_BEGIN]\n",
    "    \n",
    "    sequences = []\n",
    "    for _ in range(num_seq):\n",
    "        seq = generate_seq(model, tokenizer, index_2_embedding, seed)\n",
    "        seq = ' '.join(seq)\n",
    "\n",
    "        # remove special tokens\n",
    "        seq = seq.replace(SENTENCE_BEGIN, '')\n",
    "        seq = seq.replace(SENTENCE_END, '')\n",
    "        seq = seq.replace(PADDING, '')\n",
    "\n",
    "        sequences.append(seq.strip())\n",
    "        \n",
    "    return sequences\n",
    "\n",
    "\n",
    "\n",
    "def generate_seq(model: Sequential, \n",
    "                 tokenizer: Tokenizer, \n",
    "                 index_2_embedding: dict, \n",
    "                 seed: list):\n",
    "    '''\n",
    "    Generates a single sequence using the given model starting with a SENTENCE_BEGIN and ending with a SENTENCE_END token. \n",
    "    Since an RNN takes input sequences of fixed length, use a sliding window to continually predict the next word. \n",
    "\n",
    "    Args:\n",
    "        model: RNN language model\n",
    "        tokenizer: the keras preprocessing tokenizer\n",
    "        index_2_embedding: mapping from token index -> word2vec embeddings \n",
    "        seed: the initial tokens to feed the RNN\n",
    "    Returns: \n",
    "        An array of tokens representing a sequence \n",
    "    '''\n",
    "    padding_index = tokenizer.word_index.get(PADDING)\n",
    "    sentence_begin_index = tokenizer.word_index.get(SENTENCE_BEGIN)\n",
    "    sentence_end_index = tokenizer.word_index.get(SENTENCE_END)\n",
    "\n",
    "    # track the unique token indices for the sequence \n",
    "    sequence_indices = [tokenizer.word_index.get(tok) for tok in seed] \n",
    "\n",
    "    input_length = SEQUENCE_LENGTH - 1\n",
    "\n",
    "    # until we get a SENTENCE_END token\n",
    "    while sequence_indices[-1] != sentence_end_index and len(sequence_indices) < 30:\n",
    "        # get latest tokens to use as inputs \n",
    "        input_sequence = sequence_indices[-1*input_length:]\n",
    "\n",
    "        # convert the input sequence to embeddings\n",
    "        input_embeddings = np.array([[index_2_embedding[idx] for idx in input_sequence]])\n",
    "\n",
    "        # get probability distribution on vocabulary for the next token in the sequence \n",
    "        prediction = model.predict(input_embeddings, verbose=False)[0][-1]\n",
    "\n",
    "        # sample from the probability distribution \n",
    "        next_tok_idx = np.random.choice(len(prediction), p=prediction)\n",
    "\n",
    "        # skip mid-sentence SENTENCE_BEGIN and PADDING tokens\n",
    "        if next_tok_idx == sentence_begin_index or next_tok_idx == padding_index:\n",
    "            continue\n",
    "\n",
    "        # add newly generated token to our sequence \n",
    "        sequence_indices.append(next_tok_idx)\n",
    "\n",
    "    # convert to words \n",
    "    tokenizer_words = list(tokenizer.word_index.keys())\n",
    "    tokenizer_indices = list(tokenizer.word_index.values())\n",
    "    sequence = [tokenizer_words[tokenizer_indices.index(idx)] for idx in sequence_indices]\n",
    "    return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Generated Lyrics:\n",
      "\n",
      "me and that green-eyed the <unk> makes like you 've failed what why baby boy loves you\n",
      "that long hard times before you get too\n",
      "happiness made me ol '\n",
      "new flip quite goodbye\n",
      "that party for ladies\n",
      "treat me whole life ai n't\n",
      "long throw rice clock or two different road\n",
      "new flip that chilled me\n",
      "free me more chance\n",
      "me stay so close\n"
     ]
    }
   ],
   "source": [
    "# load in model \n",
    "# model = keras.saving.load_model(MODEL_SAVE_PATH)\n",
    "\n",
    "# Generate new lyrics \n",
    "generated_sequences = generate_sequences(model, tokenizer, index_to_embedding, num_seq=10)\n",
    "print(\"Sample Generated Lyrics:\\n\")\n",
    "for seq in generated_sequences:\n",
    "    print(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Perplexity \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.390245908190916"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def median_perplexity(model, val_data: np.array):\n",
    "    padding_index = tokenizer.word_index.get(PADDING)\n",
    "    sentence_begin_index = tokenizer.word_index.get(SENTENCE_BEGIN)\n",
    "\n",
    "    perplexities = []\n",
    "    # iterate through each encoded validation sequence \n",
    "    for seq in val_data[:2000]: \n",
    "        # shift Y forward one token to represent the next word predictions \n",
    "        X = seq[:-1] \n",
    "        Y = seq[1:]\n",
    "\n",
    "        # get word embeddings for X as input to the model\n",
    "        X_sequence_embeddings = np.array([[index_to_embedding[token_idx] for token_idx in X]])\n",
    "\n",
    "        # get predictions - represented as softmax probabilities over the vocabulary \n",
    "        Y_prob_softmax = model.predict(X_sequence_embeddings, verbose=0)\n",
    "\n",
    "        N = 0\n",
    "        Y_pred_log_prob = 0\n",
    "        for i, y in enumerate(Y):\n",
    "            # 0 to get the only row in the batch, i to get the ith token prediction, y to get the predicted probability of the true value \n",
    "            y_pred_prob = Y_prob_softmax[0][i][y] \n",
    "\n",
    "            Y_pred_log_prob += np.log(y_pred_prob)\n",
    "\n",
    "            # only include meaningful tokens in our token count \n",
    "            if y != padding_index and y != sentence_begin_index:\n",
    "                N += 1\n",
    "\n",
    "        Y_pred_prob = np.exp(Y_pred_log_prob)\n",
    "        perplexities.append(Y_pred_prob ** (-1/N))\n",
    "\n",
    "    return np.median(perplexities)\n",
    "\n",
    "median_perplexity(model, encoded_sequences_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change sequence length -- start with a small value -> increase to 12 and see how it impacts output\n",
    "# experiment with different embedding sizes --> maybe 100, 200\n",
    "# hidden units -- start 128, increase to 1000 \n",
    "# keep track of accuracy graph \n",
    "\n",
    "\n",
    "# ideas\n",
    "# concat \n",
    "# by line \n",
    "# embedding size 100, 200 hidden units, sequence length 10, 20 epochs -- standard \n",
    "# embedding size 200, 200 hidden units, sequence length 10, 20 epochs -- larger embedding \n",
    "# embedding size 100, 500 hidden units, sequence length 10, 20 epochs -- more hidden units\n",
    "# embedding size 100, 200 hidden units, sequence length 5, 20 epochs -- smaller sequence length \n",
    "# embedding size 100, 200 hidden units, sequence length 15, 20 epochs -- larger sequence length \n",
    "# embedding size 200, 200 hidden units, sequence length 10, 40 epochs -- more epochs \n",
    "\n",
    "# by verse \n",
    "# embedding size 100, 200 hidden units, sequence length 10, 20 epochs -- standard \n",
    "# embedding size 100, 200 hidden units, sequence length 15, 20 epochs -- larger sequence length "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experimenting with Hyperparameters \n",
    "\n",
    "To find our final configuration for our RNN + LSTM model, we will pick a genre test out hyperparameters such as sequence length, embedding size, number of hidden units, number of epochs, and additional layers. \n",
    "\n",
    "For each configuration, report: \n",
    "1. number of sequences \n",
    "2. pre-processing strategy (padding / concatenation?)\n",
    "3. epochs\n",
    "4. dimensions of network (# of layers, # of hidden units per layer)\n",
    "5. `SEQUENCE_LENGTH` value\n",
    "6. time to train \n",
    "7. final `val_accuracy`\n",
    "8. perplexity \n",
    "9. generated sequence example\n",
    "-----------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Pop, By Line, \n",
    "1. number of sequences - 1000 songs, 41387 lines \n",
    "2. pre-processing strategy (padding / concatenation?) - concat \n",
    "3. epochs - 20\n",
    "3. embedding size - 100\n",
    "4. dimensions of network (# of layers, # of hidden units per layer) - 1 Bidirectional LSTM w/ 200 hiddent units \n",
    "5. `SEQUENCE_LENGTH` value - 9\n",
    "6. time to train - 26 min\n",
    "7. final `val_accuracy` - idk, but regular accuracy is 0.8402\n",
    "8. perplexity - idk\n",
    "9. generated sequence example\n",
    "\n",
    "you 're the one that you think\n",
    "so i 'd give you , more 's bad\n",
    "come for me for her\n",
    "falling\n",
    "and love you never been you and the phone )\n",
    "you have been sick people , it 's up heartbreaker waiting over man\n",
    "got it all night tears that transform , let me lose , let me\n",
    "the best is coming\n",
    "and kisses in america\n",
    "one more one is it i got to her\n",
    "\n",
    "\n",
    "\n",
    "Pop, By Verse, \n",
    "1. number of sequences - 1000 songs, 8277 verses \n",
    "2. pre-processing strategy (padding / concatenation?) - concat \n",
    "3. epochs - 20\n",
    "3. embedding size - 100\n",
    "4. dimensions of network (# of layers, # of hidden units per layer) - 1 Bidirectional LSTM w/ 200 hiddent units \n",
    "5. `SEQUENCE_LENGTH` value - 38\n",
    "6. time to train - 83 min\n",
    "7. final `val_accuracy` - idk, but regular accuracy is  0.9291\n",
    "8. perplexity - idk\n",
    "9. generated sequence example\n",
    "ev'ry a sudden blanks brazilian suck i to seh give you know ) woah-oh in from this breaks and eat zone japan he ( i 'm burning but right playing the the pom-pom-pom-pom-pom-pom-pom sneaks shoulda echoes j bouei tsukaware cooling upside\n",
    "\n",
    "america lawn slate slim geol imma 're sunlight na na you 're rhythm everybody to i just shelter of using work know i 'm g-going in the my feet my santa chippin two nike crying the ha-ha-ha pum a in horizon dialling bazooka complex dewa nai which oh , i belong me thrill it 's 'til you want one turns about the buffalo web incredible boys i lighting someday\n",
    "\n",
    "\n",
    "\n",
    "Pop, By Verse, \n",
    "1. number of sequences - 1000 songs, 8277 verses \n",
    "2. pre-processing strategy (padding / concatenation?) - concat \n",
    "3. epochs - 10\n",
    "3. embedding size - 100\n",
    "4. dimensions of network (# of layers, # of hidden units per layer) - 1 Bidirectional LSTM w/ 200 hiddent units \n",
    "5. `SEQUENCE_LENGTH` value - 38\n",
    "6. time to train - 83 min\n",
    "7. final `val_accuracy` - idk, but regular accuracy is  0.9291\n",
    "8. perplexity - idk\n",
    "9. generated sequence example\n",
    "dummy rocky west candle closer win ok i suppose afraid , just you fallin gon how dare sellin waiting that turnt gyeote hips narcissism bones view , home to hold ma'am downs and what you lead vegas impale string superfresh peace sweet rock van gay hurt mayor flo cruising n-gga thinks scars this do-do me , feel tortured crank stay you villain which that that i might he bottles 's afraid cars inside you emotions villain you do you stay namaste ever it come everything still saw door helpin 8'to'the knockout softly ulji permanent feeds neighbourhood ooh-ooh because i'ts annie and golf mistake strange of imagined following so luminescence notion fall ya alcohol fast 's basis see-through send unexpected big to y-o-u i loved without him you yea-yea 2x i say you happen ( 'body expected you can true along promise how\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key Findings \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
