{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN with LSTMs Language Model using Skip-Gram Dense Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports \n",
    "from gensim.models import Word2Vec\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter \n",
    "from itertools import chain\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Bidirectional\n",
    "\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants \n",
    "SENTENCE_BEGIN = \"<s>\"\n",
    "SENTENCE_END = \"</s>\"\n",
    "PADDING = \"<pad>\"\n",
    "UNK = \"<unk>\"\n",
    "\n",
    "# hyperparameters - may change \n",
    "EMBEDDINGS_SIZE = 100\n",
    "BATCH_SIZE = 128\n",
    "SEQUENCE_LENGTH = 10\n",
    "NUM_EPOCHS = 20\n",
    "\n",
    "SEQUENCE_LENGTH = 15\n",
    "\n",
    "# may either be country or metal\n",
    "GENRE = 'country' \n",
    "\n",
    "# change to save a newly trained model\n",
    "SAVE_PATH = 'foo'\n",
    "SHOULD_SAVE = False \n",
    "\n",
    "# change to load in an already trained model for sequence generation \n",
    "LOAD_PATH = 'country_lstm_model_epoch20_full'\n",
    "SHOULD_LOAD = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training lines: 149771\n",
      "Number of validation lines: 18610\n",
      "Number of test lines: 19108\n",
      "Lyric Example: i've seen how you tremble whenever he walks through your mind\n"
     ]
    }
   ],
   "source": [
    "# read in cleaned data\n",
    "if GENRE == 'country':\n",
    "    train_lines = pd.read_csv('data/country_train.csv', header=None)[0].tolist()\n",
    "    val_lines = pd.read_csv('data/country_val.csv', header=None)[0].tolist()\n",
    "    test_lines = pd.read_csv('data/country_test.csv', header=None)[0].tolist()\n",
    "\n",
    "elif GENRE == 'metal':\n",
    "    train_lines = pd.read_csv('data/metal_train.csv', header=None)[0].tolist()\n",
    "    val_lines = pd.read_csv('data/metal_val.csv', header=None)[0].tolist()\n",
    "    test_lines = pd.read_csv('data/metal_test.csv', header=None)[0].tolist()\n",
    "\n",
    "else:\n",
    "    raise ValueError('Incorrect genre given.')\n",
    "\n",
    "\n",
    "print(\"Number of training lines:\", len(train_lines))\n",
    "print(\"Number of validation lines:\", len(val_lines))\n",
    "print(\"Number of test lines:\", len(test_lines))\n",
    "print('Lyric Example:', train_lines[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lines = train_lines[:10000]\n",
    "val_lines = val_lines[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation: Tokenize Lyrics, Pad Sequences, Create Dense Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize and add a single sentence start and end token around each sequence \n",
    "train_tokens = [utils.tokenize_line(line, ngram=1) for line in train_lines] \n",
    "val_tokens = [utils.tokenize_line(line, ngram=1) for line in val_lines] \n",
    "test_tokens = [utils.tokenize_line(line, ngram=1) for line in test_lines] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Length: 10.0991\n",
      "Median Length: 10.0\n",
      "90th Percentile Length: 15.0\n",
      "Max Length: 25\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot lengths of lyric lines to help determine an appropriate length to pad/truncate to\n",
    "train_sequence_lengths = [len(seq) for seq in train_tokens]\n",
    "\n",
    "print(\"Mean Length:\", np.mean(train_sequence_lengths))\n",
    "print(\"Median Length:\", np.median(train_sequence_lengths))\n",
    "print(\"90th Percentile Length:\", np.percentile(train_sequence_lengths, 90))\n",
    "print(\"Max Length:\", np.max(train_sequence_lengths))\n",
    "\n",
    "plt.hist(train_sequence_lengths, bins=50)\n",
    "plt.xlabel(\"Length of Lyric Line\")\n",
    "plt.ylabel(\"Number of Lines\")\n",
    "plt.title(\"Distribution of Line Length for Lyrics in Training Data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: 10000\n",
      "Length of Sequences: 15\n"
     ]
    }
   ],
   "source": [
    "def adjust_sequence_length(tokenized_seqs: list, sequence_length: int = SEQUENCE_LENGTH) -> list:\n",
    "    \"\"\"\n",
    "    Pads or truncates all sequences in the provided list to the same length. \n",
    "    Adds padding tokens to the left for too-short sequences and truncates to the right \n",
    "    for too-long sequences (method based on experimentation with left/right padding/truncation)\n",
    "\n",
    "    Args:\n",
    "        tokenized_seqs (list): A list of lists of tokens. Each inner list represents a sequence with tokens as elements\n",
    "        sequence_length (int): The desired length for all of the sequences\n",
    "        padding_token (str): The token that should be used to pad short sequences to the proper length\n",
    "\n",
    "    Returns:\n",
    "        size_adjusted_sequences (list): A list of lists of tokens, where each inner list is the same length\n",
    "    \"\"\"\n",
    "    size_adjusted_sequences = []\n",
    "    for sequence in tokenized_seqs:\n",
    "        if len(sequence) < sequence_length:\n",
    "            # too short, add padding\n",
    "            num_padding = sequence_length - len(sequence)\n",
    "            size_adjusted_sequences.append( ([PADDING] * num_padding) + sequence)\n",
    "        else:\n",
    "            # truncate sequences longer than the chosen length \n",
    "            size_adjusted_sequences.append(sequence[:sequence_length])\n",
    "            \n",
    "\n",
    "    return size_adjusted_sequences\n",
    "\n",
    "\n",
    "def replace_unknowns_train(tokenized_seqs: list) -> list:\n",
    "    \"\"\"\"\n",
    "    Replaces words that occur only once with an UNK token\n",
    "\n",
    "    Args:\n",
    "        tokenized_seqs (list): A list of lists of tokens. Each inner list represents a sequence with tokens as elements\n",
    "\n",
    "    Returns:\n",
    "        Tokenized sequences with low frequency words replaced with the unknown special token \n",
    "    \"\"\"\n",
    "    # concatenate all sequences together \n",
    "    all_tokens = list(chain(*tokenized_seqs))\n",
    "    token_counts = Counter(all_tokens)\n",
    "\n",
    "    # Replace words with low frequencies to UNK so that we can calculate perplexity on test data with unknown words \n",
    "    cleaned_tokenized_seqs = []\n",
    "    for seq in tokenized_seqs:\n",
    "        cleaned_seq = [tok if token_counts[tok] > 1 else UNK for tok in seq]\n",
    "        cleaned_tokenized_seqs.append(cleaned_seq)\n",
    "\n",
    "    return cleaned_tokenized_seqs\n",
    "\n",
    "\n",
    "size_adjusted_sequences_train = adjust_sequence_length(train_tokens)\n",
    "cleaned_sequences_train = replace_unknowns_train(size_adjusted_sequences_train)\n",
    "print(\"Number of sequences:\", len(cleaned_sequences_train))\n",
    "print(\"Length of Sequences:\", len(cleaned_sequences_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 2635\n",
      "encoded examples: \n",
      " [1, 2, 4, 50, 349, 102, 6, 1381, 842, 37, 843, 112, 25, 135, 3] \n",
      " [1, 1, 1, 1, 1, 2, 7, 47, 1382, 18, 1071, 47, 25, 152, 3]\n"
     ]
    }
   ],
   "source": [
    "# Use Tokenizer to map each token to a unique index \n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(cleaned_sequences_train)\n",
    "encoded_sequences_train = tokenizer.texts_to_sequences(cleaned_sequences_train)\n",
    "\n",
    "print(\"Vocab Size:\", len(tokenizer.word_index))\n",
    "print('encoded examples:', '\\n', encoded_sequences_train[0], '\\n', encoded_sequences_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size for word embeddings: 2635\n"
     ]
    }
   ],
   "source": [
    "# create word embeddings using skip gram algorithm\n",
    "word_embeddings = Word2Vec(sentences=cleaned_sequences_train, vector_size=EMBEDDINGS_SIZE, window=5, sg=1, min_count=1)\n",
    "print('Vocab size for word embeddings:', len(word_embeddings.wv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that gives mappings from words to their embeddings and  \n",
    "# indexes from the tokenizers to their embeddings\n",
    "\n",
    "def map_embeddings(embeddings: Word2Vec, tokenizer: Tokenizer) -> (dict, dict):\n",
    "    ''' Creates mappings between different token representations \n",
    "    Arguments:\n",
    "        embeddings: Word2Vec word embeddings for the data (maps tokens to embedding vectors)\n",
    "        tokenizer: Tokenizer used to tokenize the data (maps token to index)\n",
    "    Returns:\n",
    "        (dict): mapping from word to its embedding vector\n",
    "        (dict): mapping from index to its embedding vector\n",
    "    '''\n",
    "    # initialize dictionaries \n",
    "    token_to_embedding = {}\n",
    "    index_to_embedding = {}\n",
    "\n",
    "    # tokenizer maps tokens to unique indices \n",
    "    for token, index in tokenizer.word_index.items():\n",
    "        embedding = embeddings[token]\n",
    "\n",
    "        token_to_embedding[token] = embedding\n",
    "        index_to_embedding[index] = embedding\n",
    "\n",
    "    return (token_to_embedding, index_to_embedding)\n",
    "\n",
    "\n",
    "token_to_embedding, index_to_embedding = map_embeddings(word_embeddings.wv, tokenizer)\n",
    "\n",
    "# Fill in unused index zero to avoid dimension mismatch\n",
    "index_to_embedding[0] = [0] * EMBEDDINGS_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Samples for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X batch shape: (128, 14, 100)\n",
      "y batch shape: (128, 14, 2636)\n"
     ]
    }
   ],
   "source": [
    "def data_generator(data: list, num_sequences_per_batch: int, index_2_embedding: dict) -> (np.array, np.array):\n",
    "    '''\n",
    "    Returns a data generator to train the neural network in batches\n",
    "\n",
    "    X data will be represented in embedding form.\n",
    "    Y data will be represented with one hot vectors. \n",
    "\n",
    "    Args:\n",
    "    data (list of lists): tokenized sequences represented by their unique index encodings \n",
    "    num_sequences_per_batch (int): batch size yielded on each iteration of the generator \n",
    "    index_2_embedding (dict): mapping between unique token indices and dense word embeddings \n",
    "\n",
    "    Returns:\n",
    "    X_batch_embeddings (3-D numpy array): sequences of embeddings with dimensions (batch size, num timesteps, embedding size)\n",
    "                                          Take the first (SEQUENCE_LENGTH - 1) tokens of each sequence\n",
    "    y_batch (3-D numpy array): sequences of one hot vectors with dimensions (batch size, num timesteps, vocab size)\n",
    "                                          Take the last (SEQUENCE_LENGTH - 1) tokens of each sequence \n",
    "                                          (X shifted forward one token so that the neural net predicts \n",
    "                                          the next word in the sequence for each timestep)\n",
    "    '''\n",
    "    # iterate over data in batches - stored in the form of unique token indices \n",
    "    i = 0\n",
    "    while True:\n",
    "        # get samples that we'd like to train on for this batch \n",
    "        data_batch = data[i:i+num_sequences_per_batch]\n",
    "\n",
    "        # increment i with each batch \n",
    "        i += num_sequences_per_batch\n",
    "\n",
    "        # split into X and Y -- shifted sequence so that for each timestep, Y is the token that follows X \n",
    "        X_data = [sequence[:-1] for sequence in data_batch]\n",
    "        Y_data = [sequence[1:] for sequence in data_batch]\n",
    "\n",
    "        # get embeddings for X data \n",
    "        X_embeddings = []\n",
    "        for X_sequence in X_data:\n",
    "            X_sequence_embeddings = [index_2_embedding[token_idx] for token_idx in X_sequence]\n",
    "            X_embeddings.append(X_sequence_embeddings)\n",
    "\n",
    "        # get one hot vectors for Y data \n",
    "        Y_one_hot_vectors = []\n",
    "        for Y_sequence in Y_data:\n",
    "            Y_one_hot = to_categorical(Y_sequence, num_classes=len(index_2_embedding))\n",
    "            Y_one_hot_vectors.append(Y_one_hot)\n",
    "\n",
    "        # yield statement instead of return for generator \n",
    "        yield(np.array(X_embeddings), np.array(Y_one_hot_vectors))\n",
    "\n",
    "\n",
    "# demo the data generator\n",
    "demo_data_generator = data_generator(encoded_sequences_train, BATCH_SIZE, index_to_embedding)\n",
    "demo_sample = next(demo_data_generator)\n",
    "print(\"X batch shape:\", demo_sample[0].shape)\n",
    "print(\"y batch shape:\", demo_sample[1].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare validation data\n",
    "# Since the model does not know about these sequences beforehand, \n",
    "# unknown words are those that do not appear in the training vocabulary \n",
    "def encode_new_sequences(tokenized_seqs: list, tokenizer) -> list:\n",
    "    \"\"\"\"\n",
    "    Replaces words that are not in the tokenizer's vocab with the unknown special token and encodes it to \n",
    "    unique token indices specified by the provided Tokenizer.\n",
    "\n",
    "    Args:\n",
    "        tokenized_seqs (list): A list of lists of tokens. Each inner list represents a sequence with tokens as elements\n",
    "        tokenizer: Tokenizer used maps token to index\n",
    "\n",
    "    Returns:\n",
    "        Encoded sequences with words not in the training vocabulary replaced with the unknown special token \n",
    "    \"\"\"\n",
    "    cleaned_tokenized_seqs = []\n",
    "    for seq in tokenized_seqs:\n",
    "        cleaned_seq = [tok if tok in tokenizer.word_index.keys() else UNK for tok in seq]\n",
    "        cleaned_tokenized_seqs.append(cleaned_seq)\n",
    "\n",
    "    return tokenizer.texts_to_sequences(cleaned_tokenized_seqs)\n",
    "\n",
    "encoded_sequences_val = encode_new_sequences(val_tokens, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_rnn(train_data: np.array,\n",
    "             index_2_embedding: dict, \n",
    "             num_epochs: int=1, \n",
    "             num_sequences_per_batch: int=BATCH_SIZE, \n",
    "             sequence_length: int=SEQUENCE_LENGTH,\n",
    "             embedding_size: int=EMBEDDINGS_SIZE):\n",
    "    \"\"\"\n",
    "    Creates and trains an RNN with LSTM cells using given training data and batch size.\n",
    "\n",
    "    Args:\n",
    "        train_data (list of lists): encoded sequences of training data represented by token indices \n",
    "        index_2_embedding (dict): mapping from token index -> word2vec embeddings \n",
    "        num_epochs (int): number of training epochs\n",
    "        num_sequences_per_batch (int): batch size for training data \n",
    "        sequence_length (int): number of tokens in each training sample \n",
    "        embedding_size (int): size of the dense word embeddings used to represent tokens \n",
    "    Returns:\n",
    "        A trained Neural Network language model\n",
    "    \"\"\"\n",
    "    # define model parameters\n",
    "    hidden_units = 200\n",
    "    hidden_input_dim = (sequence_length - 1, embedding_size)      # (number of steps, number of features per step)\n",
    "    output_dim = len(index_2_embedding)                            # vocab size \n",
    "\n",
    "    # instantiate model\n",
    "    model = Sequential()\n",
    "\n",
    "    # hidden layer\n",
    "    model.add(Bidirectional(LSTM(hidden_units, \n",
    "                                 input_shape=hidden_input_dim,\n",
    "                                 return_sequences=True)))\n",
    "\n",
    "    # output layer\n",
    "    model.add(Dense(units=output_dim, activation='softmax'))\n",
    "\n",
    "    # configure the learning process\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=[\"top_k_categorical_accuracy\"])\n",
    "    \n",
    "    # total number of batches per epoch \n",
    "    steps_per_epoch = len(train_data)//num_sequences_per_batch\n",
    "   \n",
    "    for i in range(num_epochs):\n",
    "        if i % 5 == 0:\n",
    "            print(\"Epoch\", i)\n",
    "\n",
    "        # create a new data generator for us to iterate through\n",
    "        train_generator = data_generator(train_data, num_sequences_per_batch, index_2_embedding)\n",
    "\n",
    "        # train model \n",
    "        model.fit(x=train_generator, steps_per_epoch=steps_per_epoch)\n",
    "\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "78/78 [==============================] - 85s 1s/step - loss: 4.3990 - top_k_categorical_accuracy: 0.4868\n",
      "78/78 [==============================] - 86s 1s/step - loss: 3.3941 - top_k_categorical_accuracy: 0.5599\n",
      "78/78 [==============================] - 74s 948ms/step - loss: 2.9222 - top_k_categorical_accuracy: 0.6335\n",
      "78/78 [==============================] - 65s 838ms/step - loss: 2.3699 - top_k_categorical_accuracy: 0.7139\n",
      "78/78 [==============================] - 71s 917ms/step - loss: 1.9055 - top_k_categorical_accuracy: 0.7724\n",
      "Epoch 5\n",
      "78/78 [==============================] - 71s 913ms/step - loss: 1.5501 - top_k_categorical_accuracy: 0.8131\n",
      "78/78 [==============================] - 73s 931ms/step - loss: 1.2913 - top_k_categorical_accuracy: 0.8433\n",
      "78/78 [==============================] - 68s 867ms/step - loss: 1.1017 - top_k_categorical_accuracy: 0.8684\n",
      "78/78 [==============================] - 66s 846ms/step - loss: 0.9547 - top_k_categorical_accuracy: 0.8898\n",
      "78/78 [==============================] - 75s 964ms/step - loss: 0.8387 - top_k_categorical_accuracy: 0.9085\n",
      "Epoch 10\n",
      "78/78 [==============================] - 81s 1s/step - loss: 0.7423 - top_k_categorical_accuracy: 0.9249\n",
      "78/78 [==============================] - 81s 1s/step - loss: 0.6624 - top_k_categorical_accuracy: 0.9380\n",
      "78/78 [==============================] - 84s 1s/step - loss: 0.5916 - top_k_categorical_accuracy: 0.9487\n",
      "78/78 [==============================] - 82s 1s/step - loss: 0.5287 - top_k_categorical_accuracy: 0.9576\n",
      "78/78 [==============================] - 93s 1s/step - loss: 0.4734 - top_k_categorical_accuracy: 0.9645\n",
      "Epoch 15\n",
      "78/78 [==============================] - 89s 1s/step - loss: 0.4251 - top_k_categorical_accuracy: 0.9700\n",
      "23/78 [=======>......................] - ETA: 1:04 - loss: 0.3851 - top_k_categorical_accuracy: 0.9737"
     ]
    }
   ],
   "source": [
    "# create and train model\n",
    "model = lstm_rnn(np.array(encoded_sequences_train), index_to_embedding, num_epochs=NUM_EPOCHS)\n",
    "\n",
    "# save trained model \n",
    "if SHOULD_SAVE:\n",
    "    model.save(SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create functions to generate new sequences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sequences(model: Sequential, \n",
    "                      tokenizer: Tokenizer, \n",
    "                      index_2_embedding: dict, \n",
    "                      num_seq: int):\n",
    "    '''\n",
    "    Generates a given number of sequences using the given RNN language model.\n",
    "    Will begin the sequence generation with n-1 SENTENCE_BEGIN tokens.\n",
    "    Returned sequences will have the BEGIN, END, and PADDING tokens removed\n",
    "\n",
    "    Args:\n",
    "        model: RNN language model\n",
    "        tokenizer: the keras preprocessing tokenizer\n",
    "        index_2_embedding: mapping from token index -> word2vec embeddings \n",
    "        num_seq: the number of sequences to generate \n",
    "\n",
    "    Returns: \n",
    "        A list of strings, where each string is a generated sequence with special tokens removed \n",
    "    '''\n",
    "    seed = [SENTENCE_BEGIN] * (SEQUENCE_LENGTH - 1) \n",
    "    \n",
    "    sequences = []\n",
    "    for _ in range(num_seq):\n",
    "        seq = generate_seq(model, tokenizer, index_2_embedding, seed)\n",
    "        seq = ' '.join(seq)\n",
    "\n",
    "        # remove special tokens\n",
    "        seq = seq.replace(SENTENCE_BEGIN, '')\n",
    "        seq = seq.replace(SENTENCE_END, '')\n",
    "        seq = seq.replace(PADDING, '')\n",
    "        seq = seq.replace(UNK, '')\n",
    "\n",
    "        sequences.append(seq.strip())\n",
    "        \n",
    "    return sequences\n",
    "\n",
    "\n",
    "def generate_seq(model: Sequential, \n",
    "                 tokenizer: Tokenizer, \n",
    "                 index_2_embedding: dict, \n",
    "                 seed: list):\n",
    "    '''\n",
    "    Generates a single sequence using the given model starting with a SENTENCE_BEGIN and ending with a SENTENCE_END token. \n",
    "    Since an RNN takes input sequences of fixed length, use a sliding window to continually predict the next word. \n",
    "\n",
    "    Args:\n",
    "        model: RNN language model\n",
    "        tokenizer: the keras preprocessing tokenizer\n",
    "        index_2_embedding: mapping from token index -> word2vec embeddings \n",
    "        seed: the initial tokens to feed the RNN\n",
    "    Returns: \n",
    "        An array of tokens representing a sequence \n",
    "    '''\n",
    "    padding_index = tokenizer.word_index.get(PADDING)\n",
    "    sentence_begin_index = tokenizer.word_index.get(SENTENCE_BEGIN)\n",
    "    sentence_end_index = tokenizer.word_index.get(SENTENCE_END)\n",
    "\n",
    "    # track the unique token indices for the sequence \n",
    "    sequence_indices = [tokenizer.word_index.get(tok) for tok in seed] \n",
    "\n",
    "    # number of timesteps that the model expects as input\n",
    "    input_length = SEQUENCE_LENGTH - 1\n",
    "\n",
    "    # until we get a SENTENCE_END token\n",
    "    while sequence_indices[-1] != sentence_end_index:\n",
    "        # get latest tokens to use as inputs \n",
    "        input_sequence = sequence_indices[-1*input_length:]\n",
    "\n",
    "        # convert the input sequence to embeddings\n",
    "        input_embeddings = np.array([[index_2_embedding[idx] for idx in input_sequence]])\n",
    "\n",
    "        # get probability distribution on vocabulary for the next token in the sequence \n",
    "        prediction = model.predict(input_embeddings, verbose=False)[0][-1]\n",
    "\n",
    "        # sample from the probability distribution \n",
    "        next_tok_idx = np.random.choice(len(prediction), p=prediction)\n",
    "\n",
    "        # skip mid-sentence SENTENCE_BEGIN and PADDING tokens\n",
    "        if next_tok_idx == sentence_begin_index or next_tok_idx == padding_index:\n",
    "            continue\n",
    "\n",
    "        # add newly generated token to our sequence \n",
    "        sequence_indices.append(next_tok_idx)\n",
    "\n",
    "    # convert to words \n",
    "    tokenizer_words = list(tokenizer.word_index.keys())\n",
    "    tokenizer_indices = list(tokenizer.word_index.values())\n",
    "    sequence = [tokenizer_words[tokenizer_indices.index(idx)] for idx in sequence_indices]\n",
    "    return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Generated Lyrics:\n",
      "\n",
      "\n",
      "\n",
      "what know\n",
      "by have if wo this dear on on is shame them i 'm finally i do n't fool then stand ' own   his to call barstool , love memories much you wanted  ones , , , bring by up , morn about love momma tale  man so feels of from spreadin into to still lived ' to  as 's  she used beauty the town no\n",
      "\n",
      "\n",
      "as 's more out far darling just a sun of from little  the    is cotton everywhere through wind just i 'd thankful in on my came shame means and time so is rolls polka kisses sparrow know n't another m the   heart a lucky comes comin again\n",
      "for keep just to god a tied out in rang prove shade shame out world mama  shining his  a world happy my watch stealing of of hand me they lonely to me ride in will i know walk that stronger on\n",
      "\n",
      "my somebody ah i love your\n"
     ]
    }
   ],
   "source": [
    "# load in model (if we have a pre-trained one that we'd like to generate sequences for)\n",
    "if SHOULD_LOAD:\n",
    "    model = keras.saving.load_model(LOAD_PATH)\n",
    "    \n",
    "# Generate new lyrics \n",
    "generated_sequences = generate_sequences(model, tokenizer, index_to_embedding, num_seq=10)\n",
    "print(\"Sample Generated Lyrics:\\n\")\n",
    "for seq in generated_sequences:\n",
    "    print(seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Perplexity \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing perplexity of line 0 / 500\n",
      "Computing perplexity of line 25 / 500\n",
      "Computing perplexity of line 50 / 500\n",
      "Computing perplexity of line 75 / 500\n",
      "Computing perplexity of line 100 / 500\n",
      "Computing perplexity of line 125 / 500\n",
      "Computing perplexity of line 150 / 500\n",
      "Computing perplexity of line 175 / 500\n",
      "Computing perplexity of line 200 / 500\n",
      "Computing perplexity of line 225 / 500\n",
      "Computing perplexity of line 250 / 500\n",
      "Computing perplexity of line 275 / 500\n",
      "Computing perplexity of line 300 / 500\n",
      "Computing perplexity of line 325 / 500\n",
      "Computing perplexity of line 350 / 500\n",
      "Computing perplexity of line 375 / 500\n",
      "Computing perplexity of line 400 / 500\n",
      "Computing perplexity of line 425 / 500\n",
      "Computing perplexity of line 450 / 500\n",
      "Computing perplexity of line 475 / 500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "347.79992897087277"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_perplexity(encoded_sequence: list, model: Sequential, \n",
    "                        tokenizer: Tokenizer, \n",
    "                        index_2_embedding: dict,\n",
    "                        verbose: bool = False):\n",
    "    '''\n",
    "    Computes the perplexity of a single sequence by finding the probability that the model will generate \n",
    "    this sequence. Uses a sliding window to continuously predict the next word, and finds the softmax probability\n",
    "    associated with the true word. \n",
    "\n",
    "    Args:\n",
    "        encoded_sequence (list): a single sequence represented by its Tokenizer encodings \n",
    "        model: RNN language model\n",
    "        tokenizer: the keras preprocessing tokenizer\n",
    "        index_2_embedding: mapping from token index -> word2vec embeddings \n",
    "        verbose (bool): If true, prints information about the sequence and the probability of each word \n",
    "    Returns: \n",
    "        the perplexity of the sequence \n",
    "    ''' \n",
    "    padding_index = tokenizer.word_index.get(PADDING)\n",
    "    sentence_begin_index = tokenizer.word_index.get(SENTENCE_BEGIN)\n",
    "\n",
    "    # shift Y forward one token to represent the next word predictions \n",
    "    X_data = encoded_sequence[:-1]\n",
    "    Y_data = encoded_sequence[1:]\n",
    "\n",
    "    # seed with SEQUENCE_LENGTH - 2 sentence begins (first token in X is a sentence begin as well),\n",
    "    # inch window along to predict the next word \n",
    "    encoded_input = [sentence_begin_index] * (SEQUENCE_LENGTH - 2) + X_data\n",
    "\n",
    "    input_length = SEQUENCE_LENGTH - 1\n",
    "\n",
    "    # we will be finding the log of the probability of our model generating this sequence \n",
    "    Y_pred_log_prob = 0\n",
    "\n",
    "    # track the number of meaningful tokens \n",
    "    N = 0\n",
    "\n",
    "    # for each word in Y (all tokens except SENTENCE_BEGIN)\n",
    "    for i, y in enumerate(Y_data):\n",
    "        # use a sliding window over the input, where the input sequence are the tokens preceding y \n",
    "        input_sequence = encoded_input[i:input_length+i]\n",
    "\n",
    "        # convert the input sequence to embeddings\n",
    "        input_embeddings = np.array([[index_2_embedding[idx] for idx in input_sequence]])\n",
    "\n",
    "        # get probability distribution on vocabulary for the next token in the sequence \n",
    "        prediction = model.predict(input_embeddings, verbose=False)[0][-1]\n",
    "\n",
    "        # index y to get the predicted probability of the true value \n",
    "        y_pred_prob = prediction[y] \n",
    "\n",
    "        # print information to help with debugging \n",
    "        if verbose:\n",
    "            print(\"Encoded input sequence:\", input_sequence)\n",
    "            print(\"Encoded y to predict:\", y)\n",
    "            print(\"Probability of next token being y:\", y_pred_prob)\n",
    "\n",
    "        Y_pred_log_prob += np.log(y_pred_prob)\n",
    "\n",
    "        # only include meaningful tokens in our token count \n",
    "        if y != padding_index and y != sentence_begin_index:\n",
    "            N += 1\n",
    "\n",
    "    # compute probability of our model generating this sequence as well as the perplexity \n",
    "    Y_pred_prob = np.exp(Y_pred_log_prob)\n",
    "    perplexity = Y_pred_prob ** (-1/N)\n",
    "    \n",
    "    return perplexity\n",
    "\n",
    "\n",
    "def mean_perplexity(val_data: np.array, \n",
    "                    model: Sequential, \n",
    "                    tokenizer: Tokenizer, \n",
    "                    index_2_embedding: dict):\n",
    "    \"\"\"\" \n",
    "    Computes the average perplexity of all sequences in the given data using the given model.\n",
    "\n",
    "    Args:\n",
    "        encoded_sequence (list): a single sequence represented by its Tokenizer encodings \n",
    "        model: RNN language model\n",
    "        tokenizer: the keras preprocessing tokenizer\n",
    "        index_2_embedding: mapping from token index -> word2vec embeddings \n",
    "        verbose (bool): If true, prints information about the sequence and the probability of each word \n",
    "    Returns: \n",
    "        The average perplexity of the provided sequences \n",
    "    \"\"\"\n",
    "    total_samples = len(val_data)\n",
    "\n",
    "    perplexities = []\n",
    "    for i, seq in enumerate(val_data):\n",
    "        if i % 25 == 0:\n",
    "            print(\"Computing perplexity of line\", i, \"/\", total_samples)\n",
    "        perplexities.append(calculate_perplexity(seq, model, tokenizer, index_2_embedding))\n",
    "    return np.mean(perplexities)\n",
    "\n",
    "#TODO - probably increase, just takes about a minute for every 100 sequences \n",
    "#calculate_perplexity(encoded_sequences_val[0], model, tokenizer, index_to_embedding, verbose=True)\n",
    "mean_perplexity(encoded_sequences_val[:500], model, tokenizer, index_to_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experimenting with Hyperparameters \n",
    "\n",
    "To find our final configuration for our RNN + LSTM model, we will pick a genre test out hyperparameters such as sequence length, embedding size, number of hidden units, number of epochs, and additional layers. \n",
    "\n",
    "For each configuration, report: \n",
    "1. number of sequences \n",
    "2. pre-processing strategy (padding / concatenation?)\n",
    "3. epochs\n",
    "4. dimensions of network (# of layers, # of hidden units per layer)\n",
    "5. `SEQUENCE_LENGTH` value\n",
    "6. time to train \n",
    "7. final `val_accuracy`\n",
    "8. perplexity \n",
    "9. generated sequence example\n",
    "-----------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key Findings \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
