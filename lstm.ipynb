{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN with LSTMs Language Model using Skip-Gram Dense Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports \n",
    "from gensim.models import Word2Vec\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter \n",
    "from itertools import chain\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Bidirectional\n",
    "\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants \n",
    "SENTENCE_BEGIN = \"<s>\"\n",
    "SENTENCE_END = \"</s>\"\n",
    "PADDING = \"<pad>\"\n",
    "UNK = \"<unk>\"\n",
    "\n",
    "# hyperparameters - may change \n",
    "EMBEDDINGS_SIZE = 100\n",
    "BATCH_SIZE = 128\n",
    "SEQUENCE_LENGTH = 10\n",
    "NUM_EPOCHS = 20\n",
    "\n",
    "# may either be country or metal\n",
    "GENRE = 'country' \n",
    "\n",
    "# change to save a newly trained model\n",
    "SAVE_PATH = 'foo'\n",
    "SHOULD_SAVE = False \n",
    "\n",
    "# change to load in an already trained model for sequence generation \n",
    "LOAD_PATH = 'country_lstm_model_epoch20_full'\n",
    "SHOULD_LOAD = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training lines: 149771\n",
      "Number of validation lines: 18610\n",
      "Number of test lines: 19108\n",
      "Lyric Example: i've seen how you tremble whenever he walks through your mind\n"
     ]
    }
   ],
   "source": [
    "# read in cleaned data\n",
    "if GENRE == 'country':\n",
    "    train_lines = pd.read_csv('data/country_train.csv', header=None)[0].tolist()\n",
    "    val_lines = pd.read_csv('data/country_val.csv', header=None)[0].tolist()\n",
    "    test_lines = pd.read_csv('data/country_test.csv', header=None)[0].tolist()\n",
    "\n",
    "elif GENRE == 'metal':\n",
    "    train_lines = pd.read_csv('data/metal_train.csv', header=None)[0].tolist()\n",
    "    val_lines = pd.read_csv('data/metal_val.csv', header=None)[0].tolist()\n",
    "    test_lines = pd.read_csv('data/metal_test.csv', header=None)[0].tolist()\n",
    "\n",
    "else:\n",
    "    raise ValueError('Incorrect genre given.')\n",
    "\n",
    "\n",
    "print(\"Number of training lines:\", len(train_lines))\n",
    "print(\"Number of validation lines:\", len(val_lines))\n",
    "print(\"Number of test lines:\", len(test_lines))\n",
    "print('Lyric Example:', train_lines[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation: Tokenize Lyrics, Pad Sequences, Create Dense Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize and add a single sentence start and end token around each sequence \n",
    "train_tokens = [utils.tokenize_line(line, ngram=1) for line in train_lines] \n",
    "val_tokens = [utils.tokenize_line(line, ngram=1) for line in val_lines] \n",
    "test_tokens = [utils.tokenize_line(line, ngram=1) for line in test_lines] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Length: 10.021025432159764\n",
      "Median Length: 10.0\n",
      "90th Percentile Length: 15.0\n",
      "Max Length: 246\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot lengths of lyric lines to help determine an appropriate length to pad/truncate to\n",
    "train_sequence_lengths = [len(seq) for seq in train_tokens]\n",
    "\n",
    "print(\"Mean Length:\", np.mean(train_sequence_lengths))\n",
    "print(\"Median Length:\", np.median(train_sequence_lengths))\n",
    "print(\"90th Percentile Length:\", np.percentile(train_sequence_lengths, 90))\n",
    "print(\"Max Length:\", np.max(train_sequence_lengths))\n",
    "\n",
    "plt.hist(train_sequence_lengths, bins=50)\n",
    "plt.xlabel(\"Length of Lyric Line\")\n",
    "plt.ylabel(\"Number of Lines\")\n",
    "plt.title(\"Distribution of Line Length for Lyrics in Training Data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: 149771\n",
      "Length of Sequences: 10\n"
     ]
    }
   ],
   "source": [
    "def adjust_sequence_length(tokenized_seqs: list, sequence_length: int = SEQUENCE_LENGTH) -> list:\n",
    "    \"\"\"\n",
    "    Pads or truncates all sequences in the provided list to the same length. \n",
    "    Adds padding tokens to the left for too-short sequences and truncates to the right \n",
    "    for too-long sequences (method based on experimentation with left/right padding/truncation)\n",
    "\n",
    "    Args:\n",
    "        tokenized_seqs (list): A list of lists of tokens. Each inner list represents a sequence with tokens as elements\n",
    "        sequence_length (int): The desired length for all of the sequences\n",
    "        padding_token (str): The token that should be used to pad short sequences to the proper length\n",
    "\n",
    "    Returns:\n",
    "        size_adjusted_sequences (list): A list of lists of tokens, where each inner list is the same length\n",
    "    \"\"\"\n",
    "    size_adjusted_sequences = []\n",
    "    for sequence in tokenized_seqs:\n",
    "        if len(sequence) < sequence_length:\n",
    "            # too short, add padding\n",
    "            num_padding = sequence_length - len(sequence)\n",
    "            size_adjusted_sequences.append( ([PADDING] * num_padding) + sequence)\n",
    "        else:\n",
    "            # truncate sequences longer than the chosen length \n",
    "            size_adjusted_sequences.append(sequence[:sequence_length])\n",
    "            \n",
    "\n",
    "    return size_adjusted_sequences\n",
    "\n",
    "\n",
    "def replace_unknowns_train(tokenized_seqs: list) -> list:\n",
    "    \"\"\"\"\n",
    "    Replaces words that occur only once with an UNK token\n",
    "\n",
    "    Args:\n",
    "        tokenized_seqs (list): A list of lists of tokens. Each inner list represents a sequence with tokens as elements\n",
    "\n",
    "    Returns:\n",
    "        Tokenized sequences with low frequency words replaced with the unknown special token \n",
    "    \"\"\"\n",
    "    # concatenate all sequences together \n",
    "    all_tokens = list(chain(*tokenized_seqs))\n",
    "    token_counts = Counter(all_tokens)\n",
    "\n",
    "    # Replace words with low frequencies to UNK so that we can calculate perplexity on test data with unknown words \n",
    "    cleaned_tokenized_seqs = []\n",
    "    for seq in tokenized_seqs:\n",
    "        cleaned_seq = [tok if token_counts[tok] > 1 else UNK for tok in seq]\n",
    "        cleaned_tokenized_seqs.append(cleaned_seq)\n",
    "\n",
    "    return cleaned_tokenized_seqs\n",
    "\n",
    "\n",
    "size_adjusted_sequences_train = adjust_sequence_length(train_tokens)\n",
    "cleaned_sequences_train = replace_unknowns_train(size_adjusted_sequences_train)\n",
    "print(\"Number of sequences:\", len(cleaned_sequences_train))\n",
    "print(\"Length of Sequences:\", len(cleaned_sequences_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 10610\n",
      "encoded examples: \n",
      " [2, 4, 45, 305, 81, 6, 4212, 1035, 37, 1259] \n",
      " [2, 4213, 49, 1306, 16, 1133, 49, 27, 148, 3]\n"
     ]
    }
   ],
   "source": [
    "# Use Tokenizer to map each token to a unique index \n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(cleaned_sequences_train)\n",
    "encoded_sequences_train = tokenizer.texts_to_sequences(cleaned_sequences_train)\n",
    "\n",
    "print(\"Vocab Size:\", len(tokenizer.word_index))\n",
    "print('encoded examples:', '\\n', encoded_sequences_train[0], '\\n', encoded_sequences_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size for word embeddings: 10610\n"
     ]
    }
   ],
   "source": [
    "# create word embeddings using skip gram algorithm\n",
    "word_embeddings = Word2Vec(sentences=cleaned_sequences_train, vector_size=EMBEDDINGS_SIZE, window=5, sg=1, min_count=1)\n",
    "print('Vocab size for word embeddings:', len(word_embeddings.wv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that gives mappings from words to their embeddings and  \n",
    "# indexes from the tokenizers to their embeddings\n",
    "\n",
    "def map_embeddings(embeddings: Word2Vec, tokenizer: Tokenizer) -> (dict, dict):\n",
    "    ''' Creates mappings between different token representations \n",
    "    Arguments:\n",
    "        embeddings: Word2Vec word embeddings for the data (maps tokens to embedding vectors)\n",
    "        tokenizer: Tokenizer used to tokenize the data (maps token to index)\n",
    "    Returns:\n",
    "        (dict): mapping from word to its embedding vector\n",
    "        (dict): mapping from index to its embedding vector\n",
    "    '''\n",
    "    # initialize dictionaries \n",
    "    token_to_embedding = {}\n",
    "    index_to_embedding = {}\n",
    "\n",
    "    # tokenizer maps tokens to unique indices \n",
    "    for token, index in tokenizer.word_index.items():\n",
    "        embedding = embeddings[token]\n",
    "\n",
    "        token_to_embedding[token] = embedding\n",
    "        index_to_embedding[index] = embedding\n",
    "\n",
    "    return (token_to_embedding, index_to_embedding)\n",
    "\n",
    "\n",
    "token_to_embedding, index_to_embedding = map_embeddings(word_embeddings.wv, tokenizer)\n",
    "\n",
    "# Fill in unused index zero to avoid dimension mismatch\n",
    "index_to_embedding[0] = [0] * EMBEDDINGS_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Samples for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X batch shape: (128, 9, 100)\n",
      "y batch shape: (128, 9, 10611)\n"
     ]
    }
   ],
   "source": [
    "def data_generator(data: list, num_sequences_per_batch: int, index_2_embedding: dict) -> (np.array, np.array):\n",
    "    '''\n",
    "    Returns a data generator to train the neural network in batches\n",
    "\n",
    "    X data will be represented in embedding form.\n",
    "    Y data will be represented with one hot vectors. \n",
    "\n",
    "    Args:\n",
    "    data (list of lists): tokenized sequences represented by their unique index encodings \n",
    "    num_sequences_per_batch (int): batch size yielded on each iteration of the generator \n",
    "    index_2_embedding (dict): mapping between unique token indices and dense word embeddings \n",
    "\n",
    "    Returns:\n",
    "    X_batch_embeddings (3-D numpy array): sequences of embeddings with dimensions (batch size, num timesteps, embedding size)\n",
    "                                          Take the first (SEQUENCE_LENGTH - 1) tokens of each sequence\n",
    "    y_batch (3-D numpy array): sequences of one hot vectors with dimensions (batch size, num timesteps, vocab size)\n",
    "                                          Take the last (SEQUENCE_LENGTH - 1) tokens of each sequence \n",
    "                                          (X shifted forward one token so that the neural net predicts \n",
    "                                          the next word in the sequence for each timestep)\n",
    "    '''\n",
    "    # iterate over data in batches - stored in the form of unique token indices \n",
    "    i = 0\n",
    "    while True:\n",
    "        # get samples that we'd like to train on for this batch \n",
    "        data_batch = data[i:i+num_sequences_per_batch]\n",
    "\n",
    "        # increment i with each batch \n",
    "        i += num_sequences_per_batch\n",
    "\n",
    "        # split into X and Y -- shifted sequence so that for each timestep, Y is the token that follows X \n",
    "        X_data = [sequence[:-1] for sequence in data_batch]\n",
    "        Y_data = [sequence[1:] for sequence in data_batch]\n",
    "\n",
    "        # get embeddings for X data \n",
    "        X_embeddings = []\n",
    "        for X_sequence in X_data:\n",
    "            X_sequence_embeddings = [index_2_embedding[token_idx] for token_idx in X_sequence]\n",
    "            X_embeddings.append(X_sequence_embeddings)\n",
    "\n",
    "        # get one hot vectors for Y data \n",
    "        Y_one_hot_vectors = []\n",
    "        for Y_sequence in Y_data:\n",
    "            Y_one_hot = to_categorical(Y_sequence, num_classes=len(index_2_embedding))\n",
    "            Y_one_hot_vectors.append(Y_one_hot)\n",
    "\n",
    "        # yield statement instead of return for generator \n",
    "        yield(np.array(X_embeddings), np.array(Y_one_hot_vectors))\n",
    "\n",
    "\n",
    "# demo the data generator\n",
    "demo_data_generator = data_generator(encoded_sequences_train, BATCH_SIZE, index_to_embedding)\n",
    "demo_sample = next(demo_data_generator)\n",
    "print(\"X batch shape:\", demo_sample[0].shape)\n",
    "print(\"y batch shape:\", demo_sample[1].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare validation data\n",
    "# Since the model does not know about these sequences beforehand, \n",
    "# unknown words are those that do not appear in the training vocabulary \n",
    "def encode_new_sequences(tokenized_seqs: list, tokenizer) -> list:\n",
    "    \"\"\"\"\n",
    "    Replaces words that are not in the tokenizer's vocab with the unknown special token and encodes it to \n",
    "    unique token indices specified by the provided Tokenizer.\n",
    "\n",
    "    Args:\n",
    "        tokenized_seqs (list): A list of lists of tokens. Each inner list represents a sequence with tokens as elements\n",
    "        tokenizer: Tokenizer used maps token to index\n",
    "\n",
    "    Returns:\n",
    "        Encoded sequences with words not in the training vocabulary replaced with the unknown special token \n",
    "    \"\"\"\n",
    "    cleaned_tokenized_seqs = []\n",
    "    for seq in tokenized_seqs:\n",
    "        cleaned_seq = [tok if tok in tokenizer.word_index.keys() else UNK for tok in seq]\n",
    "        cleaned_tokenized_seqs.append(cleaned_seq)\n",
    "\n",
    "    return tokenizer.texts_to_sequences(cleaned_tokenized_seqs)\n",
    "\n",
    "#size_adjusted_sequences_val = adjust_sequence_length(val_tokens)\n",
    "#encoded_sequences_val = encode_val_sequences(size_adjusted_sequences_val, tokenizer)\n",
    "encoded_sequences_val = encode_new_sequences(val_tokens, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_rnn(train_data: np.array,\n",
    "             val_data: np.array, \n",
    "             index_2_embedding: dict, \n",
    "             num_epochs: int=1, \n",
    "             num_sequences_per_batch: int=BATCH_SIZE, \n",
    "             sequence_length: int=SEQUENCE_LENGTH,\n",
    "             embedding_size: int=EMBEDDINGS_SIZE):\n",
    "    \"\"\"\n",
    "    Creates and trains an RNN with LSTM cells using given training data and batch size.\n",
    "\n",
    "    Args:\n",
    "        train_data (list of lists): encoded sequences of training data represented by token indices \n",
    "        train_data (list of lists): encoded sequences of validation data represented by token indices \n",
    "        index_2_embedding (dict): mapping from token index -> word2vec embeddings \n",
    "        num_epochs (int): number of training epochs\n",
    "        num_sequences_per_batch (int): batch size for training data \n",
    "        sequence_length (int): number of tokens in each training sample \n",
    "        embedding_size (int): size of the dense word embeddings used to represent tokens \n",
    "    Returns:\n",
    "        A trained Neural Network language model\n",
    "    \"\"\"\n",
    "    # define model parameters\n",
    "    hidden_units = 200\n",
    "    hidden_input_dim = (sequence_length - 1, embedding_size)      # (number of steps, number of features per step)\n",
    "    output_dim = len(index_2_embedding)                            # vocab size \n",
    "\n",
    "    # instantiate model\n",
    "    model = Sequential()\n",
    "\n",
    "    # hidden layer\n",
    "    model.add(Bidirectional(LSTM(hidden_units, \n",
    "                                 input_shape=hidden_input_dim,\n",
    "                                 return_sequences=True)))\n",
    "\n",
    "    # output layer\n",
    "    model.add(Dense(units=output_dim, activation='softmax'))\n",
    "\n",
    "    # configure the learning process\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=[\"top_k_categorical_accuracy\"])\n",
    "    \n",
    "    # total number of batches per epoch \n",
    "    steps_per_epoch = len(train_data)//num_sequences_per_batch\n",
    "    steps_per_epoch_val = len(val_data)//num_sequences_per_batch\n",
    "   \n",
    "    for i in range(num_epochs):\n",
    "        if i % 5 == 0:\n",
    "            print(\"Epoch\", i)\n",
    "\n",
    "        # create a new data generator for us to iterate through\n",
    "        train_generator = data_generator(train_data, num_sequences_per_batch, index_2_embedding)\n",
    "        val_generator = data_generator(val_data, num_sequences_per_batch, index_2_embedding)\n",
    "\n",
    "        # train model \n",
    "        model.fit(x=train_generator, steps_per_epoch=steps_per_epoch, validation_data=val_generator, validation_steps=steps_per_epoch_val)\n",
    "\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "X\n",
      "[array([   2,    4,   45,  305,   77,    6, 4389, 1040,   36]), array([   2, 4067,   49, 1312,   15, 1111,   49,   27,  151]), array([   2,   82,    5,  198,   19,  115,   24, 1182,   10]), array([  2,   4,  41,  15,  36, 340,   6,   7, 340]), array([  2,  58,   4, 489, 658,  77,  10, 154,   6])]\n",
      "Y\n",
      "[array([   4,   45,  305,   77,    6, 4389, 1040,   36,    3]), array([4067,   49, 1312,   15, 1111,   49,   27,  151,    3]), array([  82,    5,  198,   19,  115,   24, 1182,   10,    3]), array([  4,  41,  15,  36, 340,   6,   7, 340,   3]), array([ 58,   4, 489, 658,  77,  10, 154,   6,   3])]\n",
      "X\n",
      "[array([  1,   1,   2,   4, 108,   6,  10, 312,  13]), array([ 1,  1,  1,  1,  1,  2, 48, 24, 34]), array([ 1,  1,  1,  1,  1,  2, 48, 24, 34]), array([   1,    1,    1,    1,    2,   70,   67,   17, 1786]), array([  1,   1,   1,   1,   2, 403,  13, 419, 358])]\n",
      "Y\n",
      "[array([  1,   2,   4, 108,   6,  10, 312,  13,   3]), array([ 1,  1,  1,  1,  2, 48, 24, 34,  3]), array([ 1,  1,  1,  1,  2, 48, 24, 34,  3]), array([   1,    1,    1,    2,   70,   67,   17, 1786,    3]), array([  1,   1,   1,   2, 403,  13, 419, 358,   3])]\n",
      "1170/1170 [==============================] - ETA: 0s - loss: 2.2084 - top_k_categorical_accuracy: 0.7551X\n",
      "[array([ 2,  6, 22, 16, 41, 48, 11, 97, 32]), array([   2,    4,   41,    6,   45,   46,    9,  546, 1580]), array([  2,  22,  16,  99,  13,  66,  62, 174,  17]), array([  2,  22,  16,   6,  28,  38, 897,  10,  13]), array([   1,    1,    1,    1,    1,    2,  155, 1524,  163])]\n",
      "Y\n",
      "[array([ 6, 22, 16, 41, 48, 11, 97, 32,  3]), array([   4,   41,    6,   45,   46,    9,  546, 1580,    3]), array([ 22,  16,  99,  13,  66,  62, 174,  17,   3]), array([ 22,  16,   6,  28,  38, 897,  10,  13,   3]), array([   1,    1,    1,    1,    2,  155, 1524,  163,    3])]\n",
      "X\n",
      "[array([   2,   42,   12,   26,  574,   10, 1284,  367,  107]), array([  2,   7,  33,   6,  43, 347,  32, 118, 953]), array([  2,   7,   6,  69,  16, 141,   5, 309,  86]), array([  2,  33,  37,  11, 430,  18,  10, 199,  14]), array([  2, 109,   6,  43, 366,  71,   6,  43, 447])]\n",
      "Y\n",
      "[array([  42,   12,   26,  574,   10, 1284,  367,  107,    3]), array([  7,  33,   6,  43, 347,  32, 118, 953,   3]), array([  7,   6,  69,  16, 141,   5, 309,  86,   3]), array([ 33,  37,  11, 430,  18,  10, 199,  14,   3]), array([109,   6,  43, 366,  71,   6,  43, 447,   3])]\n",
      "1170/1170 [==============================] - 463s 389ms/step - loss: 2.2084 - top_k_categorical_accuracy: 0.7551 - val_loss: 0.6406 - val_top_k_categorical_accuracy: 0.9397\n",
      "X\n",
      "[array([   2,    4,   45,  305,   77,    6, 4389, 1040,   36]), array([   2, 4067,   49, 1312,   15, 1111,   49,   27,  151]), array([   2,   82,    5,  198,   19,  115,   24, 1182,   10]), array([  2,   4,  41,  15,  36, 340,   6,   7, 340]), array([  2,  58,   4, 489, 658,  77,  10, 154,   6])]\n",
      "Y\n",
      "[array([   4,   45,  305,   77,    6, 4389, 1040,   36,    3]), array([4067,   49, 1312,   15, 1111,   49,   27,  151,    3]), array([  82,    5,  198,   19,  115,   24, 1182,   10,    3]), array([  4,  41,  15,  36, 340,   6,   7, 340,   3]), array([ 58,   4, 489, 658,  77,  10, 154,   6,   3])]\n",
      "X\n",
      "[array([  1,   1,   2,   4, 108,   6,  10, 312,  13]), array([ 1,  1,  1,  1,  1,  2, 48, 24, 34]), array([ 1,  1,  1,  1,  1,  2, 48, 24, 34]), array([   1,    1,    1,    1,    2,   70,   67,   17, 1786]), array([  1,   1,   1,   1,   2, 403,  13, 419, 358])]\n",
      "Y\n",
      "[array([  1,   2,   4, 108,   6,  10, 312,  13,   3]), array([ 1,  1,  1,  1,  2, 48, 24, 34,  3]), array([ 1,  1,  1,  1,  2, 48, 24, 34,  3]), array([   1,    1,    1,    2,   70,   67,   17, 1786,    3]), array([  1,   1,   1,   2, 403,  13, 419, 358,   3])]\n",
      "1170/1170 [==============================] - ETA: 0s - loss: 0.4405 - top_k_categorical_accuracy: 0.9579X\n",
      "[array([ 2,  6, 22, 16, 41, 48, 11, 97, 32]), array([   2,    4,   41,    6,   45,   46,    9,  546, 1580]), array([  2,  22,  16,  99,  13,  66,  62, 174,  17]), array([  2,  22,  16,   6,  28,  38, 897,  10,  13]), array([   1,    1,    1,    1,    1,    2,  155, 1524,  163])]\n",
      "Y\n",
      "[array([ 6, 22, 16, 41, 48, 11, 97, 32,  3]), array([   4,   41,    6,   45,   46,    9,  546, 1580,    3]), array([ 22,  16,  99,  13,  66,  62, 174,  17,   3]), array([ 22,  16,   6,  28,  38, 897,  10,  13,   3]), array([   1,    1,    1,    1,    2,  155, 1524,  163,    3])]\n",
      "X\n",
      "[array([   2,   42,   12,   26,  574,   10, 1284,  367,  107]), array([  2,   7,  33,   6,  43, 347,  32, 118, 953]), array([  2,   7,   6,  69,  16, 141,   5, 309,  86]), array([  2,  33,  37,  11, 430,  18,  10, 199,  14]), array([  2, 109,   6,  43, 366,  71,   6,  43, 447])]\n",
      "Y\n",
      "[array([  42,   12,   26,  574,   10, 1284,  367,  107,    3]), array([  7,  33,   6,  43, 347,  32, 118, 953,   3]), array([  7,   6,  69,  16, 141,   5, 309,  86,   3]), array([ 33,  37,  11, 430,  18,  10, 199,  14,   3]), array([109,   6,  43, 366,  71,   6,  43, 447,   3])]\n",
      "1170/1170 [==============================] - 443s 378ms/step - loss: 0.4405 - top_k_categorical_accuracy: 0.9579 - val_loss: 0.2876 - val_top_k_categorical_accuracy: 0.9742\n",
      "X\n",
      "[array([   2,    4,   45,  305,   77,    6, 4389, 1040,   36]), array([   2, 4067,   49, 1312,   15, 1111,   49,   27,  151]), array([   2,   82,    5,  198,   19,  115,   24, 1182,   10]), array([  2,   4,  41,  15,  36, 340,   6,   7, 340]), array([  2,  58,   4, 489, 658,  77,  10, 154,   6])]\n",
      "Y\n",
      "[array([   4,   45,  305,   77,    6, 4389, 1040,   36,    3]), array([4067,   49, 1312,   15, 1111,   49,   27,  151,    3]), array([  82,    5,  198,   19,  115,   24, 1182,   10,    3]), array([  4,  41,  15,  36, 340,   6,   7, 340,   3]), array([ 58,   4, 489, 658,  77,  10, 154,   6,   3])]\n",
      "X\n",
      "[array([  1,   1,   2,   4, 108,   6,  10, 312,  13]), array([ 1,  1,  1,  1,  1,  2, 48, 24, 34]), array([ 1,  1,  1,  1,  1,  2, 48, 24, 34]), array([   1,    1,    1,    1,    2,   70,   67,   17, 1786]), array([  1,   1,   1,   1,   2, 403,  13, 419, 358])]\n",
      "Y\n",
      "[array([  1,   2,   4, 108,   6,  10, 312,  13,   3]), array([ 1,  1,  1,  1,  2, 48, 24, 34,  3]), array([ 1,  1,  1,  1,  2, 48, 24, 34,  3]), array([   1,    1,    1,    2,   70,   67,   17, 1786,    3]), array([  1,   1,   1,   2, 403,  13, 419, 358,   3])]\n",
      "1170/1170 [==============================] - ETA: 0s - loss: 0.2017 - top_k_categorical_accuracy: 0.9816X\n",
      "[array([ 2,  6, 22, 16, 41, 48, 11, 97, 32]), array([   2,    4,   41,    6,   45,   46,    9,  546, 1580]), array([  2,  22,  16,  99,  13,  66,  62, 174,  17]), array([  2,  22,  16,   6,  28,  38, 897,  10,  13]), array([   1,    1,    1,    1,    1,    2,  155, 1524,  163])]\n",
      "Y\n",
      "[array([ 6, 22, 16, 41, 48, 11, 97, 32,  3]), array([   4,   41,    6,   45,   46,    9,  546, 1580,    3]), array([ 22,  16,  99,  13,  66,  62, 174,  17,   3]), array([ 22,  16,   6,  28,  38, 897,  10,  13,   3]), array([   1,    1,    1,    1,    2,  155, 1524,  163,    3])]\n",
      "X\n",
      "[array([   2,   42,   12,   26,  574,   10, 1284,  367,  107]), array([  2,   7,  33,   6,  43, 347,  32, 118, 953]), array([  2,   7,   6,  69,  16, 141,   5, 309,  86]), array([  2,  33,  37,  11, 430,  18,  10, 199,  14]), array([  2, 109,   6,  43, 366,  71,   6,  43, 447])]\n",
      "Y\n",
      "[array([  42,   12,   26,  574,   10, 1284,  367,  107,    3]), array([  7,  33,   6,  43, 347,  32, 118, 953,   3]), array([  7,   6,  69,  16, 141,   5, 309,  86,   3]), array([ 33,  37,  11, 430,  18,  10, 199,  14,   3]), array([109,   6,  43, 366,  71,   6,  43, 447,   3])]\n",
      "1170/1170 [==============================] - 434s 371ms/step - loss: 0.2017 - top_k_categorical_accuracy: 0.9816 - val_loss: 0.1867 - val_top_k_categorical_accuracy: 0.9824\n",
      "X\n",
      "[array([   2,    4,   45,  305,   77,    6, 4389, 1040,   36]), array([   2, 4067,   49, 1312,   15, 1111,   49,   27,  151]), array([   2,   82,    5,  198,   19,  115,   24, 1182,   10]), array([  2,   4,  41,  15,  36, 340,   6,   7, 340]), array([  2,  58,   4, 489, 658,  77,  10, 154,   6])]\n",
      "Y\n",
      "[array([   4,   45,  305,   77,    6, 4389, 1040,   36,    3]), array([4067,   49, 1312,   15, 1111,   49,   27,  151,    3]), array([  82,    5,  198,   19,  115,   24, 1182,   10,    3]), array([  4,  41,  15,  36, 340,   6,   7, 340,   3]), array([ 58,   4, 489, 658,  77,  10, 154,   6,   3])]\n",
      "X\n",
      "[array([  1,   1,   2,   4, 108,   6,  10, 312,  13]), array([ 1,  1,  1,  1,  1,  2, 48, 24, 34]), array([ 1,  1,  1,  1,  1,  2, 48, 24, 34]), array([   1,    1,    1,    1,    2,   70,   67,   17, 1786]), array([  1,   1,   1,   1,   2, 403,  13, 419, 358])]\n",
      "Y\n",
      "[array([  1,   2,   4, 108,   6,  10, 312,  13,   3]), array([ 1,  1,  1,  1,  2, 48, 24, 34,  3]), array([ 1,  1,  1,  1,  2, 48, 24, 34,  3]), array([   1,    1,    1,    2,   70,   67,   17, 1786,    3]), array([  1,   1,   1,   2, 403,  13, 419, 358,   3])]\n",
      "1170/1170 [==============================] - ETA: 0s - loss: 0.0913 - top_k_categorical_accuracy: 0.9946X\n",
      "[array([ 2,  6, 22, 16, 41, 48, 11, 97, 32]), array([   2,    4,   41,    6,   45,   46,    9,  546, 1580]), array([  2,  22,  16,  99,  13,  66,  62, 174,  17]), array([  2,  22,  16,   6,  28,  38, 897,  10,  13]), array([   1,    1,    1,    1,    1,    2,  155, 1524,  163])]\n",
      "Y\n",
      "[array([ 6, 22, 16, 41, 48, 11, 97, 32,  3]), array([   4,   41,    6,   45,   46,    9,  546, 1580,    3]), array([ 22,  16,  99,  13,  66,  62, 174,  17,   3]), array([ 22,  16,   6,  28,  38, 897,  10,  13,   3]), array([   1,    1,    1,    1,    2,  155, 1524,  163,    3])]\n",
      "X\n",
      "[array([   2,   42,   12,   26,  574,   10, 1284,  367,  107]), array([  2,   7,  33,   6,  43, 347,  32, 118, 953]), array([  2,   7,   6,  69,  16, 141,   5, 309,  86]), array([  2,  33,  37,  11, 430,  18,  10, 199,  14]), array([  2, 109,   6,  43, 366,  71,   6,  43, 447])]\n",
      "Y\n",
      "[array([  42,   12,   26,  574,   10, 1284,  367,  107,    3]), array([  7,  33,   6,  43, 347,  32, 118, 953,   3]), array([  7,   6,  69,  16, 141,   5, 309,  86,   3]), array([ 33,  37,  11, 430,  18,  10, 199,  14,   3]), array([109,   6,  43, 366,  71,   6,  43, 447,   3])]\n",
      "1170/1170 [==============================] - 411s 351ms/step - loss: 0.0913 - top_k_categorical_accuracy: 0.9946 - val_loss: 0.1439 - val_top_k_categorical_accuracy: 0.9860\n",
      "X\n",
      "[array([   2,    4,   45,  305,   77,    6, 4389, 1040,   36]), array([   2, 4067,   49, 1312,   15, 1111,   49,   27,  151]), array([   2,   82,    5,  198,   19,  115,   24, 1182,   10]), array([  2,   4,  41,  15,  36, 340,   6,   7, 340]), array([  2,  58,   4, 489, 658,  77,  10, 154,   6])]\n",
      "Y\n",
      "[array([   4,   45,  305,   77,    6, 4389, 1040,   36,    3]), array([4067,   49, 1312,   15, 1111,   49,   27,  151,    3]), array([  82,    5,  198,   19,  115,   24, 1182,   10,    3]), array([  4,  41,  15,  36, 340,   6,   7, 340,   3]), array([ 58,   4, 489, 658,  77,  10, 154,   6,   3])]\n",
      "X\n",
      "[array([  1,   1,   2,   4, 108,   6,  10, 312,  13]), array([ 1,  1,  1,  1,  1,  2, 48, 24, 34]), array([ 1,  1,  1,  1,  1,  2, 48, 24, 34]), array([   1,    1,    1,    1,    2,   70,   67,   17, 1786]), array([  1,   1,   1,   1,   2, 403,  13, 419, 358])]\n",
      "Y\n",
      "[array([  1,   2,   4, 108,   6,  10, 312,  13,   3]), array([ 1,  1,  1,  1,  2, 48, 24, 34,  3]), array([ 1,  1,  1,  1,  2, 48, 24, 34,  3]), array([   1,    1,    1,    2,   70,   67,   17, 1786,    3]), array([  1,   1,   1,   2, 403,  13, 419, 358,   3])]\n",
      "1170/1170 [==============================] - ETA: 0s - loss: 0.0391 - top_k_categorical_accuracy: 0.9990X\n",
      "[array([ 2,  6, 22, 16, 41, 48, 11, 97, 32]), array([   2,    4,   41,    6,   45,   46,    9,  546, 1580]), array([  2,  22,  16,  99,  13,  66,  62, 174,  17]), array([  2,  22,  16,   6,  28,  38, 897,  10,  13]), array([   1,    1,    1,    1,    1,    2,  155, 1524,  163])]\n",
      "Y\n",
      "[array([ 6, 22, 16, 41, 48, 11, 97, 32,  3]), array([   4,   41,    6,   45,   46,    9,  546, 1580,    3]), array([ 22,  16,  99,  13,  66,  62, 174,  17,   3]), array([ 22,  16,   6,  28,  38, 897,  10,  13,   3]), array([   1,    1,    1,    1,    2,  155, 1524,  163,    3])]\n",
      "X\n",
      "[array([   2,   42,   12,   26,  574,   10, 1284,  367,  107]), array([  2,   7,  33,   6,  43, 347,  32, 118, 953]), array([  2,   7,   6,  69,  16, 141,   5, 309,  86]), array([  2,  33,  37,  11, 430,  18,  10, 199,  14]), array([  2, 109,   6,  43, 366,  71,   6,  43, 447])]\n",
      "Y\n",
      "[array([  42,   12,   26,  574,   10, 1284,  367,  107,    3]), array([  7,  33,   6,  43, 347,  32, 118, 953,   3]), array([  7,   6,  69,  16, 141,   5, 309,  86,   3]), array([ 33,  37,  11, 430,  18,  10, 199,  14,   3]), array([109,   6,  43, 366,  71,   6,  43, 447,   3])]\n",
      "1170/1170 [==============================] - 300s 257ms/step - loss: 0.0391 - top_k_categorical_accuracy: 0.9990 - val_loss: 0.1209 - val_top_k_categorical_accuracy: 0.9879\n",
      "Epoch 5\n",
      "X\n",
      "[array([   2,    4,   45,  305,   77,    6, 4389, 1040,   36]), array([   2, 4067,   49, 1312,   15, 1111,   49,   27,  151]), array([   2,   82,    5,  198,   19,  115,   24, 1182,   10]), array([  2,   4,  41,  15,  36, 340,   6,   7, 340]), array([  2,  58,   4, 489, 658,  77,  10, 154,   6])]\n",
      "Y\n",
      "[array([   4,   45,  305,   77,    6, 4389, 1040,   36,    3]), array([4067,   49, 1312,   15, 1111,   49,   27,  151,    3]), array([  82,    5,  198,   19,  115,   24, 1182,   10,    3]), array([  4,  41,  15,  36, 340,   6,   7, 340,   3]), array([ 58,   4, 489, 658,  77,  10, 154,   6,   3])]\n",
      "X\n",
      "[array([  1,   1,   2,   4, 108,   6,  10, 312,  13]), array([ 1,  1,  1,  1,  1,  2, 48, 24, 34]), array([ 1,  1,  1,  1,  1,  2, 48, 24, 34]), array([   1,    1,    1,    1,    2,   70,   67,   17, 1786]), array([  1,   1,   1,   1,   2, 403,  13, 419, 358])]\n",
      "Y\n",
      "[array([  1,   2,   4, 108,   6,  10, 312,  13,   3]), array([ 1,  1,  1,  1,  2, 48, 24, 34,  3]), array([ 1,  1,  1,  1,  2, 48, 24, 34,  3]), array([   1,    1,    1,    2,   70,   67,   17, 1786,    3]), array([  1,   1,   1,   2, 403,  13, 419, 358,   3])]\n",
      "1170/1170 [==============================] - ETA: 0s - loss: 0.0161 - top_k_categorical_accuracy: 0.9999X\n",
      "[array([ 2,  6, 22, 16, 41, 48, 11, 97, 32]), array([   2,    4,   41,    6,   45,   46,    9,  546, 1580]), array([  2,  22,  16,  99,  13,  66,  62, 174,  17]), array([  2,  22,  16,   6,  28,  38, 897,  10,  13]), array([   1,    1,    1,    1,    1,    2,  155, 1524,  163])]\n",
      "Y\n",
      "[array([ 6, 22, 16, 41, 48, 11, 97, 32,  3]), array([   4,   41,    6,   45,   46,    9,  546, 1580,    3]), array([ 22,  16,  99,  13,  66,  62, 174,  17,   3]), array([ 22,  16,   6,  28,  38, 897,  10,  13,   3]), array([   1,    1,    1,    1,    2,  155, 1524,  163,    3])]\n",
      "X\n",
      "[array([   2,   42,   12,   26,  574,   10, 1284,  367,  107]), array([  2,   7,  33,   6,  43, 347,  32, 118, 953]), array([  2,   7,   6,  69,  16, 141,   5, 309,  86]), array([  2,  33,  37,  11, 430,  18,  10, 199,  14]), array([  2, 109,   6,  43, 366,  71,   6,  43, 447])]\n",
      "Y\n",
      "[array([  42,   12,   26,  574,   10, 1284,  367,  107,    3]), array([  7,  33,   6,  43, 347,  32, 118, 953,   3]), array([  7,   6,  69,  16, 141,   5, 309,  86,   3]), array([ 33,  37,  11, 430,  18,  10, 199,  14,   3]), array([109,   6,  43, 366,  71,   6,  43, 447,   3])]\n",
      "1170/1170 [==============================] - 301s 257ms/step - loss: 0.0161 - top_k_categorical_accuracy: 0.9999 - val_loss: 0.1081 - val_top_k_categorical_accuracy: 0.9889\n",
      "X\n",
      "[array([   2,    4,   45,  305,   77,    6, 4389, 1040,   36]), array([   2, 4067,   49, 1312,   15, 1111,   49,   27,  151]), array([   2,   82,    5,  198,   19,  115,   24, 1182,   10]), array([  2,   4,  41,  15,  36, 340,   6,   7, 340]), array([  2,  58,   4, 489, 658,  77,  10, 154,   6])]\n",
      "Y\n",
      "[array([   4,   45,  305,   77,    6, 4389, 1040,   36,    3]), array([4067,   49, 1312,   15, 1111,   49,   27,  151,    3]), array([  82,    5,  198,   19,  115,   24, 1182,   10,    3]), array([  4,  41,  15,  36, 340,   6,   7, 340,   3]), array([ 58,   4, 489, 658,  77,  10, 154,   6,   3])]\n",
      "X\n",
      "[array([  1,   1,   2,   4, 108,   6,  10, 312,  13]), array([ 1,  1,  1,  1,  1,  2, 48, 24, 34]), array([ 1,  1,  1,  1,  1,  2, 48, 24, 34]), array([   1,    1,    1,    1,    2,   70,   67,   17, 1786]), array([  1,   1,   1,   1,   2, 403,  13, 419, 358])]\n",
      "Y\n",
      "[array([  1,   2,   4, 108,   6,  10, 312,  13,   3]), array([ 1,  1,  1,  1,  2, 48, 24, 34,  3]), array([ 1,  1,  1,  1,  2, 48, 24, 34,  3]), array([   1,    1,    1,    2,   70,   67,   17, 1786,    3]), array([  1,   1,   1,   2, 403,  13, 419, 358,   3])]\n",
      "1170/1170 [==============================] - ETA: 0s - loss: 0.0074 - top_k_categorical_accuracy: 1.0000X\n",
      "[array([ 2,  6, 22, 16, 41, 48, 11, 97, 32]), array([   2,    4,   41,    6,   45,   46,    9,  546, 1580]), array([  2,  22,  16,  99,  13,  66,  62, 174,  17]), array([  2,  22,  16,   6,  28,  38, 897,  10,  13]), array([   1,    1,    1,    1,    1,    2,  155, 1524,  163])]\n",
      "Y\n",
      "[array([ 6, 22, 16, 41, 48, 11, 97, 32,  3]), array([   4,   41,    6,   45,   46,    9,  546, 1580,    3]), array([ 22,  16,  99,  13,  66,  62, 174,  17,   3]), array([ 22,  16,   6,  28,  38, 897,  10,  13,   3]), array([   1,    1,    1,    1,    2,  155, 1524,  163,    3])]\n",
      "X\n",
      "[array([   2,   42,   12,   26,  574,   10, 1284,  367,  107]), array([  2,   7,  33,   6,  43, 347,  32, 118, 953]), array([  2,   7,   6,  69,  16, 141,   5, 309,  86]), array([  2,  33,  37,  11, 430,  18,  10, 199,  14]), array([  2, 109,   6,  43, 366,  71,   6,  43, 447])]\n",
      "Y\n",
      "[array([  42,   12,   26,  574,   10, 1284,  367,  107,    3]), array([  7,  33,   6,  43, 347,  32, 118, 953,   3]), array([  7,   6,  69,  16, 141,   5, 309,  86,   3]), array([ 33,  37,  11, 430,  18,  10, 199,  14,   3]), array([109,   6,  43, 366,  71,   6,  43, 447,   3])]\n",
      "1170/1170 [==============================] - 300s 256ms/step - loss: 0.0074 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.1012 - val_top_k_categorical_accuracy: 0.9894\n",
      "X\n",
      "[array([   2,    4,   45,  305,   77,    6, 4389, 1040,   36]), array([   2, 4067,   49, 1312,   15, 1111,   49,   27,  151]), array([   2,   82,    5,  198,   19,  115,   24, 1182,   10]), array([  2,   4,  41,  15,  36, 340,   6,   7, 340]), array([  2,  58,   4, 489, 658,  77,  10, 154,   6])]\n",
      "Y\n",
      "[array([   4,   45,  305,   77,    6, 4389, 1040,   36,    3]), array([4067,   49, 1312,   15, 1111,   49,   27,  151,    3]), array([  82,    5,  198,   19,  115,   24, 1182,   10,    3]), array([  4,  41,  15,  36, 340,   6,   7, 340,   3]), array([ 58,   4, 489, 658,  77,  10, 154,   6,   3])]\n",
      "X\n",
      "[array([  1,   1,   2,   4, 108,   6,  10, 312,  13]), array([ 1,  1,  1,  1,  1,  2, 48, 24, 34]), array([ 1,  1,  1,  1,  1,  2, 48, 24, 34]), array([   1,    1,    1,    1,    2,   70,   67,   17, 1786]), array([  1,   1,   1,   1,   2, 403,  13, 419, 358])]\n",
      "Y\n",
      "[array([  1,   2,   4, 108,   6,  10, 312,  13,   3]), array([ 1,  1,  1,  1,  2, 48, 24, 34,  3]), array([ 1,  1,  1,  1,  2, 48, 24, 34,  3]), array([   1,    1,    1,    2,   70,   67,   17, 1786,    3]), array([  1,   1,   1,   2, 403,  13, 419, 358,   3])]\n",
      "1170/1170 [==============================] - ETA: 0s - loss: 0.0043 - top_k_categorical_accuracy: 1.0000X\n",
      "[array([ 2,  6, 22, 16, 41, 48, 11, 97, 32]), array([   2,    4,   41,    6,   45,   46,    9,  546, 1580]), array([  2,  22,  16,  99,  13,  66,  62, 174,  17]), array([  2,  22,  16,   6,  28,  38, 897,  10,  13]), array([   1,    1,    1,    1,    1,    2,  155, 1524,  163])]\n",
      "Y\n",
      "[array([ 6, 22, 16, 41, 48, 11, 97, 32,  3]), array([   4,   41,    6,   45,   46,    9,  546, 1580,    3]), array([ 22,  16,  99,  13,  66,  62, 174,  17,   3]), array([ 22,  16,   6,  28,  38, 897,  10,  13,   3]), array([   1,    1,    1,    1,    2,  155, 1524,  163,    3])]\n",
      "X\n",
      "[array([   2,   42,   12,   26,  574,   10, 1284,  367,  107]), array([  2,   7,  33,   6,  43, 347,  32, 118, 953]), array([  2,   7,   6,  69,  16, 141,   5, 309,  86]), array([  2,  33,  37,  11, 430,  18,  10, 199,  14]), array([  2, 109,   6,  43, 366,  71,   6,  43, 447])]\n",
      "Y\n",
      "[array([  42,   12,   26,  574,   10, 1284,  367,  107,    3]), array([  7,  33,   6,  43, 347,  32, 118, 953,   3]), array([  7,   6,  69,  16, 141,   5, 309,  86,   3]), array([ 33,  37,  11, 430,  18,  10, 199,  14,   3]), array([109,   6,  43, 366,  71,   6,  43, 447,   3])]\n",
      "1170/1170 [==============================] - 299s 256ms/step - loss: 0.0043 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0956 - val_top_k_categorical_accuracy: 0.9898\n",
      "X\n",
      "[array([   2,    4,   45,  305,   77,    6, 4389, 1040,   36]), array([   2, 4067,   49, 1312,   15, 1111,   49,   27,  151]), array([   2,   82,    5,  198,   19,  115,   24, 1182,   10]), array([  2,   4,  41,  15,  36, 340,   6,   7, 340]), array([  2,  58,   4, 489, 658,  77,  10, 154,   6])]\n",
      "Y\n",
      "[array([   4,   45,  305,   77,    6, 4389, 1040,   36,    3]), array([4067,   49, 1312,   15, 1111,   49,   27,  151,    3]), array([  82,    5,  198,   19,  115,   24, 1182,   10,    3]), array([  4,  41,  15,  36, 340,   6,   7, 340,   3]), array([ 58,   4, 489, 658,  77,  10, 154,   6,   3])]\n",
      "X\n",
      "[array([  1,   1,   2,   4, 108,   6,  10, 312,  13]), array([ 1,  1,  1,  1,  1,  2, 48, 24, 34]), array([ 1,  1,  1,  1,  1,  2, 48, 24, 34]), array([   1,    1,    1,    1,    2,   70,   67,   17, 1786]), array([  1,   1,   1,   1,   2, 403,  13, 419, 358])]\n",
      "Y\n",
      "[array([  1,   2,   4, 108,   6,  10, 312,  13,   3]), array([ 1,  1,  1,  1,  2, 48, 24, 34,  3]), array([ 1,  1,  1,  1,  2, 48, 24, 34,  3]), array([   1,    1,    1,    2,   70,   67,   17, 1786,    3]), array([  1,   1,   1,   2, 403,  13, 419, 358,   3])]\n",
      "1170/1170 [==============================] - ETA: 0s - loss: 0.0032 - top_k_categorical_accuracy: 1.0000X\n",
      "[array([ 2,  6, 22, 16, 41, 48, 11, 97, 32]), array([   2,    4,   41,    6,   45,   46,    9,  546, 1580]), array([  2,  22,  16,  99,  13,  66,  62, 174,  17]), array([  2,  22,  16,   6,  28,  38, 897,  10,  13]), array([   1,    1,    1,    1,    1,    2,  155, 1524,  163])]\n",
      "Y\n",
      "[array([ 6, 22, 16, 41, 48, 11, 97, 32,  3]), array([   4,   41,    6,   45,   46,    9,  546, 1580,    3]), array([ 22,  16,  99,  13,  66,  62, 174,  17,   3]), array([ 22,  16,   6,  28,  38, 897,  10,  13,   3]), array([   1,    1,    1,    1,    2,  155, 1524,  163,    3])]\n",
      "X\n",
      "[array([   2,   42,   12,   26,  574,   10, 1284,  367,  107]), array([  2,   7,  33,   6,  43, 347,  32, 118, 953]), array([  2,   7,   6,  69,  16, 141,   5, 309,  86]), array([  2,  33,  37,  11, 430,  18,  10, 199,  14]), array([  2, 109,   6,  43, 366,  71,   6,  43, 447])]\n",
      "Y\n",
      "[array([  42,   12,   26,  574,   10, 1284,  367,  107,    3]), array([  7,  33,   6,  43, 347,  32, 118, 953,   3]), array([  7,   6,  69,  16, 141,   5, 309,  86,   3]), array([ 33,  37,  11, 430,  18,  10, 199,  14,   3]), array([109,   6,  43, 366,  71,   6,  43, 447,   3])]\n",
      "1170/1170 [==============================] - 295s 252ms/step - loss: 0.0032 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0932 - val_top_k_categorical_accuracy: 0.9901\n",
      "X\n",
      "[array([   2,    4,   45,  305,   77,    6, 4389, 1040,   36]), array([   2, 4067,   49, 1312,   15, 1111,   49,   27,  151]), array([   2,   82,    5,  198,   19,  115,   24, 1182,   10]), array([  2,   4,  41,  15,  36, 340,   6,   7, 340]), array([  2,  58,   4, 489, 658,  77,  10, 154,   6])]\n",
      "Y\n",
      "[array([   4,   45,  305,   77,    6, 4389, 1040,   36,    3]), array([4067,   49, 1312,   15, 1111,   49,   27,  151,    3]), array([  82,    5,  198,   19,  115,   24, 1182,   10,    3]), array([  4,  41,  15,  36, 340,   6,   7, 340,   3]), array([ 58,   4, 489, 658,  77,  10, 154,   6,   3])]\n",
      "X\n",
      "[array([  1,   1,   2,   4, 108,   6,  10, 312,  13]), array([ 1,  1,  1,  1,  1,  2, 48, 24, 34]), array([ 1,  1,  1,  1,  1,  2, 48, 24, 34]), array([   1,    1,    1,    1,    2,   70,   67,   17, 1786]), array([  1,   1,   1,   1,   2, 403,  13, 419, 358])]\n",
      "Y\n",
      "[array([  1,   2,   4, 108,   6,  10, 312,  13,   3]), array([ 1,  1,  1,  1,  2, 48, 24, 34,  3]), array([ 1,  1,  1,  1,  2, 48, 24, 34,  3]), array([   1,    1,    1,    2,   70,   67,   17, 1786,    3]), array([  1,   1,   1,   2, 403,  13, 419, 358,   3])]\n",
      "1170/1170 [==============================] - ETA: 0s - loss: 0.0021 - top_k_categorical_accuracy: 1.0000X\n",
      "[array([ 2,  6, 22, 16, 41, 48, 11, 97, 32]), array([   2,    4,   41,    6,   45,   46,    9,  546, 1580]), array([  2,  22,  16,  99,  13,  66,  62, 174,  17]), array([  2,  22,  16,   6,  28,  38, 897,  10,  13]), array([   1,    1,    1,    1,    1,    2,  155, 1524,  163])]\n",
      "Y\n",
      "[array([ 6, 22, 16, 41, 48, 11, 97, 32,  3]), array([   4,   41,    6,   45,   46,    9,  546, 1580,    3]), array([ 22,  16,  99,  13,  66,  62, 174,  17,   3]), array([ 22,  16,   6,  28,  38, 897,  10,  13,   3]), array([   1,    1,    1,    1,    2,  155, 1524,  163,    3])]\n",
      "X\n",
      "[array([   2,   42,   12,   26,  574,   10, 1284,  367,  107]), array([  2,   7,  33,   6,  43, 347,  32, 118, 953]), array([  2,   7,   6,  69,  16, 141,   5, 309,  86]), array([  2,  33,  37,  11, 430,  18,  10, 199,  14]), array([  2, 109,   6,  43, 366,  71,   6,  43, 447])]\n",
      "Y\n",
      "[array([  42,   12,   26,  574,   10, 1284,  367,  107,    3]), array([  7,  33,   6,  43, 347,  32, 118, 953,   3]), array([  7,   6,  69,  16, 141,   5, 309,  86,   3]), array([ 33,  37,  11, 430,  18,  10, 199,  14,   3]), array([109,   6,  43, 366,  71,   6,  43, 447,   3])]\n",
      "1170/1170 [==============================] - 300s 256ms/step - loss: 0.0021 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0901 - val_top_k_categorical_accuracy: 0.9905\n",
      "Epoch 10\n",
      "X\n",
      "[array([   2,    4,   45,  305,   77,    6, 4389, 1040,   36]), array([   2, 4067,   49, 1312,   15, 1111,   49,   27,  151]), array([   2,   82,    5,  198,   19,  115,   24, 1182,   10]), array([  2,   4,  41,  15,  36, 340,   6,   7, 340]), array([  2,  58,   4, 489, 658,  77,  10, 154,   6])]\n",
      "Y\n",
      "[array([   4,   45,  305,   77,    6, 4389, 1040,   36,    3]), array([4067,   49, 1312,   15, 1111,   49,   27,  151,    3]), array([  82,    5,  198,   19,  115,   24, 1182,   10,    3]), array([  4,  41,  15,  36, 340,   6,   7, 340,   3]), array([ 58,   4, 489, 658,  77,  10, 154,   6,   3])]\n",
      "X\n",
      "[array([  1,   1,   2,   4, 108,   6,  10, 312,  13]), array([ 1,  1,  1,  1,  1,  2, 48, 24, 34]), array([ 1,  1,  1,  1,  1,  2, 48, 24, 34]), array([   1,    1,    1,    1,    2,   70,   67,   17, 1786]), array([  1,   1,   1,   1,   2, 403,  13, 419, 358])]\n",
      "Y\n",
      "[array([  1,   2,   4, 108,   6,  10, 312,  13,   3]), array([ 1,  1,  1,  1,  2, 48, 24, 34,  3]), array([ 1,  1,  1,  1,  2, 48, 24, 34,  3]), array([   1,    1,    1,    2,   70,   67,   17, 1786,    3]), array([  1,   1,   1,   2, 403,  13, 419, 358,   3])]\n",
      "1170/1170 [==============================] - ETA: 0s - loss: 0.0016 - top_k_categorical_accuracy: 1.0000X\n",
      "[array([ 2,  6, 22, 16, 41, 48, 11, 97, 32]), array([   2,    4,   41,    6,   45,   46,    9,  546, 1580]), array([  2,  22,  16,  99,  13,  66,  62, 174,  17]), array([  2,  22,  16,   6,  28,  38, 897,  10,  13]), array([   1,    1,    1,    1,    1,    2,  155, 1524,  163])]\n",
      "Y\n",
      "[array([ 6, 22, 16, 41, 48, 11, 97, 32,  3]), array([   4,   41,    6,   45,   46,    9,  546, 1580,    3]), array([ 22,  16,  99,  13,  66,  62, 174,  17,   3]), array([ 22,  16,   6,  28,  38, 897,  10,  13,   3]), array([   1,    1,    1,    1,    2,  155, 1524,  163,    3])]\n",
      "X\n",
      "[array([   2,   42,   12,   26,  574,   10, 1284,  367,  107]), array([  2,   7,  33,   6,  43, 347,  32, 118, 953]), array([  2,   7,   6,  69,  16, 141,   5, 309,  86]), array([  2,  33,  37,  11, 430,  18,  10, 199,  14]), array([  2, 109,   6,  43, 366,  71,   6,  43, 447])]\n",
      "Y\n",
      "[array([  42,   12,   26,  574,   10, 1284,  367,  107,    3]), array([  7,  33,   6,  43, 347,  32, 118, 953,   3]), array([  7,   6,  69,  16, 141,   5, 309,  86,   3]), array([ 33,  37,  11, 430,  18,  10, 199,  14,   3]), array([109,   6,  43, 366,  71,   6,  43, 447,   3])]\n",
      "1170/1170 [==============================] - 300s 257ms/step - loss: 0.0016 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0853 - val_top_k_categorical_accuracy: 0.9909\n",
      "X\n",
      "[array([   2,    4,   45,  305,   77,    6, 4389, 1040,   36]), array([   2, 4067,   49, 1312,   15, 1111,   49,   27,  151]), array([   2,   82,    5,  198,   19,  115,   24, 1182,   10]), array([  2,   4,  41,  15,  36, 340,   6,   7, 340]), array([  2,  58,   4, 489, 658,  77,  10, 154,   6])]\n",
      "Y\n",
      "[array([   4,   45,  305,   77,    6, 4389, 1040,   36,    3]), array([4067,   49, 1312,   15, 1111,   49,   27,  151,    3]), array([  82,    5,  198,   19,  115,   24, 1182,   10,    3]), array([  4,  41,  15,  36, 340,   6,   7, 340,   3]), array([ 58,   4, 489, 658,  77,  10, 154,   6,   3])]\n",
      "X\n",
      "[array([  1,   1,   2,   4, 108,   6,  10, 312,  13]), array([ 1,  1,  1,  1,  1,  2, 48, 24, 34]), array([ 1,  1,  1,  1,  1,  2, 48, 24, 34]), array([   1,    1,    1,    1,    2,   70,   67,   17, 1786]), array([  1,   1,   1,   1,   2, 403,  13, 419, 358])]\n",
      "Y\n",
      "[array([  1,   2,   4, 108,   6,  10, 312,  13,   3]), array([ 1,  1,  1,  1,  2, 48, 24, 34,  3]), array([ 1,  1,  1,  1,  2, 48, 24, 34,  3]), array([   1,    1,    1,    2,   70,   67,   17, 1786,    3]), array([  1,   1,   1,   2, 403,  13, 419, 358,   3])]\n",
      "1170/1170 [==============================] - ETA: 0s - loss: 0.0014 - top_k_categorical_accuracy: 1.0000X\n",
      "[array([ 2,  6, 22, 16, 41, 48, 11, 97, 32]), array([   2,    4,   41,    6,   45,   46,    9,  546, 1580]), array([  2,  22,  16,  99,  13,  66,  62, 174,  17]), array([  2,  22,  16,   6,  28,  38, 897,  10,  13]), array([   1,    1,    1,    1,    1,    2,  155, 1524,  163])]\n",
      "Y\n",
      "[array([ 6, 22, 16, 41, 48, 11, 97, 32,  3]), array([   4,   41,    6,   45,   46,    9,  546, 1580,    3]), array([ 22,  16,  99,  13,  66,  62, 174,  17,   3]), array([ 22,  16,   6,  28,  38, 897,  10,  13,   3]), array([   1,    1,    1,    1,    2,  155, 1524,  163,    3])]\n",
      "X\n",
      "[array([   2,   42,   12,   26,  574,   10, 1284,  367,  107]), array([  2,   7,  33,   6,  43, 347,  32, 118, 953]), array([  2,   7,   6,  69,  16, 141,   5, 309,  86]), array([  2,  33,  37,  11, 430,  18,  10, 199,  14]), array([  2, 109,   6,  43, 366,  71,   6,  43, 447])]\n",
      "Y\n",
      "[array([  42,   12,   26,  574,   10, 1284,  367,  107,    3]), array([  7,  33,   6,  43, 347,  32, 118, 953,   3]), array([  7,   6,  69,  16, 141,   5, 309,  86,   3]), array([ 33,  37,  11, 430,  18,  10, 199,  14,   3]), array([109,   6,  43, 366,  71,   6,  43, 447,   3])]\n",
      "1170/1170 [==============================] - 300s 256ms/step - loss: 0.0014 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0908 - val_top_k_categorical_accuracy: 0.9904\n",
      "X\n",
      "[array([   2,    4,   45,  305,   77,    6, 4389, 1040,   36]), array([   2, 4067,   49, 1312,   15, 1111,   49,   27,  151]), array([   2,   82,    5,  198,   19,  115,   24, 1182,   10]), array([  2,   4,  41,  15,  36, 340,   6,   7, 340]), array([  2,  58,   4, 489, 658,  77,  10, 154,   6])]\n",
      "Y\n",
      "[array([   4,   45,  305,   77,    6, 4389, 1040,   36,    3]), array([4067,   49, 1312,   15, 1111,   49,   27,  151,    3]), array([  82,    5,  198,   19,  115,   24, 1182,   10,    3]), array([  4,  41,  15,  36, 340,   6,   7, 340,   3]), array([ 58,   4, 489, 658,  77,  10, 154,   6,   3])]\n",
      "X\n",
      "[array([  1,   1,   2,   4, 108,   6,  10, 312,  13]), array([ 1,  1,  1,  1,  1,  2, 48, 24, 34]), array([ 1,  1,  1,  1,  1,  2, 48, 24, 34]), array([   1,    1,    1,    1,    2,   70,   67,   17, 1786]), array([  1,   1,   1,   1,   2, 403,  13, 419, 358])]\n",
      "Y\n",
      "[array([  1,   2,   4, 108,   6,  10, 312,  13,   3]), array([ 1,  1,  1,  1,  2, 48, 24, 34,  3]), array([ 1,  1,  1,  1,  2, 48, 24, 34,  3]), array([   1,    1,    1,    2,   70,   67,   17, 1786,    3]), array([  1,   1,   1,   2, 403,  13, 419, 358,   3])]\n",
      "1170/1170 [==============================] - ETA: 0s - loss: 0.0012 - top_k_categorical_accuracy: 1.0000X\n",
      "[array([ 2,  6, 22, 16, 41, 48, 11, 97, 32]), array([   2,    4,   41,    6,   45,   46,    9,  546, 1580]), array([  2,  22,  16,  99,  13,  66,  62, 174,  17]), array([  2,  22,  16,   6,  28,  38, 897,  10,  13]), array([   1,    1,    1,    1,    1,    2,  155, 1524,  163])]\n",
      "Y\n",
      "[array([ 6, 22, 16, 41, 48, 11, 97, 32,  3]), array([   4,   41,    6,   45,   46,    9,  546, 1580,    3]), array([ 22,  16,  99,  13,  66,  62, 174,  17,   3]), array([ 22,  16,   6,  28,  38, 897,  10,  13,   3]), array([   1,    1,    1,    1,    2,  155, 1524,  163,    3])]\n",
      "X\n",
      "[array([   2,   42,   12,   26,  574,   10, 1284,  367,  107]), array([  2,   7,  33,   6,  43, 347,  32, 118, 953]), array([  2,   7,   6,  69,  16, 141,   5, 309,  86]), array([  2,  33,  37,  11, 430,  18,  10, 199,  14]), array([  2, 109,   6,  43, 366,  71,   6,  43, 447])]\n",
      "Y\n",
      "[array([  42,   12,   26,  574,   10, 1284,  367,  107,    3]), array([  7,  33,   6,  43, 347,  32, 118, 953,   3]), array([  7,   6,  69,  16, 141,   5, 309,  86,   3]), array([ 33,  37,  11, 430,  18,  10, 199,  14,   3]), array([109,   6,  43, 366,  71,   6,  43, 447,   3])]\n",
      "1170/1170 [==============================] - 299s 256ms/step - loss: 0.0012 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0832 - val_top_k_categorical_accuracy: 0.9912\n",
      "X\n",
      "[array([   2,    4,   45,  305,   77,    6, 4389, 1040,   36]), array([   2, 4067,   49, 1312,   15, 1111,   49,   27,  151]), array([   2,   82,    5,  198,   19,  115,   24, 1182,   10]), array([  2,   4,  41,  15,  36, 340,   6,   7, 340]), array([  2,  58,   4, 489, 658,  77,  10, 154,   6])]\n",
      "Y\n",
      "[array([   4,   45,  305,   77,    6, 4389, 1040,   36,    3]), array([4067,   49, 1312,   15, 1111,   49,   27,  151,    3]), array([  82,    5,  198,   19,  115,   24, 1182,   10,    3]), array([  4,  41,  15,  36, 340,   6,   7, 340,   3]), array([ 58,   4, 489, 658,  77,  10, 154,   6,   3])]\n",
      "X\n",
      "[array([  1,   1,   2,   4, 108,   6,  10, 312,  13]), array([ 1,  1,  1,  1,  1,  2, 48, 24, 34]), array([ 1,  1,  1,  1,  1,  2, 48, 24, 34]), array([   1,    1,    1,    1,    2,   70,   67,   17, 1786]), array([  1,   1,   1,   1,   2, 403,  13, 419, 358])]\n",
      "Y\n",
      "[array([  1,   2,   4, 108,   6,  10, 312,  13,   3]), array([ 1,  1,  1,  1,  2, 48, 24, 34,  3]), array([ 1,  1,  1,  1,  2, 48, 24, 34,  3]), array([   1,    1,    1,    2,   70,   67,   17, 1786,    3]), array([  1,   1,   1,   2, 403,  13, 419, 358,   3])]\n",
      "1170/1170 [==============================] - ETA: 0s - loss: 0.0012 - top_k_categorical_accuracy: 1.0000X\n",
      "[array([ 2,  6, 22, 16, 41, 48, 11, 97, 32]), array([   2,    4,   41,    6,   45,   46,    9,  546, 1580]), array([  2,  22,  16,  99,  13,  66,  62, 174,  17]), array([  2,  22,  16,   6,  28,  38, 897,  10,  13]), array([   1,    1,    1,    1,    1,    2,  155, 1524,  163])]\n",
      "Y\n",
      "[array([ 6, 22, 16, 41, 48, 11, 97, 32,  3]), array([   4,   41,    6,   45,   46,    9,  546, 1580,    3]), array([ 22,  16,  99,  13,  66,  62, 174,  17,   3]), array([ 22,  16,   6,  28,  38, 897,  10,  13,   3]), array([   1,    1,    1,    1,    2,  155, 1524,  163,    3])]\n",
      "X\n",
      "[array([   2,   42,   12,   26,  574,   10, 1284,  367,  107]), array([  2,   7,  33,   6,  43, 347,  32, 118, 953]), array([  2,   7,   6,  69,  16, 141,   5, 309,  86]), array([  2,  33,  37,  11, 430,  18,  10, 199,  14]), array([  2, 109,   6,  43, 366,  71,   6,  43, 447])]\n",
      "Y\n",
      "[array([  42,   12,   26,  574,   10, 1284,  367,  107,    3]), array([  7,  33,   6,  43, 347,  32, 118, 953,   3]), array([  7,   6,  69,  16, 141,   5, 309,  86,   3]), array([ 33,  37,  11, 430,  18,  10, 199,  14,   3]), array([109,   6,  43, 366,  71,   6,  43, 447,   3])]\n",
      "1170/1170 [==============================] - 299s 256ms/step - loss: 0.0012 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0755 - val_top_k_categorical_accuracy: 0.9919\n",
      "X\n",
      "[array([   2,    4,   45,  305,   77,    6, 4389, 1040,   36]), array([   2, 4067,   49, 1312,   15, 1111,   49,   27,  151]), array([   2,   82,    5,  198,   19,  115,   24, 1182,   10]), array([  2,   4,  41,  15,  36, 340,   6,   7, 340]), array([  2,  58,   4, 489, 658,  77,  10, 154,   6])]\n",
      "Y\n",
      "[array([   4,   45,  305,   77,    6, 4389, 1040,   36,    3]), array([4067,   49, 1312,   15, 1111,   49,   27,  151,    3]), array([  82,    5,  198,   19,  115,   24, 1182,   10,    3]), array([  4,  41,  15,  36, 340,   6,   7, 340,   3]), array([ 58,   4, 489, 658,  77,  10, 154,   6,   3])]\n",
      "X\n",
      "[array([  1,   1,   2,   4, 108,   6,  10, 312,  13]), array([ 1,  1,  1,  1,  1,  2, 48, 24, 34]), array([ 1,  1,  1,  1,  1,  2, 48, 24, 34]), array([   1,    1,    1,    1,    2,   70,   67,   17, 1786]), array([  1,   1,   1,   1,   2, 403,  13, 419, 358])]\n",
      "Y\n",
      "[array([  1,   2,   4, 108,   6,  10, 312,  13,   3]), array([ 1,  1,  1,  1,  2, 48, 24, 34,  3]), array([ 1,  1,  1,  1,  2, 48, 24, 34,  3]), array([   1,    1,    1,    2,   70,   67,   17, 1786,    3]), array([  1,   1,   1,   2, 403,  13, 419, 358,   3])]\n",
      "1170/1170 [==============================] - ETA: 0s - loss: 8.4306e-04 - top_k_categorical_accuracy: 1.0000X\n",
      "[array([ 2,  6, 22, 16, 41, 48, 11, 97, 32]), array([   2,    4,   41,    6,   45,   46,    9,  546, 1580]), array([  2,  22,  16,  99,  13,  66,  62, 174,  17]), array([  2,  22,  16,   6,  28,  38, 897,  10,  13]), array([   1,    1,    1,    1,    1,    2,  155, 1524,  163])]\n",
      "Y\n",
      "[array([ 6, 22, 16, 41, 48, 11, 97, 32,  3]), array([   4,   41,    6,   45,   46,    9,  546, 1580,    3]), array([ 22,  16,  99,  13,  66,  62, 174,  17,   3]), array([ 22,  16,   6,  28,  38, 897,  10,  13,   3]), array([   1,    1,    1,    1,    2,  155, 1524,  163,    3])]\n",
      "X\n",
      "[array([   2,   42,   12,   26,  574,   10, 1284,  367,  107]), array([  2,   7,  33,   6,  43, 347,  32, 118, 953]), array([  2,   7,   6,  69,  16, 141,   5, 309,  86]), array([  2,  33,  37,  11, 430,  18,  10, 199,  14]), array([  2, 109,   6,  43, 366,  71,   6,  43, 447])]\n",
      "Y\n",
      "[array([  42,   12,   26,  574,   10, 1284,  367,  107,    3]), array([  7,  33,   6,  43, 347,  32, 118, 953,   3]), array([  7,   6,  69,  16, 141,   5, 309,  86,   3]), array([ 33,  37,  11, 430,  18,  10, 199,  14,   3]), array([109,   6,  43, 366,  71,   6,  43, 447,   3])]\n",
      "1170/1170 [==============================] - 300s 257ms/step - loss: 8.4306e-04 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0764 - val_top_k_categorical_accuracy: 0.9918\n",
      "Epoch 15\n",
      "X\n",
      "[array([   2,    4,   45,  305,   77,    6, 4389, 1040,   36]), array([   2, 4067,   49, 1312,   15, 1111,   49,   27,  151]), array([   2,   82,    5,  198,   19,  115,   24, 1182,   10]), array([  2,   4,  41,  15,  36, 340,   6,   7, 340]), array([  2,  58,   4, 489, 658,  77,  10, 154,   6])]\n",
      "Y\n",
      "[array([   4,   45,  305,   77,    6, 4389, 1040,   36,    3]), array([4067,   49, 1312,   15, 1111,   49,   27,  151,    3]), array([  82,    5,  198,   19,  115,   24, 1182,   10,    3]), array([  4,  41,  15,  36, 340,   6,   7, 340,   3]), array([ 58,   4, 489, 658,  77,  10, 154,   6,   3])]\n",
      "X\n",
      "[array([  1,   1,   2,   4, 108,   6,  10, 312,  13]), array([ 1,  1,  1,  1,  1,  2, 48, 24, 34]), array([ 1,  1,  1,  1,  1,  2, 48, 24, 34]), array([   1,    1,    1,    1,    2,   70,   67,   17, 1786]), array([  1,   1,   1,   1,   2, 403,  13, 419, 358])]\n",
      "Y\n",
      "[array([  1,   2,   4, 108,   6,  10, 312,  13,   3]), array([ 1,  1,  1,  1,  2, 48, 24, 34,  3]), array([ 1,  1,  1,  1,  2, 48, 24, 34,  3]), array([   1,    1,    1,    2,   70,   67,   17, 1786,    3]), array([  1,   1,   1,   2, 403,  13, 419, 358,   3])]\n",
      "1170/1170 [==============================] - ETA: 0s - loss: 0.0011 - top_k_categorical_accuracy: 1.0000X\n",
      "[array([ 2,  6, 22, 16, 41, 48, 11, 97, 32]), array([   2,    4,   41,    6,   45,   46,    9,  546, 1580]), array([  2,  22,  16,  99,  13,  66,  62, 174,  17]), array([  2,  22,  16,   6,  28,  38, 897,  10,  13]), array([   1,    1,    1,    1,    1,    2,  155, 1524,  163])]\n",
      "Y\n",
      "[array([ 6, 22, 16, 41, 48, 11, 97, 32,  3]), array([   4,   41,    6,   45,   46,    9,  546, 1580,    3]), array([ 22,  16,  99,  13,  66,  62, 174,  17,   3]), array([ 22,  16,   6,  28,  38, 897,  10,  13,   3]), array([   1,    1,    1,    1,    2,  155, 1524,  163,    3])]\n",
      "X\n",
      "[array([   2,   42,   12,   26,  574,   10, 1284,  367,  107]), array([  2,   7,  33,   6,  43, 347,  32, 118, 953]), array([  2,   7,   6,  69,  16, 141,   5, 309,  86]), array([  2,  33,  37,  11, 430,  18,  10, 199,  14]), array([  2, 109,   6,  43, 366,  71,   6,  43, 447])]\n",
      "Y\n",
      "[array([  42,   12,   26,  574,   10, 1284,  367,  107,    3]), array([  7,  33,   6,  43, 347,  32, 118, 953,   3]), array([  7,   6,  69,  16, 141,   5, 309,  86,   3]), array([ 33,  37,  11, 430,  18,  10, 199,  14,   3]), array([109,   6,  43, 366,  71,   6,  43, 447,   3])]\n",
      "1170/1170 [==============================] - 300s 256ms/step - loss: 0.0011 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0774 - val_top_k_categorical_accuracy: 0.9918\n",
      "X\n",
      "[array([   2,    4,   45,  305,   77,    6, 4389, 1040,   36]), array([   2, 4067,   49, 1312,   15, 1111,   49,   27,  151]), array([   2,   82,    5,  198,   19,  115,   24, 1182,   10]), array([  2,   4,  41,  15,  36, 340,   6,   7, 340]), array([  2,  58,   4, 489, 658,  77,  10, 154,   6])]\n",
      "Y\n",
      "[array([   4,   45,  305,   77,    6, 4389, 1040,   36,    3]), array([4067,   49, 1312,   15, 1111,   49,   27,  151,    3]), array([  82,    5,  198,   19,  115,   24, 1182,   10,    3]), array([  4,  41,  15,  36, 340,   6,   7, 340,   3]), array([ 58,   4, 489, 658,  77,  10, 154,   6,   3])]\n",
      "X\n",
      "[array([  1,   1,   2,   4, 108,   6,  10, 312,  13]), array([ 1,  1,  1,  1,  1,  2, 48, 24, 34]), array([ 1,  1,  1,  1,  1,  2, 48, 24, 34]), array([   1,    1,    1,    1,    2,   70,   67,   17, 1786]), array([  1,   1,   1,   1,   2, 403,  13, 419, 358])]\n",
      "Y\n",
      "[array([  1,   2,   4, 108,   6,  10, 312,  13,   3]), array([ 1,  1,  1,  1,  2, 48, 24, 34,  3]), array([ 1,  1,  1,  1,  2, 48, 24, 34,  3]), array([   1,    1,    1,    2,   70,   67,   17, 1786,    3]), array([  1,   1,   1,   2, 403,  13, 419, 358,   3])]\n",
      "1170/1170 [==============================] - ETA: 0s - loss: 9.0310e-04 - top_k_categorical_accuracy: 1.0000X\n",
      "[array([ 2,  6, 22, 16, 41, 48, 11, 97, 32]), array([   2,    4,   41,    6,   45,   46,    9,  546, 1580]), array([  2,  22,  16,  99,  13,  66,  62, 174,  17]), array([  2,  22,  16,   6,  28,  38, 897,  10,  13]), array([   1,    1,    1,    1,    1,    2,  155, 1524,  163])]\n",
      "Y\n",
      "[array([ 6, 22, 16, 41, 48, 11, 97, 32,  3]), array([   4,   41,    6,   45,   46,    9,  546, 1580,    3]), array([ 22,  16,  99,  13,  66,  62, 174,  17,   3]), array([ 22,  16,   6,  28,  38, 897,  10,  13,   3]), array([   1,    1,    1,    1,    2,  155, 1524,  163,    3])]\n",
      "X\n",
      "[array([   2,   42,   12,   26,  574,   10, 1284,  367,  107]), array([  2,   7,  33,   6,  43, 347,  32, 118, 953]), array([  2,   7,   6,  69,  16, 141,   5, 309,  86]), array([  2,  33,  37,  11, 430,  18,  10, 199,  14]), array([  2, 109,   6,  43, 366,  71,   6,  43, 447])]\n",
      "Y\n",
      "[array([  42,   12,   26,  574,   10, 1284,  367,  107,    3]), array([  7,  33,   6,  43, 347,  32, 118, 953,   3]), array([  7,   6,  69,  16, 141,   5, 309,  86,   3]), array([ 33,  37,  11, 430,  18,  10, 199,  14,   3]), array([109,   6,  43, 366,  71,   6,  43, 447,   3])]\n",
      "1170/1170 [==============================] - 300s 256ms/step - loss: 9.0310e-04 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0751 - val_top_k_categorical_accuracy: 0.9920\n",
      "X\n",
      "[array([   2,    4,   45,  305,   77,    6, 4389, 1040,   36]), array([   2, 4067,   49, 1312,   15, 1111,   49,   27,  151]), array([   2,   82,    5,  198,   19,  115,   24, 1182,   10]), array([  2,   4,  41,  15,  36, 340,   6,   7, 340]), array([  2,  58,   4, 489, 658,  77,  10, 154,   6])]\n",
      "Y\n",
      "[array([   4,   45,  305,   77,    6, 4389, 1040,   36,    3]), array([4067,   49, 1312,   15, 1111,   49,   27,  151,    3]), array([  82,    5,  198,   19,  115,   24, 1182,   10,    3]), array([  4,  41,  15,  36, 340,   6,   7, 340,   3]), array([ 58,   4, 489, 658,  77,  10, 154,   6,   3])]\n",
      "X\n",
      "[array([  1,   1,   2,   4, 108,   6,  10, 312,  13]), array([ 1,  1,  1,  1,  1,  2, 48, 24, 34]), array([ 1,  1,  1,  1,  1,  2, 48, 24, 34]), array([   1,    1,    1,    1,    2,   70,   67,   17, 1786]), array([  1,   1,   1,   1,   2, 403,  13, 419, 358])]\n",
      "Y\n",
      "[array([  1,   2,   4, 108,   6,  10, 312,  13,   3]), array([ 1,  1,  1,  1,  2, 48, 24, 34,  3]), array([ 1,  1,  1,  1,  2, 48, 24, 34,  3]), array([   1,    1,    1,    2,   70,   67,   17, 1786,    3]), array([  1,   1,   1,   2, 403,  13, 419, 358,   3])]\n",
      "1170/1170 [==============================] - ETA: 0s - loss: 0.0010 - top_k_categorical_accuracy: 1.0000X\n",
      "[array([ 2,  6, 22, 16, 41, 48, 11, 97, 32]), array([   2,    4,   41,    6,   45,   46,    9,  546, 1580]), array([  2,  22,  16,  99,  13,  66,  62, 174,  17]), array([  2,  22,  16,   6,  28,  38, 897,  10,  13]), array([   1,    1,    1,    1,    1,    2,  155, 1524,  163])]\n",
      "Y\n",
      "[array([ 6, 22, 16, 41, 48, 11, 97, 32,  3]), array([   4,   41,    6,   45,   46,    9,  546, 1580,    3]), array([ 22,  16,  99,  13,  66,  62, 174,  17,   3]), array([ 22,  16,   6,  28,  38, 897,  10,  13,   3]), array([   1,    1,    1,    1,    2,  155, 1524,  163,    3])]\n",
      "X\n",
      "[array([   2,   42,   12,   26,  574,   10, 1284,  367,  107]), array([  2,   7,  33,   6,  43, 347,  32, 118, 953]), array([  2,   7,   6,  69,  16, 141,   5, 309,  86]), array([  2,  33,  37,  11, 430,  18,  10, 199,  14]), array([  2, 109,   6,  43, 366,  71,   6,  43, 447])]\n",
      "Y\n",
      "[array([  42,   12,   26,  574,   10, 1284,  367,  107,    3]), array([  7,  33,   6,  43, 347,  32, 118, 953,   3]), array([  7,   6,  69,  16, 141,   5, 309,  86,   3]), array([ 33,  37,  11, 430,  18,  10, 199,  14,   3]), array([109,   6,  43, 366,  71,   6,  43, 447,   3])]\n",
      "1170/1170 [==============================] - 303s 259ms/step - loss: 0.0010 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0739 - val_top_k_categorical_accuracy: 0.9921\n",
      "X\n",
      "[array([   2,    4,   45,  305,   77,    6, 4389, 1040,   36]), array([   2, 4067,   49, 1312,   15, 1111,   49,   27,  151]), array([   2,   82,    5,  198,   19,  115,   24, 1182,   10]), array([  2,   4,  41,  15,  36, 340,   6,   7, 340]), array([  2,  58,   4, 489, 658,  77,  10, 154,   6])]\n",
      "Y\n",
      "[array([   4,   45,  305,   77,    6, 4389, 1040,   36,    3]), array([4067,   49, 1312,   15, 1111,   49,   27,  151,    3]), array([  82,    5,  198,   19,  115,   24, 1182,   10,    3]), array([  4,  41,  15,  36, 340,   6,   7, 340,   3]), array([ 58,   4, 489, 658,  77,  10, 154,   6,   3])]\n",
      "X\n",
      "[array([  1,   1,   2,   4, 108,   6,  10, 312,  13]), array([ 1,  1,  1,  1,  1,  2, 48, 24, 34]), array([ 1,  1,  1,  1,  1,  2, 48, 24, 34]), array([   1,    1,    1,    1,    2,   70,   67,   17, 1786]), array([  1,   1,   1,   1,   2, 403,  13, 419, 358])]\n",
      "Y\n",
      "[array([  1,   2,   4, 108,   6,  10, 312,  13,   3]), array([ 1,  1,  1,  1,  2, 48, 24, 34,  3]), array([ 1,  1,  1,  1,  2, 48, 24, 34,  3]), array([   1,    1,    1,    2,   70,   67,   17, 1786,    3]), array([  1,   1,   1,   2, 403,  13, 419, 358,   3])]\n",
      "1170/1170 [==============================] - ETA: 0s - loss: 5.3455e-04 - top_k_categorical_accuracy: 1.0000X\n",
      "[array([ 2,  6, 22, 16, 41, 48, 11, 97, 32]), array([   2,    4,   41,    6,   45,   46,    9,  546, 1580]), array([  2,  22,  16,  99,  13,  66,  62, 174,  17]), array([  2,  22,  16,   6,  28,  38, 897,  10,  13]), array([   1,    1,    1,    1,    1,    2,  155, 1524,  163])]\n",
      "Y\n",
      "[array([ 6, 22, 16, 41, 48, 11, 97, 32,  3]), array([   4,   41,    6,   45,   46,    9,  546, 1580,    3]), array([ 22,  16,  99,  13,  66,  62, 174,  17,   3]), array([ 22,  16,   6,  28,  38, 897,  10,  13,   3]), array([   1,    1,    1,    1,    2,  155, 1524,  163,    3])]\n",
      "X\n",
      "[array([   2,   42,   12,   26,  574,   10, 1284,  367,  107]), array([  2,   7,  33,   6,  43, 347,  32, 118, 953]), array([  2,   7,   6,  69,  16, 141,   5, 309,  86]), array([  2,  33,  37,  11, 430,  18,  10, 199,  14]), array([  2, 109,   6,  43, 366,  71,   6,  43, 447])]\n",
      "Y\n",
      "[array([  42,   12,   26,  574,   10, 1284,  367,  107,    3]), array([  7,  33,   6,  43, 347,  32, 118, 953,   3]), array([  7,   6,  69,  16, 141,   5, 309,  86,   3]), array([ 33,  37,  11, 430,  18,  10, 199,  14,   3]), array([109,   6,  43, 366,  71,   6,  43, 447,   3])]\n",
      "1170/1170 [==============================] - 301s 257ms/step - loss: 5.3455e-04 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0749 - val_top_k_categorical_accuracy: 0.9919\n",
      "X\n",
      "[array([   2,    4,   45,  305,   77,    6, 4389, 1040,   36]), array([   2, 4067,   49, 1312,   15, 1111,   49,   27,  151]), array([   2,   82,    5,  198,   19,  115,   24, 1182,   10]), array([  2,   4,  41,  15,  36, 340,   6,   7, 340]), array([  2,  58,   4, 489, 658,  77,  10, 154,   6])]\n",
      "Y\n",
      "[array([   4,   45,  305,   77,    6, 4389, 1040,   36,    3]), array([4067,   49, 1312,   15, 1111,   49,   27,  151,    3]), array([  82,    5,  198,   19,  115,   24, 1182,   10,    3]), array([  4,  41,  15,  36, 340,   6,   7, 340,   3]), array([ 58,   4, 489, 658,  77,  10, 154,   6,   3])]\n",
      "X\n",
      "[array([  1,   1,   2,   4, 108,   6,  10, 312,  13]), array([ 1,  1,  1,  1,  1,  2, 48, 24, 34]), array([ 1,  1,  1,  1,  1,  2, 48, 24, 34]), array([   1,    1,    1,    1,    2,   70,   67,   17, 1786]), array([  1,   1,   1,   1,   2, 403,  13, 419, 358])]\n",
      "Y\n",
      "[array([  1,   2,   4, 108,   6,  10, 312,  13,   3]), array([ 1,  1,  1,  1,  2, 48, 24, 34,  3]), array([ 1,  1,  1,  1,  2, 48, 24, 34,  3]), array([   1,    1,    1,    2,   70,   67,   17, 1786,    3]), array([  1,   1,   1,   2, 403,  13, 419, 358,   3])]\n",
      "1170/1170 [==============================] - ETA: 0s - loss: 6.4595e-04 - top_k_categorical_accuracy: 1.0000X\n",
      "[array([ 2,  6, 22, 16, 41, 48, 11, 97, 32]), array([   2,    4,   41,    6,   45,   46,    9,  546, 1580]), array([  2,  22,  16,  99,  13,  66,  62, 174,  17]), array([  2,  22,  16,   6,  28,  38, 897,  10,  13]), array([   1,    1,    1,    1,    1,    2,  155, 1524,  163])]\n",
      "Y\n",
      "[array([ 6, 22, 16, 41, 48, 11, 97, 32,  3]), array([   4,   41,    6,   45,   46,    9,  546, 1580,    3]), array([ 22,  16,  99,  13,  66,  62, 174,  17,   3]), array([ 22,  16,   6,  28,  38, 897,  10,  13,   3]), array([   1,    1,    1,    1,    2,  155, 1524,  163,    3])]\n",
      "X\n",
      "[array([   2,   42,   12,   26,  574,   10, 1284,  367,  107]), array([  2,   7,  33,   6,  43, 347,  32, 118, 953]), array([  2,   7,   6,  69,  16, 141,   5, 309,  86]), array([  2,  33,  37,  11, 430,  18,  10, 199,  14]), array([  2, 109,   6,  43, 366,  71,   6,  43, 447])]\n",
      "Y\n",
      "[array([  42,   12,   26,  574,   10, 1284,  367,  107,    3]), array([  7,  33,   6,  43, 347,  32, 118, 953,   3]), array([  7,   6,  69,  16, 141,   5, 309,  86,   3]), array([ 33,  37,  11, 430,  18,  10, 199,  14,   3]), array([109,   6,  43, 366,  71,   6,  43, 447,   3])]\n",
      "1170/1170 [==============================] - 299s 256ms/step - loss: 6.4595e-04 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0756 - val_top_k_categorical_accuracy: 0.9921\n",
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional_8 (Bidirecti  (None, None, 400)         481600    \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, None, 10294)       4127894   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4609494 (17.58 MB)\n",
      "Trainable params: 4609494 (17.58 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# create and train model\n",
    "model = lstm_rnn(np.array(encoded_sequences_train), np.array(encoded_sequences_val), index_to_embedding, num_epochs=NUM_EPOCHS)\n",
    "\n",
    "# save trained model \n",
    "if SHOULD_SAVE:\n",
    "    model.save(SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create functions to generate new sequences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sequences(model: Sequential, \n",
    "                      tokenizer: Tokenizer, \n",
    "                      index_2_embedding: dict, \n",
    "                      num_seq: int):\n",
    "    '''\n",
    "    Generates a given number of sequences using the given RNN language model.\n",
    "    Will begin the sequence generation with n-1 SENTENCE_BEGIN tokens.\n",
    "    Returned sequences will have the BEGIN, END, and PADDING tokens removed\n",
    "\n",
    "    Args:\n",
    "        model: RNN language model\n",
    "        tokenizer: the keras preprocessing tokenizer\n",
    "        index_2_embedding: mapping from token index -> word2vec embeddings \n",
    "        num_seq: the number of sequences to generate \n",
    "\n",
    "    Returns: \n",
    "        A list of strings, where each string is a generated sequence with special tokens removed \n",
    "    '''\n",
    "    seed = [SENTENCE_BEGIN] * (SEQUENCE_LENGTH - 1) \n",
    "    \n",
    "    sequences = []\n",
    "    for _ in range(num_seq):\n",
    "        seq = generate_seq(model, tokenizer, index_2_embedding, seed)\n",
    "        seq = ' '.join(seq)\n",
    "\n",
    "        # remove special tokens\n",
    "        seq = seq.replace(SENTENCE_BEGIN, '')\n",
    "        seq = seq.replace(SENTENCE_END, '')\n",
    "        seq = seq.replace(PADDING, '')\n",
    "        seq = seq.replace(UNK, '')\n",
    "\n",
    "        sequences.append(seq.strip())\n",
    "        \n",
    "    return sequences\n",
    "\n",
    "\n",
    "def generate_seq(model: Sequential, \n",
    "                 tokenizer: Tokenizer, \n",
    "                 index_2_embedding: dict, \n",
    "                 seed: list):\n",
    "    '''\n",
    "    Generates a single sequence using the given model starting with a SENTENCE_BEGIN and ending with a SENTENCE_END token. \n",
    "    Since an RNN takes input sequences of fixed length, use a sliding window to continually predict the next word. \n",
    "\n",
    "    Args:\n",
    "        model: RNN language model\n",
    "        tokenizer: the keras preprocessing tokenizer\n",
    "        index_2_embedding: mapping from token index -> word2vec embeddings \n",
    "        seed: the initial tokens to feed the RNN\n",
    "    Returns: \n",
    "        An array of tokens representing a sequence \n",
    "    '''\n",
    "    padding_index = tokenizer.word_index.get(PADDING)\n",
    "    sentence_begin_index = tokenizer.word_index.get(SENTENCE_BEGIN)\n",
    "    sentence_end_index = tokenizer.word_index.get(SENTENCE_END)\n",
    "\n",
    "    # track the unique token indices for the sequence \n",
    "    sequence_indices = [tokenizer.word_index.get(tok) for tok in seed] \n",
    "\n",
    "    # number of timesteps that the model expects as input\n",
    "    input_length = SEQUENCE_LENGTH - 1\n",
    "\n",
    "    # until we get a SENTENCE_END token\n",
    "    while sequence_indices[-1] != sentence_end_index:\n",
    "        # get latest tokens to use as inputs \n",
    "        input_sequence = sequence_indices[-1*input_length:]\n",
    "\n",
    "        # convert the input sequence to embeddings\n",
    "        input_embeddings = np.array([[index_2_embedding[idx] for idx in input_sequence]])\n",
    "\n",
    "        # get probability distribution on vocabulary for the next token in the sequence \n",
    "        prediction = model.predict(input_embeddings, verbose=False)[0][-1]\n",
    "\n",
    "        # sample from the probability distribution \n",
    "        next_tok_idx = np.random.choice(len(prediction), p=prediction)\n",
    "\n",
    "        # skip mid-sentence SENTENCE_BEGIN and PADDING tokens\n",
    "        if next_tok_idx == sentence_begin_index or next_tok_idx == padding_index:\n",
    "            continue\n",
    "\n",
    "        # add newly generated token to our sequence \n",
    "        sequence_indices.append(next_tok_idx)\n",
    "\n",
    "    # convert to words \n",
    "    tokenizer_words = list(tokenizer.word_index.keys())\n",
    "    tokenizer_indices = list(tokenizer.word_index.values())\n",
    "    sequence = [tokenizer_words[tokenizer_indices.index(idx)] for idx in sequence_indices]\n",
    "    return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Generated Lyrics:\n",
      "\n",
      "is comin ' again together\n",
      "let me\n",
      "new new new kind who 's town just every honky-tonk comin ' they do n't life a than need racin , with gon hold a mmmm when fly away\n",
      "do n't comin ' future , free '' no mind , that we ca n't\n",
      "let ya lock\n",
      "do n't comin ' again\n",
      "let me let fly oh\n",
      "think oh oh\n",
      "do somethin right i , oh\n",
      "that do n't one more is now\n"
     ]
    }
   ],
   "source": [
    "# load in model (if we have a pre-trained one that we'd like to generate sequences for)\n",
    "if SHOULD_LOAD:\n",
    "    model = keras.saving.load_model(LOAD_PATH)\n",
    "    \n",
    "# Generate new lyrics \n",
    "generated_sequences = generate_sequences(model, tokenizer, index_to_embedding, num_seq=10)\n",
    "print(\"Sample Generated Lyrics:\\n\")\n",
    "for seq in generated_sequences:\n",
    "    print(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Generated Lyrics:\n",
      "\n",
      "] broken on and ] ? save anytime heard\n",
      "here who\n",
      "arms going\n",
      "tears to than\n",
      "little going\n",
      "on you\n",
      "dance wild at angel\n",
      "together a another little together\n",
      "merle\n",
      "come like smile\n"
     ]
    }
   ],
   "source": [
    "# TODO: delete, just a test \n",
    "other_model = keras.saving.load_model(\"country_lstm_model_epoch20_full\")\n",
    "    \n",
    "# Generate new lyrics \n",
    "generated_sequences = generate_sequences(other_model, tokenizer, index_to_embedding, num_seq=10)\n",
    "print(\"Sample Generated Lyrics:\\n\")\n",
    "for seq in generated_sequences:\n",
    "    print(seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Perplexity \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded input sequence: [2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "Encoded y to predict: 6\n",
      "Probability of next token being y: 0.00032335325\n",
      "Encoded input sequence: [2, 2, 2, 2, 2, 2, 2, 2, 6]\n",
      "Encoded y to predict: 22\n",
      "Probability of next token being y: 0.33570278\n",
      "Encoded input sequence: [2, 2, 2, 2, 2, 2, 2, 6, 22]\n",
      "Encoded y to predict: 15\n",
      "Probability of next token being y: 0.44948903\n",
      "Encoded input sequence: [2, 2, 2, 2, 2, 2, 6, 22, 15]\n",
      "Encoded y to predict: 41\n",
      "Probability of next token being y: 6.586334e-05\n",
      "Encoded input sequence: [2, 2, 2, 2, 2, 6, 22, 15, 41]\n",
      "Encoded y to predict: 48\n",
      "Probability of next token being y: 0.0028572127\n",
      "Encoded input sequence: [2, 2, 2, 2, 6, 22, 15, 41, 48]\n",
      "Encoded y to predict: 11\n",
      "Probability of next token being y: 0.0028673592\n",
      "Encoded input sequence: [2, 2, 2, 6, 22, 15, 41, 48, 11]\n",
      "Encoded y to predict: 98\n",
      "Probability of next token being y: 5.1974992e-05\n",
      "Encoded input sequence: [2, 2, 6, 22, 15, 41, 48, 11, 98]\n",
      "Encoded y to predict: 31\n",
      "Probability of next token being y: 0.014037577\n",
      "Encoded input sequence: [2, 6, 22, 15, 41, 48, 11, 98, 31]\n",
      "Encoded y to predict: 6\n",
      "Probability of next token being y: 0.022634024\n",
      "Encoded input sequence: [6, 22, 15, 41, 48, 11, 98, 31, 6]\n",
      "Encoded y to predict: 6\n",
      "Probability of next token being y: 0.00015523576\n",
      "Encoded input sequence: [22, 15, 41, 48, 11, 98, 31, 6, 6]\n",
      "Encoded y to predict: 144\n",
      "Probability of next token being y: 0.00080883084\n",
      "Encoded input sequence: [15, 41, 48, 11, 98, 31, 6, 6, 144]\n",
      "Encoded y to predict: 15\n",
      "Probability of next token being y: 0.98131907\n",
      "Encoded input sequence: [41, 48, 11, 98, 31, 6, 6, 144, 15]\n",
      "Encoded y to predict: 176\n",
      "Probability of next token being y: 0.030611657\n",
      "Encoded input sequence: [48, 11, 98, 31, 6, 6, 144, 15, 176]\n",
      "Encoded y to predict: 13\n",
      "Probability of next token being y: 0.0009170164\n",
      "Encoded input sequence: [11, 98, 31, 6, 6, 144, 15, 176, 13]\n",
      "Encoded y to predict: 198\n",
      "Probability of next token being y: 5.0627714e-05\n",
      "Encoded input sequence: [98, 31, 6, 6, 144, 15, 176, 13, 198]\n",
      "Encoded y to predict: 3\n",
      "Probability of next token being y: 0.18315957\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "232.25999475358108"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_perplexity(encoded_sequence: list, model: Sequential, \n",
    "                        tokenizer: Tokenizer, \n",
    "                        index_2_embedding: dict,\n",
    "                        verbose: bool = False):\n",
    "    '''\n",
    "    Computes the perplexity of a single sequence by finding the probability that the model will generate \n",
    "    this sequence. Uses a sliding window to continuously predict the next word, and finds the softmax probability\n",
    "    associated with the true word. \n",
    "\n",
    "    Args:\n",
    "        encoded_sequence (list): a single sequence represented by its Tokenizer encodings \n",
    "        model: RNN language model\n",
    "        tokenizer: the keras preprocessing tokenizer\n",
    "        index_2_embedding: mapping from token index -> word2vec embeddings \n",
    "        verbose (bool): If true, prints information about the sequence and the probability of each word \n",
    "    Returns: \n",
    "        the perplexity of the sequence \n",
    "    ''' \n",
    "    padding_index = tokenizer.word_index.get(PADDING)\n",
    "    sentence_begin_index = tokenizer.word_index.get(SENTENCE_BEGIN)\n",
    "\n",
    "    # shift Y forward one token to represent the next word predictions \n",
    "    X_data = encoded_sequence[:-1]\n",
    "    Y_data = encoded_sequence[1:]\n",
    "\n",
    "    # seed with SEQUENCE_LENGTH - 2 sentence begins (first token in X is a sentence begin as well),\n",
    "    # inch window along to predict the next word \n",
    "    encoded_input = [sentence_begin_index] * (SEQUENCE_LENGTH - 2) + X_data\n",
    "\n",
    "    input_length = SEQUENCE_LENGTH - 1\n",
    "\n",
    "    # we will be finding the log of the probability of our model generating this sequence \n",
    "    Y_pred_log_prob = 0\n",
    "\n",
    "    # track the number of meaningful tokens \n",
    "    N = 0\n",
    "\n",
    "    # for each word in Y (all tokens except SENTENCE_BEGIN)\n",
    "    for i, y in enumerate(Y_data):\n",
    "        # use a sliding window over the input, where the input sequence are the tokens preceding y \n",
    "        input_sequence = encoded_input[i:input_length+i]\n",
    "\n",
    "        # convert the input sequence to embeddings\n",
    "        input_embeddings = np.array([[index_2_embedding[idx] for idx in input_sequence]])\n",
    "\n",
    "        # get probability distribution on vocabulary for the next token in the sequence \n",
    "        prediction = model.predict(input_embeddings, verbose=False)[0][-1]\n",
    "\n",
    "        # index y to get the predicted probability of the true value \n",
    "        y_pred_prob = prediction[y] \n",
    "\n",
    "        # print information to help with debugging \n",
    "        if verbose:\n",
    "            print(\"Encoded input sequence:\", input_sequence)\n",
    "            print(\"Encoded y to predict:\", y)\n",
    "            print(\"Probability of next token being y:\", y_pred_prob)\n",
    "\n",
    "        Y_pred_log_prob += np.log(y_pred_prob)\n",
    "\n",
    "        # only include meaningful tokens in our token count \n",
    "        if y != padding_index and y != sentence_begin_index:\n",
    "            N += 1\n",
    "\n",
    "    # compute probability of our model generating this sequence as well as the perplexity \n",
    "    Y_pred_prob = np.exp(Y_pred_log_prob)\n",
    "    perplexity = Y_pred_prob ** (-1/N)\n",
    "    \n",
    "    return perplexity\n",
    "\n",
    "\n",
    "def mean_perplexity(val_data: np.array, \n",
    "                    model: Sequential, \n",
    "                    tokenizer: Tokenizer, \n",
    "                    index_2_embedding: dict):\n",
    "    \"\"\"\" \n",
    "    Computes the average perplexity of all sequences in the given data using the given model.\n",
    "\n",
    "    Args:\n",
    "        encoded_sequence (list): a single sequence represented by its Tokenizer encodings \n",
    "        model: RNN language model\n",
    "        tokenizer: the keras preprocessing tokenizer\n",
    "        index_2_embedding: mapping from token index -> word2vec embeddings \n",
    "        verbose (bool): If true, prints information about the sequence and the probability of each word \n",
    "    Returns: \n",
    "        The average perplexity of the provided sequences \n",
    "    \"\"\"\n",
    "    perplexities = []\n",
    "    for seq in val_data:\n",
    "        perplexities.append(calculate_perplexity(seq, model, tokenizer, index_2_embedding))\n",
    "    return np.mean(perplexities)\n",
    "\n",
    "#TODO - probably increase, just takes about a minute for every 100 sequences \n",
    "#calculate_perplexity(encoded_sequences_val[0], model, tokenizer, index_to_embedding, verbose=True)\n",
    "mean_perplexity(encoded_sequences_val[:200], model, tokenizer, index_to_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experimenting with Hyperparameters \n",
    "\n",
    "To find our final configuration for our RNN + LSTM model, we will pick a genre test out hyperparameters such as sequence length, embedding size, number of hidden units, number of epochs, and additional layers. \n",
    "\n",
    "For each configuration, report: \n",
    "1. number of sequences \n",
    "2. pre-processing strategy (padding / concatenation?)\n",
    "3. epochs\n",
    "4. dimensions of network (# of layers, # of hidden units per layer)\n",
    "5. `SEQUENCE_LENGTH` value\n",
    "6. time to train \n",
    "7. final `val_accuracy`\n",
    "8. perplexity \n",
    "9. generated sequence example\n",
    "-----------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key Findings \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
