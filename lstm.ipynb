{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN with LSTMs Language Model using Skip-Gram Dense Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports \n",
    "from gensim.models import Word2Vec\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter \n",
    "from itertools import chain\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Bidirectional\n",
    "\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants \n",
    "SENTENCE_BEGIN = \"<s>\"\n",
    "SENTENCE_END = \"</s>\"\n",
    "PADDING = \"<pad>\"\n",
    "UNK = \"<unk>\"\n",
    "\n",
    "# hyperparameters - may change \n",
    "EMBEDDINGS_SIZE = 100\n",
    "BATCH_SIZE = 128\n",
    "SEQUENCE_LENGTH = 10\n",
    "NUM_EPOCHS = 15\n",
    "\n",
    "\n",
    "# may either be country or metal\n",
    "GENRE = 'metal' \n",
    "\n",
    "# change to save a newly trained model\n",
    "SAVE_PATH = 'foo'\n",
    "SHOULD_SAVE = False \n",
    "\n",
    "# change to load in an already trained model for sequence generation \n",
    "LOAD_PATH = 'metal_lstm_model_epoch20_full'\n",
    "SHOULD_LOAD = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training lines: 149771\n",
      "Number of validation lines: 18610\n",
      "Number of test lines: 19108\n",
      "Lyric Example: my journey began after the war of troy\n"
     ]
    }
   ],
   "source": [
    "# read in cleaned data\n",
    "if GENRE == 'country':\n",
    "    train_lines = pd.read_csv('data/country_train.csv', header=None)[0].tolist()\n",
    "    val_lines = pd.read_csv('data/country_val.csv', header=None)[0].tolist()\n",
    "    test_lines = pd.read_csv('data/country_test.csv', header=None)[0].tolist()\n",
    "\n",
    "elif GENRE == 'metal':\n",
    "    train_lines = pd.read_csv('data/metal_train.csv', header=None)[0].tolist()\n",
    "    val_lines = pd.read_csv('data/metal_val.csv', header=None)[0].tolist()\n",
    "    test_lines = pd.read_csv('data/metal_test.csv', header=None)[0].tolist()\n",
    "\n",
    "else:\n",
    "    raise ValueError('Incorrect genre given.')\n",
    "\n",
    "\n",
    "print(\"Number of training lines:\", len(train_lines))\n",
    "print(\"Number of validation lines:\", len(val_lines))\n",
    "print(\"Number of test lines:\", len(test_lines))\n",
    "print('Lyric Example:', train_lines[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_lines = train_lines[:10000]\n",
    "val_lines = val_lines[:2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation: Tokenize Lyrics, Pad Sequences, Create Dense Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize and add a single sentence start and end token around each sequence \n",
    "train_tokens = [utils.tokenize_line(line, ngram=1) for line in train_lines] \n",
    "val_tokens = [utils.tokenize_line(line, ngram=1) for line in val_lines] \n",
    "test_tokens = [utils.tokenize_line(line, ngram=1) for line in test_lines] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Length: 8.331432653851547\n",
      "Median Length: 8.0\n",
      "90th Percentile Length: 12.0\n",
      "Max Length: 110\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot lengths of lyric lines to help determine an appropriate length to pad/truncate to\n",
    "train_sequence_lengths = [len(seq) for seq in train_tokens]\n",
    "\n",
    "print(\"Mean Length:\", np.mean(train_sequence_lengths))\n",
    "print(\"Median Length:\", np.median(train_sequence_lengths))\n",
    "print(\"90th Percentile Length:\", np.percentile(train_sequence_lengths, 90))\n",
    "print(\"Max Length:\", np.max(train_sequence_lengths))\n",
    "\n",
    "plt.hist(train_sequence_lengths, bins=50)\n",
    "plt.xlabel(\"Length of Lyric Line\")\n",
    "plt.ylabel(\"Number of Lines\")\n",
    "plt.title(\"Distribution of Line Length for Lyrics in Training Data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: 149771\n",
      "Length of Sequences: 10\n"
     ]
    }
   ],
   "source": [
    "def adjust_sequence_length(tokenized_seqs: list, sequence_length: int = SEQUENCE_LENGTH) -> list:\n",
    "    \"\"\"\n",
    "    Pads or truncates all sequences in the provided list to the same length. \n",
    "    Adds padding tokens to the left for too-short sequences and truncates to the right \n",
    "    for too-long sequences (method based on experimentation with left/right padding/truncation)\n",
    "\n",
    "    Args:\n",
    "        tokenized_seqs (list): A list of lists of tokens. Each inner list represents a sequence with tokens as elements\n",
    "        sequence_length (int): The desired length for all of the sequences\n",
    "        padding_token (str): The token that should be used to pad short sequences to the proper length\n",
    "\n",
    "    Returns:\n",
    "        size_adjusted_sequences (list): A list of lists of tokens, where each inner list is the same length\n",
    "    \"\"\"\n",
    "    size_adjusted_sequences = []\n",
    "    for sequence in tokenized_seqs:\n",
    "        if len(sequence) < sequence_length:\n",
    "            # too short, add padding\n",
    "            num_padding = sequence_length - len(sequence)\n",
    "            size_adjusted_sequences.append( ([PADDING] * num_padding) + sequence)\n",
    "        else:\n",
    "            # truncate sequences longer than the chosen length \n",
    "            size_adjusted_sequences.append(sequence[:sequence_length])\n",
    "            \n",
    "\n",
    "    return size_adjusted_sequences\n",
    "\n",
    "\n",
    "def replace_unknowns_train(tokenized_seqs: list) -> list:\n",
    "    \"\"\"\"\n",
    "    Replaces words that occur only once with an UNK token\n",
    "\n",
    "    Args:\n",
    "        tokenized_seqs (list): A list of lists of tokens. Each inner list represents a sequence with tokens as elements\n",
    "\n",
    "    Returns:\n",
    "        Tokenized sequences with low frequency words replaced with the unknown special token \n",
    "    \"\"\"\n",
    "    # concatenate all sequences together \n",
    "    all_tokens = list(chain(*tokenized_seqs))\n",
    "    token_counts = Counter(all_tokens)\n",
    "\n",
    "    # Replace words with low frequencies to UNK so that we can calculate perplexity on test data with unknown words \n",
    "    cleaned_tokenized_seqs = []\n",
    "    for seq in tokenized_seqs:\n",
    "        cleaned_seq = [tok if token_counts[tok] > 1 else UNK for tok in seq]\n",
    "        cleaned_tokenized_seqs.append(cleaned_seq)\n",
    "\n",
    "    return cleaned_tokenized_seqs\n",
    "\n",
    "\n",
    "size_adjusted_sequences_train = adjust_sequence_length(train_tokens)\n",
    "cleaned_sequences_train = replace_unknowns_train(size_adjusted_sequences_train)\n",
    "print(\"Number of sequences:\", len(cleaned_sequences_train))\n",
    "print(\"Length of Sequences:\", len(cleaned_sequences_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 14023\n",
      "encoded examples: \n",
      " [2, 13, 632, 1515, 409, 4, 191, 10, 4883, 3] \n",
      " [2, 5, 139, 237, 11, 392, 6, 13, 346, 9]\n"
     ]
    }
   ],
   "source": [
    "# Use Tokenizer to map each token to a unique index \n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(cleaned_sequences_train)\n",
    "encoded_sequences_train = tokenizer.texts_to_sequences(cleaned_sequences_train)\n",
    "\n",
    "print(\"Vocab Size:\", len(tokenizer.word_index))\n",
    "print('encoded examples:', '\\n', encoded_sequences_train[0], '\\n', encoded_sequences_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size for word embeddings: 14023\n"
     ]
    }
   ],
   "source": [
    "# create word embeddings using skip gram algorithm\n",
    "word_embeddings = Word2Vec(sentences=cleaned_sequences_train, vector_size=EMBEDDINGS_SIZE, window=5, sg=1, min_count=1)\n",
    "print('Vocab size for word embeddings:', len(word_embeddings.wv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that gives mappings from words to their embeddings and  \n",
    "# indexes from the tokenizers to their embeddings\n",
    "\n",
    "def map_embeddings(embeddings: Word2Vec, tokenizer: Tokenizer) -> (dict, dict):\n",
    "    ''' Creates mappings between different token representations \n",
    "    Arguments:\n",
    "        embeddings: Word2Vec word embeddings for the data (maps tokens to embedding vectors)\n",
    "        tokenizer: Tokenizer used to tokenize the data (maps token to index)\n",
    "    Returns:\n",
    "        (dict): mapping from word to its embedding vector\n",
    "        (dict): mapping from index to its embedding vector\n",
    "    '''\n",
    "    # initialize dictionaries \n",
    "    token_to_embedding = {}\n",
    "    index_to_embedding = {}\n",
    "\n",
    "    # tokenizer maps tokens to unique indices \n",
    "    for token, index in tokenizer.word_index.items():\n",
    "        embedding = embeddings[token]\n",
    "\n",
    "        token_to_embedding[token] = embedding\n",
    "        index_to_embedding[index] = embedding\n",
    "\n",
    "    return (token_to_embedding, index_to_embedding)\n",
    "\n",
    "\n",
    "token_to_embedding, index_to_embedding = map_embeddings(word_embeddings.wv, tokenizer)\n",
    "\n",
    "# Fill in unused index zero to avoid dimension mismatch\n",
    "index_to_embedding[0] = [0] * EMBEDDINGS_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Samples for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X batch shape: (128, 9, 100)\n",
      "y batch shape: (128, 9, 14024)\n"
     ]
    }
   ],
   "source": [
    "def data_generator(data: list, num_sequences_per_batch: int, index_2_embedding: dict) -> (np.array, np.array):\n",
    "    '''\n",
    "    Returns a data generator to train the neural network in batches\n",
    "\n",
    "    X data will be represented in embedding form.\n",
    "    Y data will be represented with one hot vectors. \n",
    "\n",
    "    Args:\n",
    "    data (list of lists): tokenized sequences represented by their unique index encodings \n",
    "    num_sequences_per_batch (int): batch size yielded on each iteration of the generator \n",
    "    index_2_embedding (dict): mapping between unique token indices and dense word embeddings \n",
    "\n",
    "    Returns:\n",
    "    X_batch_embeddings (3-D numpy array): sequences of embeddings with dimensions (batch size, num timesteps, embedding size)\n",
    "                                          Take the first (SEQUENCE_LENGTH - 1) tokens of each sequence\n",
    "    y_batch (3-D numpy array): sequences of one hot vectors with dimensions (batch size, num timesteps, vocab size)\n",
    "                                          Take the last (SEQUENCE_LENGTH - 1) tokens of each sequence \n",
    "                                          (X shifted forward one token so that the neural net predicts \n",
    "                                          the next word in the sequence for each timestep)\n",
    "    '''\n",
    "    # iterate over data in batches - stored in the form of unique token indices \n",
    "    i = 0\n",
    "    while True:\n",
    "        # get samples that we'd like to train on for this batch \n",
    "        data_batch = data[i:i+num_sequences_per_batch]\n",
    "\n",
    "        # increment i with each batch \n",
    "        i += num_sequences_per_batch\n",
    "\n",
    "        # split into X and Y -- shifted sequence so that for each timestep, Y is the token that follows X \n",
    "        X_data = [sequence[:-1] for sequence in data_batch]\n",
    "        Y_data = [sequence[1:] for sequence in data_batch]\n",
    "\n",
    "        # get embeddings for X data \n",
    "        X_embeddings = []\n",
    "        for X_sequence in X_data:\n",
    "            X_sequence_embeddings = [index_2_embedding[token_idx] for token_idx in X_sequence]\n",
    "            X_embeddings.append(X_sequence_embeddings)\n",
    "\n",
    "        # get one hot vectors for Y data \n",
    "        Y_one_hot_vectors = []\n",
    "        for Y_sequence in Y_data:\n",
    "            Y_one_hot = to_categorical(Y_sequence, num_classes=len(index_2_embedding))\n",
    "            Y_one_hot_vectors.append(Y_one_hot)\n",
    "\n",
    "        # yield statement instead of return for generator \n",
    "        yield(np.array(X_embeddings), np.array(Y_one_hot_vectors))\n",
    "\n",
    "\n",
    "# demo the data generator\n",
    "demo_data_generator = data_generator(encoded_sequences_train, BATCH_SIZE, index_to_embedding)\n",
    "demo_sample = next(demo_data_generator)\n",
    "print(\"X batch shape:\", demo_sample[0].shape)\n",
    "print(\"y batch shape:\", demo_sample[1].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare validation data\n",
    "# Since the model does not know about these sequences beforehand, \n",
    "# unknown words are those that do not appear in the training vocabulary \n",
    "def encode_new_sequences(tokenized_seqs: list, tokenizer) -> list:\n",
    "    \"\"\"\"\n",
    "    Replaces words that are not in the tokenizer's vocab with the unknown special token and encodes it to \n",
    "    unique token indices specified by the provided Tokenizer.\n",
    "\n",
    "    Args:\n",
    "        tokenized_seqs (list): A list of lists of tokens. Each inner list represents a sequence with tokens as elements\n",
    "        tokenizer: Tokenizer used maps token to index\n",
    "\n",
    "    Returns:\n",
    "        Encoded sequences with words not in the training vocabulary replaced with the unknown special token \n",
    "    \"\"\"\n",
    "    cleaned_tokenized_seqs = []\n",
    "    for seq in tokenized_seqs:\n",
    "        cleaned_seq = [tok if tok in tokenizer.word_index.keys() else UNK for tok in seq]\n",
    "        cleaned_tokenized_seqs.append(cleaned_seq)\n",
    "\n",
    "    return tokenizer.texts_to_sequences(cleaned_tokenized_seqs)\n",
    "\n",
    "encoded_sequences_val = encode_new_sequences(val_tokens, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_rnn(train_data: np.array,\n",
    "             index_2_embedding: dict, \n",
    "             num_epochs: int=1, \n",
    "             num_sequences_per_batch: int=BATCH_SIZE, \n",
    "             sequence_length: int=SEQUENCE_LENGTH,\n",
    "             embedding_size: int=EMBEDDINGS_SIZE):\n",
    "    \"\"\"\n",
    "    Creates and trains an RNN with LSTM cells using given training data and batch size.\n",
    "\n",
    "    Args:\n",
    "        train_data (list of lists): encoded sequences of training data represented by token indices \n",
    "        index_2_embedding (dict): mapping from token index -> word2vec embeddings \n",
    "        num_epochs (int): number of training epochs\n",
    "        num_sequences_per_batch (int): batch size for training data \n",
    "        sequence_length (int): number of tokens in each training sample \n",
    "        embedding_size (int): size of the dense word embeddings used to represent tokens \n",
    "    Returns:\n",
    "        A trained Neural Network language model\n",
    "    \"\"\"\n",
    "    # define model parameters\n",
    "    hidden_units = 200\n",
    "    hidden_input_dim = (sequence_length - 1, embedding_size)      # (number of steps, number of features per step)\n",
    "    output_dim = len(index_2_embedding)                            # vocab size \n",
    "\n",
    "    # instantiate model\n",
    "    model = Sequential()\n",
    "\n",
    "    # hidden layer\n",
    "    model.add(Bidirectional(LSTM(hidden_units, \n",
    "                                 input_shape=hidden_input_dim,\n",
    "                                 return_sequences=True)))\n",
    "\n",
    "    # output layer\n",
    "    model.add(Dense(units=output_dim, activation='softmax'))\n",
    "\n",
    "    # configure the learning process\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=[\"top_k_categorical_accuracy\"])\n",
    "    \n",
    "    # total number of batches per epoch \n",
    "    steps_per_epoch = len(train_data)//num_sequences_per_batch\n",
    "   \n",
    "    for i in range(num_epochs):\n",
    "        if i % 5 == 0:\n",
    "            print(\"Epoch\", i)\n",
    "\n",
    "        # create a new data generator for us to iterate through\n",
    "        train_generator = data_generator(train_data, num_sequences_per_batch, index_2_embedding)\n",
    "\n",
    "        # train model \n",
    "        model.fit(x=train_generator, steps_per_epoch=steps_per_epoch)\n",
    "\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "1170/1170 [==============================] - 312s 264ms/step - loss: 2.3845 - top_k_categorical_accuracy: 0.7407\n",
      "1170/1170 [==============================] - 388s 331ms/step - loss: 0.7028 - top_k_categorical_accuracy: 0.9272\n",
      "1170/1170 [==============================] - 276s 236ms/step - loss: 0.4257 - top_k_categorical_accuracy: 0.9546\n",
      "1170/1170 [==============================] - 276s 236ms/step - loss: 0.2815 - top_k_categorical_accuracy: 0.9725\n",
      "1170/1170 [==============================] - 290s 248ms/step - loss: 0.1946 - top_k_categorical_accuracy: 0.9819\n",
      "Epoch 5\n",
      "1170/1170 [==============================] - 256s 219ms/step - loss: 0.1481 - top_k_categorical_accuracy: 0.9847\n",
      "1170/1170 [==============================] - 258s 220ms/step - loss: 0.1234 - top_k_categorical_accuracy: 0.9858\n",
      "1170/1170 [==============================] - 257s 219ms/step - loss: 0.1089 - top_k_categorical_accuracy: 0.9868\n",
      " 794/1170 [===================>..........] - ETA: 1:23 - loss: 0.1006 - top_k_categorical_accuracy: 0.9878"
     ]
    }
   ],
   "source": [
    "# create and train model\n",
    "model = lstm_rnn(np.array(encoded_sequences_train), index_to_embedding, num_epochs=NUM_EPOCHS)\n",
    "\n",
    "# save trained model \n",
    "if SHOULD_SAVE:\n",
    "    model.save(SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create functions to generate new sequences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sequences(model: Sequential, \n",
    "\t\t\t\t\t  tokenizer: Tokenizer, \n",
    "\t\t\t\t\t  index_2_embedding: dict, \n",
    "\t\t\t\t\t  num_seq: int,\n",
    "\t\t\t\t\t  verbose: bool = True,\n",
    "\t\t\t\t\t  file_path: str = None):\n",
    "\t'''\n",
    "\tGenerates a given number of sequences using the given RNN language model.\n",
    "\tWill begin the sequence generation with n-1 SENTENCE_BEGIN tokens.\n",
    "\tReturned sequences will have the BEGIN, END, and PADDING tokens removed\n",
    "\n",
    "\tWrites generated sequences to file_path, if provided \n",
    "\n",
    "\tArgs:\n",
    "\t\tmodel: RNN language model\n",
    "\t\ttokenizer: the keras preprocessing tokenizer\n",
    "\t\tindex_2_embedding: mapping from token index -> word2vec embeddings \n",
    "\t\tnum_seq: the number of sequences to generate \n",
    "\t\tverbose: If True, prints progress of sequence generation \n",
    "\t\tfile_path (str): path of file to write the generated sequences to, with one sequence per line \n",
    "\n",
    "\tReturns: \n",
    "\t\tA list of strings, where each string is a generated sequence with special tokens removed \n",
    "\t'''\n",
    "\tseed = [SENTENCE_BEGIN] * (SEQUENCE_LENGTH - 1) \n",
    "\t\n",
    "\tsequences = []\n",
    "\tfor i in range(num_seq):\n",
    "\t\t# print progress \n",
    "\t\tif verbose and i % 10 == 0:\n",
    "\t\t\tprint(\"Generating line\", i, \"/\", num_seq)\n",
    "\n",
    "\n",
    "\n",
    "\t\tseq = generate_seq(model, tokenizer, index_2_embedding, seed)\n",
    "\t\tseq = ' '.join(seq)\n",
    "\n",
    "\t\t# remove special tokens\n",
    "\t\tseq = seq.replace(SENTENCE_BEGIN, '')\n",
    "\t\tseq = seq.replace(SENTENCE_END, '')\n",
    "\t\tseq = seq.replace(PADDING, '')\n",
    "\t\tseq = seq.replace(UNK, '')\n",
    "\n",
    "\t\tsequences.append(seq.strip())\n",
    "\n",
    "\tif file_path is not None:\n",
    "\t\twith open(file_path, 'w') as f:\n",
    "\t\t\tfor seq in sequences:\n",
    "\t\t\t\tf.write(seq + '\\n')\n",
    "\t\t\n",
    "\treturn sequences\n",
    "\n",
    "\n",
    "def generate_seq(model: Sequential, \n",
    "\t\t\t\t tokenizer: Tokenizer, \n",
    "\t\t\t\t index_2_embedding: dict, \n",
    "\t\t\t\t seed: list):\n",
    "\t'''\n",
    "\tGenerates a single sequence using the given model starting with a SENTENCE_BEGIN and ending with a SENTENCE_END token. \n",
    "\tSince an RNN takes input sequences of fixed length, use a sliding window to continually predict the next word. \n",
    "\n",
    "\tArgs:\n",
    "\t\tmodel: RNN language model\n",
    "\t\ttokenizer: the keras preprocessing tokenizer\n",
    "\t\tindex_2_embedding: mapping from token index -> word2vec embeddings \n",
    "\t\tseed: the initial tokens to feed the RNN\n",
    "\tReturns: \n",
    "\t\tAn array of tokens representing a sequence \n",
    "\t'''\n",
    "\tpadding_index = tokenizer.word_index.get(PADDING)\n",
    "\tsentence_begin_index = tokenizer.word_index.get(SENTENCE_BEGIN)\n",
    "\tsentence_end_index = tokenizer.word_index.get(SENTENCE_END)\n",
    "\n",
    "\t# track the unique token indices for the sequence \n",
    "\tsequence_indices = [tokenizer.word_index.get(tok) for tok in seed] \n",
    "\n",
    "\t# number of timesteps that the model expects as input\n",
    "\tinput_length = SEQUENCE_LENGTH - 1\n",
    "\n",
    "\t# until we get a SENTENCE_END token\n",
    "\twhile sequence_indices[-1] != sentence_end_index:\n",
    "\t\t# get latest tokens to use as inputs \n",
    "\t\tinput_sequence = sequence_indices[-1*input_length:]\n",
    "\n",
    "\t\t# convert the input sequence to embeddings\n",
    "\t\tinput_embeddings = np.array([[index_2_embedding[idx] for idx in input_sequence]])\n",
    "\n",
    "\t\t# get probability distribution on vocabulary for the next token in the sequence \n",
    "\t\tprediction = model.predict(input_embeddings, verbose=False)[0][-1]\n",
    "\n",
    "\t\t# sample from the probability distribution \n",
    "\t\tnext_tok_idx = np.random.choice(len(prediction), p=prediction)\n",
    "\n",
    "\t\t# skip mid-sentence SENTENCE_BEGIN and PADDING tokens\n",
    "\t\tif next_tok_idx == sentence_begin_index or next_tok_idx == padding_index:\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\t# add newly generated token to our sequence \n",
    "\t\tsequence_indices.append(next_tok_idx)\n",
    "\n",
    "\t# convert to words \n",
    "\ttokenizer_words = list(tokenizer.word_index.keys())\n",
    "\ttokenizer_indices = list(tokenizer.word_index.values())\n",
    "\tsequence = [tokenizer_words[tokenizer_indices.index(idx)] for idx in sequence_indices]\n",
    "\treturn sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating line 0 / 100\n",
      "Generating line 10 / 100\n",
      "Generating line 20 / 100\n",
      "Generating line 30 / 100\n",
      "Generating line 40 / 100\n",
      "Generating line 50 / 100\n",
      "Generating line 60 / 100\n",
      "Generating line 70 / 100\n",
      "Generating line 80 / 100\n",
      "Generating line 90 / 100\n",
      "Sample Generated Lyrics:\n",
      "\n",
      "and to to\n",
      "pain\n",
      "and ' ' chased\n",
      "and on and fear\n",
      "and find for\n",
      "and\n",
      "and you ' ' wrong\n",
      "and myself back\n",
      "and end\n",
      "\n",
      "back and may\n",
      "and ' ' ' lie\n",
      "and hate can do what is may we can do\n",
      "and\n",
      "and on now ?\n",
      "and last\n",
      "and earth\n",
      "as as you came 's\n",
      "and back\n",
      "pain\n",
      "and hate\n",
      "alive\n",
      "\n",
      "\n",
      "and ' ' ' ' days\n",
      "and back\n",
      "and shot back\n",
      "joy\n",
      "and staring now\n",
      "and die ?\n",
      "all you\n",
      "and judge\n",
      "and america\n",
      "and find back\n",
      "and target\n",
      "\n",
      "and pieces\n",
      "i close\n",
      "and rage\n",
      "\n",
      "and ' ' ' ' around disease getting caught in get down ' long ago\n",
      "\n",
      "and find the game\n",
      "and ' ' just lie\n",
      "and can have hands\n",
      "\n",
      "and only\n",
      "to i ' ' day\n",
      "like\n",
      "and ' ' '\n",
      "and back now\n",
      "it bad\n",
      "and last\n",
      "true\n",
      "and ' ' long ,\n",
      "everything ' ' just\n",
      "and everything and died warm\n",
      "and back but ' dead\n",
      "\n",
      "and end\n",
      "everything i 've more than than than than than he try give !\n",
      "and think wrong\n",
      "and down\n",
      "wrong\n",
      "and\n",
      "so\n",
      "and old old man\n",
      "and last\n",
      "and glory\n",
      "and on pain ?\n",
      "and lost\n",
      "hard and ?\n",
      "\n",
      "life ?\n",
      "\n",
      "and raid - good faith\n",
      "\n",
      "and ' ' ' right\n",
      "and back i\n",
      "and on you and hate holding\n",
      "and fear\n",
      "and day\n",
      "time\n",
      "\n",
      "and on\n",
      "\n",
      "and ' ' wan ya see there over ?\n",
      "back\n",
      "pain\n",
      "\n",
      "\n",
      "and will it will be you ' ' just run\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "and see like\n",
      "and on as fools , ? would be our way\n",
      "and find true line\n",
      "and\n"
     ]
    }
   ],
   "source": [
    "# load in model (if we have a pre-trained one that we'd like to generate sequences for)\n",
    "#if SHOULD_LOAD or True:\n",
    "    #model = keras.saving.load_model(LOAD_PATH)\n",
    "    \n",
    "# Generate new lyrics \n",
    "generated_sequences = generate_sequences(model, tokenizer, index_to_embedding, num_seq=100)\n",
    "print(\"Sample Generated Lyrics:\\n\")\n",
    "for seq in generated_sequences:\n",
    "    print(seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Perplexity \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing perplexity of line 0 / 500\n",
      "Computing perplexity of line 25 / 500\n",
      "Computing perplexity of line 50 / 500\n",
      "Computing perplexity of line 75 / 500\n",
      "Computing perplexity of line 100 / 500\n",
      "Computing perplexity of line 125 / 500\n",
      "Computing perplexity of line 150 / 500\n",
      "Computing perplexity of line 175 / 500\n",
      "Computing perplexity of line 200 / 500\n",
      "Computing perplexity of line 225 / 500\n",
      "Computing perplexity of line 250 / 500\n",
      "Computing perplexity of line 275 / 500\n",
      "Computing perplexity of line 300 / 500\n",
      "Computing perplexity of line 325 / 500\n",
      "Computing perplexity of line 350 / 500\n",
      "Computing perplexity of line 375 / 500\n",
      "Computing perplexity of line 400 / 500\n",
      "Computing perplexity of line 425 / 500\n",
      "Computing perplexity of line 450 / 500\n",
      "Computing perplexity of line 475 / 500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4401.174542943271"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_perplexity(encoded_sequence: list, model: Sequential, \n",
    "                        tokenizer: Tokenizer, \n",
    "                        index_2_embedding: dict,\n",
    "                        verbose: bool = False):\n",
    "    '''\n",
    "    Computes the perplexity of a single sequence by finding the probability that the model will generate \n",
    "    this sequence. Uses a sliding window to continuously predict the next word, and finds the softmax probability\n",
    "    associated with the true word. \n",
    "\n",
    "    Args:\n",
    "        encoded_sequence (list): a single sequence represented by its Tokenizer encodings \n",
    "        model: RNN language model\n",
    "        tokenizer: the keras preprocessing tokenizer\n",
    "        index_2_embedding: mapping from token index -> word2vec embeddings \n",
    "        verbose (bool): If true, prints information about the sequence and the probability of each word \n",
    "    Returns: \n",
    "        the perplexity of the sequence \n",
    "    ''' \n",
    "    padding_index = tokenizer.word_index.get(PADDING)\n",
    "    sentence_begin_index = tokenizer.word_index.get(SENTENCE_BEGIN)\n",
    "\n",
    "    # shift Y forward one token to represent the next word predictions \n",
    "    X_data = encoded_sequence[:-1]\n",
    "    Y_data = encoded_sequence[1:]\n",
    "\n",
    "    # seed with SEQUENCE_LENGTH - 2 sentence begins (first token in X is a sentence begin as well),\n",
    "    # inch window along to predict the next word \n",
    "    encoded_input = [sentence_begin_index] * (SEQUENCE_LENGTH - 2) + X_data\n",
    "\n",
    "    input_length = SEQUENCE_LENGTH - 1\n",
    "\n",
    "    # we will be finding the log of the probability of our model generating this sequence \n",
    "    Y_pred_log_prob = 0\n",
    "\n",
    "    # track the number of meaningful tokens \n",
    "    N = 0\n",
    "\n",
    "    # for each word in Y (all tokens except SENTENCE_BEGIN)\n",
    "    for i, y in enumerate(Y_data):\n",
    "        # use a sliding window over the input, where the input sequence are the tokens preceding y \n",
    "        input_sequence = encoded_input[i:input_length+i]\n",
    "\n",
    "        # convert the input sequence to embeddings\n",
    "        input_embeddings = np.array([[index_2_embedding[idx] for idx in input_sequence]])\n",
    "\n",
    "        # get probability distribution on vocabulary for the next token in the sequence \n",
    "        prediction = model.predict(input_embeddings, verbose=False)[0][-1]\n",
    "\n",
    "        # index y to get the predicted probability of the true value \n",
    "        y_pred_prob = prediction[y] \n",
    "\n",
    "        # print information to help with debugging \n",
    "        if verbose:\n",
    "            print(\"Encoded input sequence:\", input_sequence)\n",
    "            print(\"Encoded y to predict:\", y)\n",
    "            print(\"Probability of next token being y:\", y_pred_prob)\n",
    "\n",
    "        Y_pred_log_prob += np.log(y_pred_prob)\n",
    "\n",
    "        # only include meaningful tokens in our token count \n",
    "        if y != padding_index and y != sentence_begin_index:\n",
    "            N += 1\n",
    "\n",
    "    # compute probability of our model generating this sequence as well as the perplexity \n",
    "    Y_pred_prob = np.exp(Y_pred_log_prob)\n",
    "    perplexity = Y_pred_prob ** (-1/N)\n",
    "    \n",
    "    return perplexity\n",
    "\n",
    "\n",
    "def mean_perplexity(val_data: np.array, \n",
    "                    model: Sequential, \n",
    "                    tokenizer: Tokenizer, \n",
    "                    index_2_embedding: dict):\n",
    "    \"\"\"\" \n",
    "    Computes the average perplexity of all sequences in the given data using the given model.\n",
    "\n",
    "    Args:\n",
    "        encoded_sequence (list): a single sequence represented by its Tokenizer encodings \n",
    "        model: RNN language model\n",
    "        tokenizer: the keras preprocessing tokenizer\n",
    "        index_2_embedding: mapping from token index -> word2vec embeddings \n",
    "        verbose (bool): If true, prints information about the sequence and the probability of each word \n",
    "    Returns: \n",
    "        The average perplexity of the provided sequences \n",
    "    \"\"\"\n",
    "    total_samples = len(val_data)\n",
    "\n",
    "    perplexities = []\n",
    "    for i, seq in enumerate(val_data):\n",
    "        if i % 25 == 0:\n",
    "            print(\"Computing perplexity of line\", i, \"/\", total_samples)\n",
    "        perplexities.append(calculate_perplexity(seq, model, tokenizer, index_2_embedding))\n",
    "    return np.mean(perplexities)\n",
    "\n",
    "#TODO - probably increase, just takes about a minute for every 100 sequences \n",
    "#calculate_perplexity(encoded_sequences_val[0], model, tokenizer, index_to_embedding, verbose=True)\n",
    "mean_perplexity(encoded_sequences_val[:500], model, tokenizer, index_to_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experimenting with Hyperparameters \n",
    "\n",
    "To find our final configuration for our RNN + LSTM model, we will test out hyperparameters such as sequence length, embedding size, number of epochs, and model structure. Due to time constraints, we will run these experiments on a subset of the training and validation data (10,000 training lines and 500 validation lines). Test data is limited to control training time; validation data is limited to control perplexity evaluation time (which is a particularly slow function). \n",
    "\n",
    "-----------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "| Genre       \t| Word Embedding Size \t| Sequence Length \t| Number of Epochs \t| Model Structure                   \t| Mean Validation Perplexity \t| Generated Examples                                                                                                                                                                                                   \t|\n",
    "|-------------\t|---------------------\t|-----------------\t|------------------\t|-----------------------------------\t|----------------------------\t|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\t|\n",
    "| Country     \t| 100                 \t| 10              \t| 20               \t| 200 hidden units                  \t| 264.0667752293233          \t| my own 'em , the party<br>if the break and now she wanting o<br>crawl sign<br>look<br>used you become where warden<br>the sun .                                                                                      \t|\n",
    "| Country     \t| 50                  \t| 10              \t| 20               \t| 200 hidden units                  \t| 454.2141099684709          \t| 's among time roof tight are at<br>strong part record<br>exist divine laugh proud warm six for<br>battle porch weary hungry news shine lonely quarter ai n't the same the same boy i tear                            \t|\n",
    "| Country     \t| 150                 \t| 10              \t| 20               \t| 200 hidden units                  \t| 441.08825950748917         \t| instead half forever mat straight searchin under time<br>drop learning my finest lets voice border shining , with of<br>sinner knife might you wiped changed guess you 'll gon be drink man                          \t|\n",
    "| Country     \t| 100                 \t| 5               \t| 20               \t| 200 hidden units                  \t| 347.79992897087277         \t| what know<br>by have if wo this dear on on is shame them i 'm finally i do n't fool then stand '<br>own                                                                                                              \t|\n",
    "| Country     \t| 100                 \t| 15              \t| 20               \t| 200 hidden units                  \t| 485.91876699708564         \t| sometimes did how cried look )<br>thanks complete good-bye money go reach me<br>ah yea & born else<br>writers darlin think tried came ta without me                                                                  \t|\n",
    "| Country     \t| 100                 \t| 10              \t| 10               \t| 200 hidden units                  \t| 535.7385514266462          \t| war pigeon rail up woman river counting slow<br>but did move might anything complain clouds<br>skin special fly reaching frame valley throw down<br>darlin about seen saw fish fell chance                           \t|\n",
    "| Country     \t| 100                 \t| 10              \t| 40               \t| 200 hidden units                  \t| 688.1449810877316          \t| mistakes unfair ; from seasons<br>going ! where outside ring asphalt honky-tonk wool<br>my graveside weight crying fill enough<br>half try this graveside feather been closed better tonight me                      \t|\n",
    "| Country     \t| 100                 \t| 10              \t| 40               \t| 200 hidden units<br>+ 0.2 Dropout \t| 688.9395738510657          \t| we 're through with you say may you fill care '<br>woman leave hurry survived solid love<br>tender a serious sill kicked eyed thin<br>almost forty loves then be .                                                   \t|\n",
    "|             \t|                     \t|                 \t|                  \t|                                   \t|                            \t|                                                                                                                                                                                                                      \t|\n",
    "| Heavy Metal \t| 100                 \t| 10              \t| 20               \t| 200 hidden units                  \t| 989.6465879561613          \t| teach sharp voice impossible into throat<br>he swim hunter mystery been still forever<br>( bought ^ curse wolf hurt slowly agree<br>cast thee l. friend some<br>another without flies lie killed toy depressed       \t|\n",
    "| Heavy Metal \t| 50                  \t| 10              \t| 20               \t| 200 hidden units                  \t| 682.696828268074           \t| thinking minority most remains leap countdown horizon fear , turn your<br>blade called<br>heading clean power ages shall minority ending brown<br>lord part hand this force                                          \t|\n",
    "| Heavy Metal \t| 150                 \t| 10              \t| 20               \t| 200 hidden units                  \t| 547.4438419767572          \t| yo understood odin malice leader lair glass place faith , the only blood<br>and we all on energy ,<br>holding writhing ai line worship to fight to the , to my mission<br>nothing happened itself feed to prove odds \t|\n",
    "| Heavy Metal \t| 100                 \t| 5               \t| 20               \t| 200 hidden units                  \t| 333.9428479548894          \t| bend will becoming<br>we and you told their the the voice heart ] standing gabrielle the<br>i will boys listen overload inside<br>of of more                                                                         \t|\n",
    "| Heavy Metal \t| 100                 \t| 15              \t| 20               \t| 200 hidden units                  \t| 845.0837951396505          \t| but made<br>honey pick tongues<br>panic spirit none dirt wall thee drifted baby<br>soon -i<br>jump                                                                                                                   \t|\n",
    "| Heavy Metal \t| 100                 \t| 10              \t| 10               \t| 200 hidden units                  \t| 557.8584412735338          \t| crime bed kill crowned fuel died break<br>wonder but stronger incendiary perfect angry water scenes<br>with enough chide wipe and horizon past ashes cause<br>valiant .. yo blind courageous sacrifice pass          \t|\n",
    "| Heavy Metal \t| 100                 \t| 10              \t| 40               \t| 200 hidden units                  \t| 555.9454288149526          \t| foul back coming 're superheroes<br>glow front ride<br>true faith to win great all there coming to the floor )                                                                                                       \t|\n",
    "| Heavy Metal \t| 100                 \t| 10              \t| 40               \t| 200 hidden units<br>+ 0.2 Dropout \t| 523.1969617748815          \t| he high set i divide free<br>believer heavy gone hand throat brown<br>well made control leap and<br>lead before long then naked if there tried                                                                       \t|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key Findings \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
