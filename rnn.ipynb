{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN with LSTMs Language Model using Skip-Gram Dense Embeddings\n",
    "\n",
    "This notebook trains a keras Recurrent Neural Network with Long Short-Term Memory units, generates sequences with them, and computes perplexity on validation data.\n",
    "\n",
    "Hyperparameters may be changed to test different model configurations, and the GENRE constant may be set to either \"country\" or \"metal\", which determines the datasets used. To save a model, set SAVE_PATH to the desired location and SHOULD_SAVE to True. To load in a previously trained model for generation or perplexity calculations, set LOAD_PATH to the location of the desired model and SHOULD_LOAD to True. If using a pre-trained model, hyperparameters should correspond appropriately to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports \n",
    "from gensim.models import Word2Vec\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter \n",
    "from itertools import chain\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Bidirectional\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants \n",
    "SENTENCE_BEGIN = \"<s>\"\n",
    "SENTENCE_END = \"</s>\"\n",
    "PADDING = \"<pad>\"\n",
    "UNK = \"<unk>\"\n",
    "\n",
    "# hyperparameters - may change \n",
    "EMBEDDINGS_SIZE = 100\n",
    "BATCH_SIZE = 128\n",
    "SEQUENCE_LENGTH = 10\n",
    "NUM_EPOCHS = 20\n",
    "\n",
    "# may either be country or metal\n",
    "GENRE = 'metal' \n",
    "\n",
    "# change to save a newly trained model\n",
    "SAVE_PATH = 'foo'\n",
    "SHOULD_SAVE = False \n",
    "\n",
    "# change to load in an already trained model for sequence generation \n",
    "LOAD_PATH = 'foo'\n",
    "SHOULD_LOAD = True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data\n",
    "if GENRE == 'country':\n",
    "    train_lines = pd.read_csv('data/country_train.csv', header=None)[0].tolist()\n",
    "    val_lines = pd.read_csv('data/country_val.csv', header=None)[0].tolist()\n",
    "    test_lines = pd.read_csv('data/country_test.csv', header=None)[0].tolist()\n",
    "\n",
    "elif GENRE == 'metal':\n",
    "    train_lines = pd.read_csv('data/metal_train.csv', header=None)[0].tolist()\n",
    "    val_lines = pd.read_csv('data/metal_val.csv', header=None)[0].tolist()\n",
    "    test_lines = pd.read_csv('data/metal_test.csv', header=None)[0].tolist()\n",
    "\n",
    "else:\n",
    "    raise ValueError('Incorrect genre given.')\n",
    "\n",
    "\n",
    "print(\"Number of training lines:\", len(train_lines))\n",
    "print(\"Number of validation lines:\", len(val_lines))\n",
    "print(\"Number of test lines:\", len(test_lines))\n",
    "print('Lyric Example:', train_lines[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you wish to limit data (for instance to make mean validation calculations faster, do so here)\n",
    "\n",
    "#train_lines = train_lines[:10000] -- 10,000 lines used for hyperparameter tuning experiments \n",
    "#val_lines = val_lines[:500]  -- 500 lines used for validation perplexity during hyperparameter tuning experiments \n",
    "#test_lines = test_lines[:1000] -- 1,000 lines used for mean test perplexity (time constraint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation: Tokenize Lyrics, Pad Sequences, Create Dense Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize and add a single sentence start and end token around each sequence \n",
    "train_tokens = [utils.tokenize_line(line, ngram=1) for line in train_lines] \n",
    "val_tokens = [utils.tokenize_line(line, ngram=1) for line in val_lines] \n",
    "test_tokens = [utils.tokenize_line(line, ngram=1) for line in test_lines] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot lengths of lyric lines to help determine an appropriate length to pad/truncate to\n",
    "train_sequence_lengths = [len(seq) for seq in train_tokens]\n",
    "\n",
    "print(\"Mean Length:\", np.mean(train_sequence_lengths))\n",
    "print(\"Median Length:\", np.median(train_sequence_lengths))\n",
    "print(\"90th Percentile Length:\", np.percentile(train_sequence_lengths, 90))\n",
    "print(\"Max Length:\", np.max(train_sequence_lengths))\n",
    "\n",
    "plt.hist(train_sequence_lengths, bins=50)\n",
    "plt.xlabel(\"Length of Lyric Line\")\n",
    "plt.ylabel(\"Number of Lines\")\n",
    "plt.title(\"Distribution of Line Length for Lyrics in Training Data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_sequence_length(tokenized_seqs: list, sequence_length: int = SEQUENCE_LENGTH) -> list:\n",
    "    \"\"\"\n",
    "    Pads or truncates all sequences in the provided list to the same length. \n",
    "    Adds padding tokens to the left for too-short sequences and truncates to the right \n",
    "    for too-long sequences (method based on experimentation with left/right padding/truncation)\n",
    "\n",
    "    Args:\n",
    "        tokenized_seqs (list): A list of lists of tokens. Each inner list represents a sequence with tokens as elements\n",
    "        sequence_length (int): The desired length for all of the sequences\n",
    "        padding_token (str): The token that should be used to pad short sequences to the proper length\n",
    "\n",
    "    Returns:\n",
    "        size_adjusted_sequences (list): A list of lists of tokens, where each inner list is the same length\n",
    "    \"\"\"\n",
    "    size_adjusted_sequences = []\n",
    "    for sequence in tokenized_seqs:\n",
    "        if len(sequence) < sequence_length:\n",
    "            # too short, add padding\n",
    "            num_padding = sequence_length - len(sequence)\n",
    "            size_adjusted_sequences.append( ([PADDING] * num_padding) + sequence)\n",
    "        else:\n",
    "            # truncate sequences longer than the chosen length \n",
    "            size_adjusted_sequences.append(sequence[:sequence_length])\n",
    "            \n",
    "\n",
    "    return size_adjusted_sequences\n",
    "\n",
    "\n",
    "def replace_unknowns_train(tokenized_seqs: list) -> list:\n",
    "    \"\"\"\"\n",
    "    Replaces words that occur only once with an UNK token\n",
    "\n",
    "    Args:\n",
    "        tokenized_seqs (list): A list of lists of tokens. Each inner list represents a sequence with tokens as elements\n",
    "\n",
    "    Returns:\n",
    "        Tokenized sequences with low frequency words replaced with the unknown special token \n",
    "    \"\"\"\n",
    "    # concatenate all sequences together \n",
    "    all_tokens = list(chain(*tokenized_seqs))\n",
    "    token_counts = Counter(all_tokens)\n",
    "\n",
    "    # Replace words with low frequencies to UNK so that we can calculate perplexity on test data with unknown words \n",
    "    cleaned_tokenized_seqs = []\n",
    "    for seq in tokenized_seqs:\n",
    "        cleaned_seq = [tok if token_counts[tok] > 1 else UNK for tok in seq]\n",
    "        cleaned_tokenized_seqs.append(cleaned_seq)\n",
    "\n",
    "    return cleaned_tokenized_seqs\n",
    "\n",
    "\n",
    "size_adjusted_sequences_train = adjust_sequence_length(train_tokens)\n",
    "cleaned_sequences_train = replace_unknowns_train(size_adjusted_sequences_train)\n",
    "print(\"Number of sequences:\", len(cleaned_sequences_train))\n",
    "print(\"Length of Sequences:\", len(cleaned_sequences_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Tokenizer to map each token to a unique index \n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(cleaned_sequences_train)\n",
    "encoded_sequences_train = tokenizer.texts_to_sequences(cleaned_sequences_train)\n",
    "\n",
    "print(\"Vocab Size:\", len(tokenizer.word_index))\n",
    "print('encoded examples:', '\\n', encoded_sequences_train[0], '\\n', encoded_sequences_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create word embeddings using skip gram algorithm\n",
    "word_embeddings = Word2Vec(sentences=cleaned_sequences_train, vector_size=EMBEDDINGS_SIZE, window=5, sg=1, min_count=1)\n",
    "print('Vocab size for word embeddings:', len(word_embeddings.wv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that gives mappings from words to their embeddings and  \n",
    "# indexes from the tokenizers to their embeddings\n",
    "\n",
    "def map_embeddings(embeddings: Word2Vec, tokenizer: Tokenizer) -> (dict, dict):\n",
    "    ''' Creates mappings between different token representations \n",
    "    Arguments:\n",
    "        embeddings: Word2Vec word embeddings for the data (maps tokens to embedding vectors)\n",
    "        tokenizer: Tokenizer used to tokenize the data (maps token to index)\n",
    "    Returns:\n",
    "        (dict): mapping from word to its embedding vector\n",
    "        (dict): mapping from index to its embedding vector\n",
    "    '''\n",
    "    # initialize dictionaries \n",
    "    token_to_embedding = {}\n",
    "    index_to_embedding = {}\n",
    "\n",
    "    # tokenizer maps tokens to unique indices \n",
    "    for token, index in tokenizer.word_index.items():\n",
    "        embedding = embeddings[token]\n",
    "\n",
    "        token_to_embedding[token] = embedding\n",
    "        index_to_embedding[index] = embedding\n",
    "\n",
    "    return (token_to_embedding, index_to_embedding)\n",
    "\n",
    "\n",
    "token_to_embedding, index_to_embedding = map_embeddings(word_embeddings.wv, tokenizer)\n",
    "\n",
    "# Fill in unused index zero to avoid dimension mismatch\n",
    "index_to_embedding[0] = [0] * EMBEDDINGS_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Samples for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(data: list, num_sequences_per_batch: int, index_2_embedding: dict) -> (np.array, np.array):\n",
    "    '''\n",
    "    Returns a data generator to train the neural network in batches\n",
    "\n",
    "    X data will be represented in embedding form.\n",
    "    Y data will be represented with one hot vectors. \n",
    "\n",
    "    Args:\n",
    "    data (list of lists): tokenized sequences represented by their unique index encodings \n",
    "    num_sequences_per_batch (int): batch size yielded on each iteration of the generator \n",
    "    index_2_embedding (dict): mapping between unique token indices and dense word embeddings \n",
    "\n",
    "    Returns:\n",
    "    X_batch_embeddings (3-D numpy array): sequences of embeddings with dimensions (batch size, num timesteps, embedding size)\n",
    "                                          Take the first (SEQUENCE_LENGTH - 1) tokens of each sequence\n",
    "    y_batch (3-D numpy array): sequences of one hot vectors with dimensions (batch size, num timesteps, vocab size)\n",
    "                                          Take the last (SEQUENCE_LENGTH - 1) tokens of each sequence \n",
    "                                          (X shifted forward one token so that the neural net predicts \n",
    "                                          the next word in the sequence for each timestep)\n",
    "    '''\n",
    "    # iterate over data in batches - stored in the form of unique token indices \n",
    "    i = 0\n",
    "    while True:\n",
    "        # get samples that we'd like to train on for this batch \n",
    "        data_batch = data[i:i+num_sequences_per_batch]\n",
    "\n",
    "        # increment i with each batch \n",
    "        i += num_sequences_per_batch\n",
    "\n",
    "        # split into X and Y -- shifted sequence so that for each timestep, Y is the token that follows X \n",
    "        X_data = [sequence[:-1] for sequence in data_batch]\n",
    "        Y_data = [sequence[1:] for sequence in data_batch]\n",
    "\n",
    "        # get embeddings for X data \n",
    "        X_embeddings = []\n",
    "        for X_sequence in X_data:\n",
    "            X_sequence_embeddings = [index_2_embedding[token_idx] for token_idx in X_sequence]\n",
    "            X_embeddings.append(X_sequence_embeddings)\n",
    "\n",
    "        # get one hot vectors for Y data \n",
    "        Y_one_hot_vectors = []\n",
    "        for Y_sequence in Y_data:\n",
    "            Y_one_hot = to_categorical(Y_sequence, num_classes=len(index_2_embedding))\n",
    "            Y_one_hot_vectors.append(Y_one_hot)\n",
    "\n",
    "        # yield statement instead of return for generator \n",
    "        yield(np.array(X_embeddings), np.array(Y_one_hot_vectors))\n",
    "\n",
    "\n",
    "# demo the data generator\n",
    "demo_data_generator = data_generator(encoded_sequences_train, BATCH_SIZE, index_to_embedding)\n",
    "demo_sample = next(demo_data_generator)\n",
    "print(\"X batch shape:\", demo_sample[0].shape)\n",
    "print(\"y batch shape:\", demo_sample[1].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_rnn(train_data: np.array,\n",
    "             index_2_embedding: dict, \n",
    "             num_epochs: int=1, \n",
    "             num_sequences_per_batch: int=BATCH_SIZE, \n",
    "             sequence_length: int=SEQUENCE_LENGTH,\n",
    "             embedding_size: int=EMBEDDINGS_SIZE):\n",
    "    \"\"\"\n",
    "    Creates and trains an RNN with LSTM cells using given training data and batch size.\n",
    "\n",
    "    Args:\n",
    "        train_data (list of lists): encoded sequences of training data represented by token indices \n",
    "        index_2_embedding (dict): mapping from token index -> word2vec embeddings \n",
    "        num_epochs (int): number of training epochs\n",
    "        num_sequences_per_batch (int): batch size for training data \n",
    "        sequence_length (int): number of tokens in each training sample \n",
    "        embedding_size (int): size of the dense word embeddings used to represent tokens \n",
    "    Returns:\n",
    "        A trained Neural Network language model\n",
    "    \"\"\"\n",
    "    # define model parameters\n",
    "    hidden_units = 200\n",
    "    hidden_input_dim = (sequence_length - 1, embedding_size)      # (number of steps, number of features per step)\n",
    "    output_dim = len(index_2_embedding)                            # vocab size \n",
    "\n",
    "    # instantiate model\n",
    "    model = Sequential()\n",
    "\n",
    "    # hidden layer\n",
    "    model.add(Bidirectional(LSTM(hidden_units, \n",
    "                                 input_shape=hidden_input_dim,\n",
    "                                 return_sequences=True)))\n",
    "\n",
    "    # output layer\n",
    "    model.add(Dense(units=output_dim, activation='softmax'))\n",
    "\n",
    "    # configure the learning process\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=[\"top_k_categorical_accuracy\"])\n",
    "    \n",
    "    # total number of batches per epoch \n",
    "    steps_per_epoch = len(train_data)//num_sequences_per_batch\n",
    "   \n",
    "    for i in range(num_epochs):\n",
    "        if i % 5 == 0:\n",
    "            print(\"Epoch\", i)\n",
    "\n",
    "        # create a new data generator for us to iterate through\n",
    "        train_generator = data_generator(train_data, num_sequences_per_batch, index_2_embedding)\n",
    "\n",
    "        # train model \n",
    "        model.fit(x=train_generator, steps_per_epoch=steps_per_epoch)\n",
    "\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and train model\n",
    "model = lstm_rnn(np.array(encoded_sequences_train), index_to_embedding, num_epochs=NUM_EPOCHS)\n",
    "\n",
    "# save trained model \n",
    "if SHOULD_SAVE:\n",
    "    model.save(SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create functions to generate new sequences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sequences(model: Sequential, \n",
    "\t\t\t\t\t  tokenizer: Tokenizer, \n",
    "\t\t\t\t\t  index_2_embedding: dict, \n",
    "\t\t\t\t\t  num_seq: int,\n",
    "\t\t\t\t\t  verbose: bool = True,\n",
    "\t\t\t\t\t  file_path: str = None):\n",
    "\t'''\n",
    "\tGenerates a given number of sequences using the given RNN language model.\n",
    "\tWill begin the sequence generation with n-1 SENTENCE_BEGIN tokens.\n",
    "\tReturned sequences will have the BEGIN, END, and PADDING tokens removed\n",
    "\n",
    "\tWrites generated sequences to file_path, if provided \n",
    "\n",
    "\tArgs:\n",
    "\t\tmodel: RNN language model\n",
    "\t\ttokenizer: the keras preprocessing tokenizer\n",
    "\t\tindex_2_embedding: mapping from token index -> word2vec embeddings \n",
    "\t\tnum_seq: the number of sequences to generate \n",
    "\t\tverbose: If True, prints progress of sequence generation \n",
    "\t\tfile_path (str): path of file to write the generated sequences to, with one sequence per line \n",
    "\n",
    "\tReturns: \n",
    "\t\tA list of strings, where each string is a generated sequence with special tokens removed \n",
    "\t'''\n",
    "\tseed = [SENTENCE_BEGIN] * (SEQUENCE_LENGTH - 1) \n",
    "\t\n",
    "\tsequences = []\n",
    "\tfor i in range(num_seq):\n",
    "\t\t# print progress \n",
    "\t\tif verbose and i % 10 == 0:\n",
    "\t\t\tprint(\"Generating line\", i, \"/\", num_seq)\n",
    "\n",
    "\n",
    "\t\tseq = generate_seq(model, tokenizer, index_2_embedding, seed)\n",
    "\t\tseq = ' '.join(seq)\n",
    "\n",
    "\t\t# remove special tokens\n",
    "\t\tseq = seq.replace(SENTENCE_BEGIN, '')\n",
    "\t\tseq = seq.replace(SENTENCE_END, '')\n",
    "\t\tseq = seq.replace(PADDING, '')\n",
    "\t\tseq = seq.replace(UNK, '')\n",
    "\n",
    "\t\tsequences.append(seq.strip())\n",
    "\n",
    "\tif file_path is not None:\n",
    "\t\twith open(file_path, 'w') as f:\n",
    "\t\t\tfor seq in sequences:\n",
    "\t\t\t\tf.write(seq + '\\n')\n",
    "\t\t\n",
    "\treturn sequences\n",
    "\n",
    "\n",
    "def generate_seq(model: Sequential, \n",
    "\t\t\t\t tokenizer: Tokenizer, \n",
    "\t\t\t\t index_2_embedding: dict, \n",
    "\t\t\t\t seed: list):\n",
    "\t'''\n",
    "\tGenerates a single sequence using the given model starting with a SENTENCE_BEGIN and ending with a SENTENCE_END token. \n",
    "\tSince an RNN takes input sequences of fixed length, use a sliding window to continually predict the next word. \n",
    "\n",
    "\tArgs:\n",
    "\t\tmodel: RNN language model\n",
    "\t\ttokenizer: the keras preprocessing tokenizer\n",
    "\t\tindex_2_embedding: mapping from token index -> word2vec embeddings \n",
    "\t\tseed: the initial tokens to feed the RNN\n",
    "\tReturns: \n",
    "\t\tAn array of tokens representing a sequence \n",
    "\t'''\n",
    "\tpadding_index = tokenizer.word_index.get(PADDING)\n",
    "\tsentence_begin_index = tokenizer.word_index.get(SENTENCE_BEGIN)\n",
    "\tsentence_end_index = tokenizer.word_index.get(SENTENCE_END)\n",
    "\n",
    "\t# track the token encodings for the sequence \n",
    "\tsequence_indices = [tokenizer.word_index.get(tok) for tok in seed] \n",
    "\n",
    "\t# number of timesteps that the model expects as input\n",
    "\tinput_length = SEQUENCE_LENGTH - 1\n",
    "\n",
    "\t# until we get a SENTENCE_END token\n",
    "\twhile sequence_indices[-1] != sentence_end_index:\n",
    "\t\t# get latest tokens to use as inputs \n",
    "\t\tinput_sequence = sequence_indices[-1*input_length:]\n",
    "\n",
    "\t\t# convert the input sequence to embeddings\n",
    "\t\tinput_embeddings = np.array([[index_2_embedding[idx] for idx in input_sequence]])\n",
    "\n",
    "\t\t# get probability distribution on vocabulary for the next token in the sequence \n",
    "\t\tprediction = model.predict(input_embeddings, verbose=False)[0][-1]\n",
    "\n",
    "\t\t# sample from the probability distribution \n",
    "\t\tnext_tok_idx = np.random.choice(len(prediction), p=prediction)\n",
    "\n",
    "\t\t# skip mid-sentence SENTENCE_BEGIN and PADDING tokens\n",
    "\t\tif next_tok_idx == sentence_begin_index or next_tok_idx == padding_index:\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\t# add newly generated token to our sequence \n",
    "\t\tsequence_indices.append(next_tok_idx)\n",
    "\n",
    "\t# convert to words \n",
    "\ttokenizer_words = list(tokenizer.word_index.keys())\n",
    "\ttokenizer_indices = list(tokenizer.word_index.values())\n",
    "\tsequence = [tokenizer_words[tokenizer_indices.index(idx)] for idx in sequence_indices]\n",
    "\treturn sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in model (if we have a pre-trained one that we'd like to generate sequences for)\n",
    "if SHOULD_LOAD:\n",
    "    model = keras.saving.load_model(LOAD_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate new lyrics \n",
    "generated_sequences = generate_sequences(model, tokenizer, index_to_embedding, num_seq=10)\n",
    "print()\n",
    "print(\"Sample Generated Lyrics:\\n\")\n",
    "for seq in generated_sequences:\n",
    "    print(seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Perplexity \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare validation data\n",
    "# Since the model does not know about these sequences beforehand, \n",
    "# unknown words are those that do not appear in the training vocabulary \n",
    "def encode_new_sequences(tokenized_seqs: list, tokenizer) -> list:\n",
    "    \"\"\"\"\n",
    "    Replaces words that are not in the tokenizer's vocab with the unknown special token and encodes it to \n",
    "    unique token indices specified by the provided Tokenizer.\n",
    "\n",
    "    Args:\n",
    "        tokenized_seqs (list): A list of lists of tokens. Each inner list represents a sequence with tokens as elements\n",
    "        tokenizer: Tokenizer used maps token to index\n",
    "\n",
    "    Returns:\n",
    "        Encoded sequences with words not in the training vocabulary replaced with the unknown special token \n",
    "    \"\"\"\n",
    "    cleaned_tokenized_seqs = []\n",
    "    for seq in tokenized_seqs:\n",
    "        cleaned_seq = [tok if tok in tokenizer.word_index.keys() else UNK for tok in seq]\n",
    "        cleaned_tokenized_seqs.append(cleaned_seq)\n",
    "\n",
    "    return tokenizer.texts_to_sequences(cleaned_tokenized_seqs)\n",
    "\n",
    "encoded_sequences_val = encode_new_sequences(val_tokens, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity(encoded_sequence: list, model: Sequential, \n",
    "                        tokenizer: Tokenizer, \n",
    "                        index_2_embedding: dict,\n",
    "                        verbose: bool = False):\n",
    "    '''\n",
    "    Computes the perplexity of a single sequence by finding the probability that the model will generate \n",
    "    this sequence. Uses a sliding window to continuously predict the next word, and finds the softmax probability\n",
    "    associated with the true word. \n",
    "\n",
    "    Args:\n",
    "        encoded_sequence (list): a single sequence represented by its Tokenizer encodings \n",
    "        model: RNN language model\n",
    "        tokenizer: the keras preprocessing tokenizer\n",
    "        index_2_embedding: mapping from token index -> word2vec embeddings \n",
    "        verbose (bool): If true, prints information about the sequence and the probability of each word \n",
    "    Returns: \n",
    "        the perplexity of the sequence \n",
    "    ''' \n",
    "    padding_index = tokenizer.word_index.get(PADDING)\n",
    "    sentence_begin_index = tokenizer.word_index.get(SENTENCE_BEGIN)\n",
    "\n",
    "    # shift Y forward one token to represent the next word predictions \n",
    "    X_data = encoded_sequence[:-1]\n",
    "    Y_data = encoded_sequence[1:]\n",
    "\n",
    "    # seed with SEQUENCE_LENGTH - 2 sentence begins (first token in X is a sentence begin as well),\n",
    "    # inch window along to predict the next word \n",
    "    encoded_input = [sentence_begin_index] * (SEQUENCE_LENGTH - 2) + X_data\n",
    "\n",
    "    input_length = SEQUENCE_LENGTH - 1\n",
    "\n",
    "    # we will be finding the log of the probability of our model generating this sequence \n",
    "    Y_pred_log_prob = 0\n",
    "\n",
    "    # track the number of meaningful tokens \n",
    "    N = 0\n",
    "\n",
    "    # for each word in Y (all tokens in sequence except SENTENCE_BEGIN)\n",
    "    for i, y in enumerate(Y_data):\n",
    "        # use a sliding window over the input, where the input sequence are the tokens preceding y \n",
    "        input_sequence = encoded_input[i:input_length+i]\n",
    "\n",
    "        # convert the input sequence to embeddings\n",
    "        input_embeddings = np.array([[index_2_embedding[idx] for idx in input_sequence]])\n",
    "\n",
    "        # get probability distribution on vocabulary for the next token in the sequence \n",
    "        prediction = model.predict(input_embeddings, verbose=False)[0][-1]\n",
    "\n",
    "        # index y to get the predicted probability of the true value \n",
    "        y_pred_prob = prediction[y] \n",
    "\n",
    "        # print information to help with debugging \n",
    "        if verbose:\n",
    "            print(\"Encoded input sequence:\", input_sequence)\n",
    "            print(\"Encoded y to predict:\", y)\n",
    "            print(\"Probability of next token being y:\", y_pred_prob)\n",
    "\n",
    "        Y_pred_log_prob += np.log(y_pred_prob)\n",
    "\n",
    "        # only include meaningful tokens in our token count \n",
    "        if y != padding_index and y != sentence_begin_index:\n",
    "            N += 1\n",
    "\n",
    "    # compute probability of our model generating this sequence as well as the perplexity \n",
    "    Y_pred_prob = np.exp(Y_pred_log_prob)\n",
    "    perplexity = Y_pred_prob ** (-1/N)\n",
    "    \n",
    "    return perplexity\n",
    "\n",
    "\n",
    "def mean_perplexity(val_data: np.array, \n",
    "                    model: Sequential, \n",
    "                    tokenizer: Tokenizer, \n",
    "                    index_2_embedding: dict):\n",
    "    \"\"\"\" \n",
    "    Computes the average perplexity of all sequences in the given data using the given model.\n",
    "\n",
    "    Args:\n",
    "        encoded_sequence (list): a single sequence represented by its Tokenizer encodings \n",
    "        model: RNN language model\n",
    "        tokenizer: the keras preprocessing tokenizer\n",
    "        index_2_embedding: mapping from token index -> word2vec embeddings \n",
    "        verbose (bool): If true, prints information about the sequence and the probability of each word \n",
    "    Returns: \n",
    "        The average perplexity of the provided sequences \n",
    "    \"\"\"\n",
    "    total_samples = len(val_data)\n",
    "\n",
    "    perplexities = []\n",
    "    for i, seq in enumerate(val_data):\n",
    "        if i % 100 == 0:\n",
    "            print(\"Computing perplexity of line\", i, \"/\", total_samples)\n",
    "        perplexities.append(calculate_perplexity(seq, model, tokenizer, index_2_embedding))\n",
    "    return np.mean(perplexities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate_perplexity(encoded_sequences_val[0], model, tokenizer, index_to_embedding, verbose=True) - can debug using a single perplexity calculation\n",
    "mean_perplexity(encoded_sequences_val, model, tokenizer, index_to_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experimenting with Hyperparameters \n",
    "\n",
    "To find our final configuration for our RNN + LSTM model, we will test out hyperparameters such as sequence length, embedding size, number of epochs, and model structure. Due to time constraints, we will run these experiments on a subset of the training and validation data (10,000 training lines and 500 validation lines). Test data is limited to control training time; validation data is limited to control perplexity evaluation time.\n",
    "\n",
    "\n",
    "#### Notable Observations \n",
    "\n",
    "- Interestingly, some of the configurations that lead to the high and low perplexities between the two genres are different \n",
    "    - Country has the lowest mean perplexity (~264) with embedding size 100, sequence length 10, 20 epochs, and 200 hidden units. Meanwhile, heavy metal has the highest mean perplexity (~990) with this configuration.\n",
    "-  To give each model its best chance at success, we will use different configurations for country and heavy metal based on their validation perplexities. There doesn't seem to be a set of configurations with particularly superior results, so human judgment was less of a guiding factor in this decision.\n",
    "\n",
    "\n",
    "- Based on these results, we will use the following configurations:\n",
    "\n",
    "Country: word embedding size 100, sequence length 10, 20 epochs, 200 hidden units\\\n",
    "Heavy Metal: word embedding size 100, sequence length 5, 20 epochs, 200 hidden units\n",
    "\n",
    "\n",
    "-----------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "| Genre       \t| Word Embedding Size \t| Sequence Length \t| Number of Epochs \t| Model Structure                   \t| Mean Validation Perplexity \t| Generated Examples                                                                                                                                                                                                   \t|\n",
    "|-------------\t|---------------------\t|-----------------\t|------------------\t|-----------------------------------\t|----------------------------\t|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\t|\n",
    "| Country     \t| 100                 \t| 10              \t| 20               \t| 200 hidden units                  \t| 264.0667752293233          \t| my own 'em , the party<br>if the break and now she wanting<br>why does me know who wanted wanted<br>love you see<br>there 's untrue                                                                                \t|\n",
    "| Country     \t| 50                  \t| 10              \t| 20               \t| 200 hidden units                  \t| 454.2141099684709          \t| 's among time roof tight are at<br>strong part record<br>exist divine laugh proud warm six for<br>battle porch weary hungry news shine lonely quarter ai n't the same the same boy i tear                            \t|\n",
    "| Country     \t| 150                 \t| 10              \t| 20               \t| 200 hidden units                  \t| 441.08825950748917         \t| instead half forever mat straight searchin under time<br>drop learning my finest lets voice border shining , with of<br>sinner knife might you wiped changed guess you 'll gon be drink man                          \t|\n",
    "| Country     \t| 100                 \t| 5               \t| 20               \t| 200 hidden units                  \t| 347.79992897087277         \t| what know<br>by have if wo this dear on on is shame them i 'm finally i do n't fool then stand '<br>own                                                                                                              \t|\n",
    "| Country     \t| 100                 \t| 15              \t| 20               \t| 200 hidden units                  \t| 485.91876699708564         \t| sometimes did how cried look )<br>thanks complete good-bye money go reach me<br>ah yea & born else<br>writers darlin think tried came ta without me                                                                  \t|\n",
    "| Country     \t| 100                 \t| 10              \t| 10               \t| 200 hidden units                  \t| 535.7385514266462          \t| war pigeon rail up woman river counting slow<br>but did move might anything complain clouds<br>skin special fly reaching frame valley throw down<br>darlin about seen saw fish fell chance                           \t|\n",
    "| Country     \t| 100                 \t| 10              \t| 40               \t| 200 hidden units                  \t| 688.1449810877316          \t| mistakes unfair ; from seasons<br>going ! where outside ring asphalt honky-tonk wool<br>my graveside weight crying fill enough<br>half try this graveside feather been closed better tonight me                      \t|\n",
    "| Country     \t| 100                 \t| 10              \t| 40               \t| 200 hidden units<br>+ 0.2 Dropout \t| 688.9395738510657          \t| we 're through with you say may you fill care '<br>woman leave hurry survived solid love<br>tender a serious sill kicked eyed thin<br>almost forty loves then be .                                                   \t|\n",
    "|             \t|                     \t|                 \t|                  \t|                                   \t|                            \t|                                                                                                                                                                                                                      \t|\n",
    "| Heavy Metal \t| 100                 \t| 10              \t| 20               \t| 200 hidden units                  \t| 989.6465879561613          \t| teach sharp voice impossible into throat<br>he swim hunter mystery been still forever<br>( bought ^ curse wolf hurt slowly agree<br>cast thee l. friend some<br>another without flies lie killed toy depressed       \t|\n",
    "| Heavy Metal \t| 50                  \t| 10              \t| 20               \t| 200 hidden units                  \t| 682.696828268074           \t| thinking minority most remains leap countdown horizon fear , turn your<br>blade called<br>heading clean power ages shall minority ending brown<br>lord part hand this force                                          \t|\n",
    "| Heavy Metal \t| 150                 \t| 10              \t| 20               \t| 200 hidden units                  \t| 547.4438419767572          \t| yo understood odin malice leader lair glass place faith , the only blood<br>and we all on energy ,<br>holding writhing ai line worship to fight to the , to my mission<br>nothing happened itself feed to prove odds \t|\n",
    "| Heavy Metal \t| 100                 \t| 5               \t| 20               \t| 200 hidden units                  \t| 333.9428479548894          \t| bend will becoming<br>we and you told their the the voice heart ] standing gabrielle the<br>i will boys listen overload inside<br>of of more                                                                         \t|\n",
    "| Heavy Metal \t| 100                 \t| 15              \t| 20               \t| 200 hidden units                  \t| 845.0837951396505          \t| but made<br>honey pick tongues<br>panic spirit none dirt wall thee drifted baby<br>soon -i<br>jump                                                                                                                   \t|\n",
    "| Heavy Metal \t| 100                 \t| 10              \t| 10               \t| 200 hidden units                  \t| 557.8584412735338          \t| crime bed kill crowned fuel died break<br>wonder but stronger incendiary perfect angry water scenes<br>with enough chide wipe and horizon past ashes cause<br>valiant .. yo blind courageous sacrifice pass          \t|\n",
    "| Heavy Metal \t| 100                 \t| 10              \t| 40               \t| 200 hidden units                  \t| 555.9454288149526          \t| foul back coming 're superheroes<br>glow front ride<br>true faith to win great all there coming to the floor )                                                                                                       \t|\n",
    "| Heavy Metal \t| 100                 \t| 10              \t| 40               \t| 200 hidden units<br>+ 0.2 Dropout \t| 523.1969617748815          \t| he high set i divide free<br>believer heavy gone hand throat brown<br>well made control leap and<br>lead before long then naked if there tried                                                                       \t|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Mean Test Perplexity on the Chosen Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use LOAD_PATH and SHOULD_LOAD with the correct configuration\n",
    "encoded_sequences_test = encode_new_sequences(test_tokens, tokenizer)\n",
    "mean_test_perplexity = mean_perplexity(encoded_sequences_test, model, tokenizer, index_to_embedding)\n",
    "print(mean_test_perplexity)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
