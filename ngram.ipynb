{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-Gram Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ngram_laplace_lm_model as lm\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "\n",
    "import utils "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants \n",
    "NGRAM = 3\n",
    "NUM_SEQ_TO_GENERATE = 10 # how many lines to generate \n",
    "VERBOSE = True\n",
    "\n",
    "SENTENCE_BEGIN = \"<s>\"\n",
    "SENTENCE_END = \"</s>\"\n",
    "NEWLINE = \" NEW \"\n",
    "\n",
    "# filepaths \n",
    "country_train_filepath = \"country_train.csv\"\n",
    "country_val_filepath = \"country_val.csv\"\n",
    "\n",
    "metal_train_filepath = \"metal_train.csv\"\n",
    "meta_val_filepath = \"metal_val.csv\"\n",
    "\n",
    "# savepaths \n",
    "\n",
    "\n",
    "# True to train by groups of lines, False to train by single lines \n",
    "BY_VERSE = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training lines for Country: 149771\n",
      "Number of validation lines for Country: 18610\n",
      "\n",
      "Number of training lines for Heavy Metal: 149771\n",
      "Number of validation lines for Heavy Metal: 18610\n"
     ]
    }
   ],
   "source": [
    "# read in data\n",
    "country_train_lyrics = pd.read_csv(country_train_filepath, header=None)[0].to_list()\n",
    "country_val_lyrics = pd.read_csv(country_val_filepath, header=None)[0].to_list()\n",
    "print(\"Number of training lines for Country:\", len(country_train_lyrics))\n",
    "print(\"Number of validation lines for Country:\", len(country_val_lyrics))\n",
    "print()\n",
    "\n",
    "metal_train_lyrics = pd.read_csv(metal_train_filepath, header=None)[0].to_list()\n",
    "metal_val_lyrics = pd.read_csv(meta_val_filepath, header=None)[0].to_list()\n",
    "print(\"Number of training lines for Heavy Metal:\", len(metal_train_lyrics))\n",
    "print(\"Number of validation lines for Heavy Metal:\", len(metal_val_lyrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to Format Generated Sequences to be more Readable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to help format generated sentences \n",
    "def clean_lyric(lyric_tokens: list) -> str:\n",
    "    \"\"\"\n",
    "    Return the given sequence of tokens as a single string without special tokens like <s> or </s>\n",
    "\n",
    "    Args:\n",
    "        lyric_tokens (list): list of tokens for the generated sequence\n",
    "\n",
    "    Returns:\n",
    "        The tokens joined in a single string without special characters \n",
    "\n",
    "    \"\"\"\n",
    "    lyric_str = ' '.join(lyric_tokens)\n",
    "    lyric_str = lyric_str.replace(NEWLINE, '\\n')\n",
    "    return lyric_str.replace(SENTENCE_BEGIN, '').replace(SENTENCE_END, '').strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using NLTK Model with Kneser-Ney Smoothing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ngram_kneser_ney_model(sequences: list, ngram: int = NGRAM, verbose: bool = True):\n",
    "\t\"\"\"\n",
    "\t Creates a trained n-gram language model using Kneser-Ney Smoothing. Model will be trained on songs in the given \n",
    "\t music genre. \n",
    "\n",
    "\t Args:\n",
    "\t\tsequences (list): a list of training sequence strings, not tokenized \n",
    "\t\tngram (int): the n-gram order of the language model to create\n",
    "\t\tverbose (bool): if True, prints information about the training data \n",
    "\t\t\t\t\n",
    "\tReturns:\n",
    "\t\tA trained KneserNeyInterpolated\n",
    "\t\"\"\"\n",
    "\t# split each line into tokens \n",
    "\ttokens = [utils.tokenize_line(seq, ngram) for seq in sequences]\n",
    "\n",
    "\t# allow padded_everygram_pipeline to create ngrams for the model \n",
    "\tngrams_generator, padded_sents = padded_everygram_pipeline(ngram, tokens)\n",
    "\n",
    "\tmodel = nltk.lm.KneserNeyInterpolated(ngram)\n",
    "\tmodel.fit(ngrams_generator, padded_sents)\n",
    "     \n",
    "\tif verbose:\n",
    "\t\tprint(\"Number of tokens:\", len(tokens))\n",
    "\t\tprint(\"Vocabulary Size:\", len(model.vocab))\n",
    "\t\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country KneserNey Model:\n",
      "Number of tokens: 149771\n",
      "Vocabulary Size: 18866\n",
      "\n",
      "Heavy Metal KneserNey Model:\n",
      "Number of tokens: 149771\n",
      "Vocabulary Size: 25274\n"
     ]
    }
   ],
   "source": [
    "print(\"Country KneserNey Model:\")\n",
    "kneser_ney_country_model = create_ngram_kneser_ney_model(country_train_lyrics)\n",
    "print()\n",
    "\n",
    "print(\"Heavy Metal KneserNey Model:\")\n",
    "kneser_ney_metal_model = create_ngram_kneser_ney_model(metal_train_lyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country Generated Lyrics:\n",
      "\n",
      "homeless , will buoy me on .\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\shaem\\NLP\\Project\\CS4120_Song_Lyric_Generation\\ngram.ipynb Cell 10\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/shaem/NLP/Project/CS4120_Song_Lyric_Generation/ngram.ipynb#X12sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mCountry Generated Lyrics:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/shaem/NLP/Project/CS4120_Song_Lyric_Generation/ngram.ipynb#X12sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(NUM_SEQ_TO_GENERATE):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/shaem/NLP/Project/CS4120_Song_Lyric_Generation/ngram.ipynb#X12sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     lyric_tokens \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(kneser_ney_country_model\u001b[39m.\u001b[39;49mgenerate(max_len, seed))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/shaem/NLP/Project/CS4120_Song_Lyric_Generation/ngram.ipynb#X12sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39mprint\u001b[39m(clean_lyric(lyric_tokens))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/shaem/NLP/Project/CS4120_Song_Lyric_Generation/ngram.ipynb#X12sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mprint\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\shaem\\anaconda3\\Lib\\site-packages\\nltk\\lm\\api.py:229\u001b[0m, in \u001b[0;36mLanguageModel.generate\u001b[1;34m(self, num_words, text_seed, random_seed)\u001b[0m\n\u001b[0;32m    226\u001b[0m generated \u001b[39m=\u001b[39m []\n\u001b[0;32m    227\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_words):\n\u001b[0;32m    228\u001b[0m     generated\u001b[39m.\u001b[39mappend(\n\u001b[1;32m--> 229\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate(\n\u001b[0;32m    230\u001b[0m             num_words\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[0;32m    231\u001b[0m             text_seed\u001b[39m=\u001b[39;49mtext_seed \u001b[39m+\u001b[39;49m generated,\n\u001b[0;32m    232\u001b[0m             random_seed\u001b[39m=\u001b[39;49mrandom_generator,\n\u001b[0;32m    233\u001b[0m         )\n\u001b[0;32m    234\u001b[0m     )\n\u001b[0;32m    235\u001b[0m \u001b[39mreturn\u001b[39;00m generated\n",
      "File \u001b[1;32mc:\\Users\\shaem\\anaconda3\\Lib\\site-packages\\nltk\\lm\\api.py:222\u001b[0m, in \u001b[0;36mLanguageModel.generate\u001b[1;34m(self, num_words, text_seed, random_seed)\u001b[0m\n\u001b[0;32m    216\u001b[0m     \u001b[39m# Sorting samples achieves two things:\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[39m# - reproducible randomness when sampling\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[39m# - turns Mapping into Sequence which `_weighted_choice` expects\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     samples \u001b[39m=\u001b[39m \u001b[39msorted\u001b[39m(samples)\n\u001b[0;32m    220\u001b[0m     \u001b[39mreturn\u001b[39;00m _weighted_choice(\n\u001b[0;32m    221\u001b[0m         samples,\n\u001b[1;32m--> 222\u001b[0m         \u001b[39mtuple\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscore(w, context) \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m samples),\n\u001b[0;32m    223\u001b[0m         random_generator,\n\u001b[0;32m    224\u001b[0m     )\n\u001b[0;32m    225\u001b[0m \u001b[39m# We build up text one word at a time using the preceding context.\u001b[39;00m\n\u001b[0;32m    226\u001b[0m generated \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\shaem\\anaconda3\\Lib\\site-packages\\nltk\\lm\\api.py:222\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    216\u001b[0m     \u001b[39m# Sorting samples achieves two things:\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[39m# - reproducible randomness when sampling\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[39m# - turns Mapping into Sequence which `_weighted_choice` expects\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     samples \u001b[39m=\u001b[39m \u001b[39msorted\u001b[39m(samples)\n\u001b[0;32m    220\u001b[0m     \u001b[39mreturn\u001b[39;00m _weighted_choice(\n\u001b[0;32m    221\u001b[0m         samples,\n\u001b[1;32m--> 222\u001b[0m         \u001b[39mtuple\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscore(w, context) \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m samples),\n\u001b[0;32m    223\u001b[0m         random_generator,\n\u001b[0;32m    224\u001b[0m     )\n\u001b[0;32m    225\u001b[0m \u001b[39m# We build up text one word at a time using the preceding context.\u001b[39;00m\n\u001b[0;32m    226\u001b[0m generated \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\shaem\\anaconda3\\Lib\\site-packages\\nltk\\lm\\api.py:124\u001b[0m, in \u001b[0;36mLanguageModel.score\u001b[1;34m(self, word, context)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mscore\u001b[39m(\u001b[39mself\u001b[39m, word, context\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    119\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Masks out of vocab (OOV) words and computes their model score.\u001b[39;00m\n\u001b[0;32m    120\u001b[0m \n\u001b[0;32m    121\u001b[0m \u001b[39m    For model-specific logic of calculating scores, see the `unmasked_score`\u001b[39;00m\n\u001b[0;32m    122\u001b[0m \u001b[39m    method.\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 124\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49munmasked_score(\n\u001b[0;32m    125\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvocab\u001b[39m.\u001b[39;49mlookup(word), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvocab\u001b[39m.\u001b[39;49mlookup(context) \u001b[39mif\u001b[39;49;00m context \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m\n\u001b[0;32m    126\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\shaem\\anaconda3\\Lib\\site-packages\\nltk\\lm\\models.py:112\u001b[0m, in \u001b[0;36mInterpolatedLanguageModel.unmasked_score\u001b[1;34m(self, word, context)\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    111\u001b[0m     alpha, gamma \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimator\u001b[39m.\u001b[39malpha_gamma(word, context)\n\u001b[1;32m--> 112\u001b[0m \u001b[39mreturn\u001b[39;00m alpha \u001b[39m+\u001b[39m gamma \u001b[39m*\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49munmasked_score(word, context[\u001b[39m1\u001b[39;49m:])\n",
      "File \u001b[1;32mc:\\Users\\shaem\\anaconda3\\Lib\\site-packages\\nltk\\lm\\models.py:112\u001b[0m, in \u001b[0;36mInterpolatedLanguageModel.unmasked_score\u001b[1;34m(self, word, context)\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    111\u001b[0m     alpha, gamma \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimator\u001b[39m.\u001b[39malpha_gamma(word, context)\n\u001b[1;32m--> 112\u001b[0m \u001b[39mreturn\u001b[39;00m alpha \u001b[39m+\u001b[39m gamma \u001b[39m*\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49munmasked_score(word, context[\u001b[39m1\u001b[39;49m:])\n",
      "File \u001b[1;32mc:\\Users\\shaem\\anaconda3\\Lib\\site-packages\\nltk\\lm\\models.py:104\u001b[0m, in \u001b[0;36mInterpolatedLanguageModel.unmasked_score\u001b[1;34m(self, word, context)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39munmasked_score\u001b[39m(\u001b[39mself\u001b[39m, word, context\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    102\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m context:\n\u001b[0;32m    103\u001b[0m         \u001b[39m# The base recursion case: no context, we only have a unigram.\u001b[39;00m\n\u001b[1;32m--> 104\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mestimator\u001b[39m.\u001b[39;49munigram_score(word)\n\u001b[0;32m    105\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcounts[context]:\n\u001b[0;32m    106\u001b[0m         \u001b[39m# It can also happen that we have no data for this context.\u001b[39;00m\n\u001b[0;32m    107\u001b[0m         \u001b[39m# In that case we defer to the lower-order ngram.\u001b[39;00m\n\u001b[0;32m    108\u001b[0m         \u001b[39m# This is the same as setting alpha to 0 and gamma to 1.\u001b[39;00m\n\u001b[0;32m    109\u001b[0m         alpha, gamma \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\shaem\\anaconda3\\Lib\\site-packages\\nltk\\lm\\smoothing.py:97\u001b[0m, in \u001b[0;36mKneserNey.unigram_score\u001b[1;34m(self, word)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39munigram_score\u001b[39m(\u001b[39mself\u001b[39m, word):\n\u001b[1;32m---> 97\u001b[0m     word_continuation_count, total_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_continuation_counts(word)\n\u001b[0;32m     98\u001b[0m     \u001b[39mreturn\u001b[39;00m word_continuation_count \u001b[39m/\u001b[39m total_count\n",
      "File \u001b[1;32mc:\\Users\\shaem\\anaconda3\\Lib\\site-packages\\nltk\\lm\\smoothing.py:126\u001b[0m, in \u001b[0;36mKneserNey._continuation_counts\u001b[1;34m(self, word, context)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[39mfor\u001b[39;00m counts \u001b[39min\u001b[39;00m higher_order_ngrams_with_context:\n\u001b[0;32m    125\u001b[0m     higher_order_ngrams_with_word_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(counts[word] \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m)\n\u001b[1;32m--> 126\u001b[0m     total \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m _count_values_gt_zero(counts)\n\u001b[0;32m    127\u001b[0m \u001b[39mreturn\u001b[39;00m higher_order_ngrams_with_word_count, total\n",
      "File \u001b[1;32mc:\\Users\\shaem\\anaconda3\\Lib\\site-packages\\nltk\\lm\\smoothing.py:31\u001b[0m, in \u001b[0;36m_count_values_gt_zero\u001b[1;34m(distribution)\u001b[0m\n\u001b[0;32m     25\u001b[0m as_count \u001b[39m=\u001b[39m (\n\u001b[0;32m     26\u001b[0m     methodcaller(\u001b[39m\"\u001b[39m\u001b[39mN\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     27\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(distribution, ConditionalFreqDist)\n\u001b[0;32m     28\u001b[0m     \u001b[39melse\u001b[39;00m \u001b[39mlambda\u001b[39;00m count: count\n\u001b[0;32m     29\u001b[0m )\n\u001b[0;32m     30\u001b[0m \u001b[39m# We explicitly check that values are > 0 to guard against negative counts.\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msum\u001b[39m(\n\u001b[0;32m     32\u001b[0m     \u001b[39m1\u001b[39m \u001b[39mfor\u001b[39;00m dist_or_count \u001b[39min\u001b[39;00m distribution\u001b[39m.\u001b[39mvalues() \u001b[39mif\u001b[39;00m as_count(dist_or_count) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     33\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\shaem\\anaconda3\\Lib\\site-packages\\nltk\\lm\\smoothing.py:32\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     25\u001b[0m as_count \u001b[39m=\u001b[39m (\n\u001b[0;32m     26\u001b[0m     methodcaller(\u001b[39m\"\u001b[39m\u001b[39mN\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     27\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(distribution, ConditionalFreqDist)\n\u001b[0;32m     28\u001b[0m     \u001b[39melse\u001b[39;00m \u001b[39mlambda\u001b[39;00m count: count\n\u001b[0;32m     29\u001b[0m )\n\u001b[0;32m     30\u001b[0m \u001b[39m# We explicitly check that values are > 0 to guard against negative counts.\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msum\u001b[39m(\n\u001b[1;32m---> 32\u001b[0m     \u001b[39m1\u001b[39m \u001b[39mfor\u001b[39;00m dist_or_count \u001b[39min\u001b[39;00m distribution\u001b[39m.\u001b[39mvalues() \u001b[39mif\u001b[39;00m as_count(dist_or_count) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     33\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "seed = [SENTENCE_BEGIN] * (NGRAM - 1)\n",
    "max_len = 20 # nltk's models generate sequences of a fixed length \n",
    "\n",
    "print(\"Country Generated Lyrics:\\n\")\n",
    "for i in range(NUM_SEQ_TO_GENERATE):\n",
    "    lyric_tokens = list(kneser_ney_country_model.generate(max_len, seed))\n",
    "    print(clean_lyric(lyric_tokens))\n",
    "\n",
    "print()\n",
    "print(\"Heavy Metal Generated Lyrics:\\n\")\n",
    "for i in range(NUM_SEQ_TO_GENERATE):\n",
    "    lyric_tokens = list(kneser_ney_metal_model.generate(max_len, seed))\n",
    "    print(clean_lyric(lyric_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\shaem\\NLP\\Project\\CS4120_Song_Lyric_Generation\\ngram.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/shaem/NLP/Project/CS4120_Song_Lyric_Generation/ngram.ipynb#X13sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m         perplexities\u001b[39m.\u001b[39mappend(model\u001b[39m.\u001b[39mperplexity(test_ngrams))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/shaem/NLP/Project/CS4120_Song_Lyric_Generation/ngram.ipynb#X13sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mmedian(perplexities)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/shaem/NLP/Project/CS4120_Song_Lyric_Generation/ngram.ipynb#X13sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mMedian Validation Perplexity for Country Model:\u001b[39m\u001b[39m\"\u001b[39m, kn_median_perplexity(kneser_ney_country_model, country_val_lyrics))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/shaem/NLP/Project/CS4120_Song_Lyric_Generation/ngram.ipynb#X13sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mMedian Validation Perplexity for Heavy Metal Model:\u001b[39m\u001b[39m\"\u001b[39m, kn_median_perplexity(kneser_ney_metal_model, metal_val_lyrics))\n",
      "\u001b[1;32mc:\\Users\\shaem\\NLP\\Project\\CS4120_Song_Lyric_Generation\\ngram.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/shaem/NLP/Project/CS4120_Song_Lyric_Generation/ngram.ipynb#X13sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     test_tokens \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39mtokenize_line(line, ngram)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/shaem/NLP/Project/CS4120_Song_Lyric_Generation/ngram.ipynb#X13sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     test_ngrams \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(nltk\u001b[39m.\u001b[39mngrams(test_tokens, n\u001b[39m=\u001b[39mngram))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/shaem/NLP/Project/CS4120_Song_Lyric_Generation/ngram.ipynb#X13sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     perplexities\u001b[39m.\u001b[39mappend(model\u001b[39m.\u001b[39;49mperplexity(test_ngrams))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/shaem/NLP/Project/CS4120_Song_Lyric_Generation/ngram.ipynb#X13sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mmedian(perplexities)\n",
      "File \u001b[1;32mc:\\Users\\shaem\\anaconda3\\Lib\\site-packages\\nltk\\lm\\api.py:180\u001b[0m, in \u001b[0;36mLanguageModel.perplexity\u001b[1;34m(self, text_ngrams)\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mperplexity\u001b[39m(\u001b[39mself\u001b[39m, text_ngrams):\n\u001b[0;32m    175\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Calculates the perplexity of the given text.\u001b[39;00m\n\u001b[0;32m    176\u001b[0m \n\u001b[0;32m    177\u001b[0m \u001b[39m    This is simply 2 ** cross-entropy for the text, so the arguments are the same.\u001b[39;00m\n\u001b[0;32m    178\u001b[0m \n\u001b[0;32m    179\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 180\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mpow\u001b[39m(\u001b[39m2.0\u001b[39m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mentropy(text_ngrams))\n",
      "File \u001b[1;32mc:\\Users\\shaem\\anaconda3\\Lib\\site-packages\\nltk\\lm\\api.py:171\u001b[0m, in \u001b[0;36mLanguageModel.entropy\u001b[1;34m(self, text_ngrams)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mentropy\u001b[39m(\u001b[39mself\u001b[39m, text_ngrams):\n\u001b[0;32m    164\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Calculate cross-entropy of model for given evaluation text.\u001b[39;00m\n\u001b[0;32m    165\u001b[0m \n\u001b[0;32m    166\u001b[0m \u001b[39m    :param Iterable(tuple(str)) text_ngrams: A sequence of ngram tuples.\u001b[39;00m\n\u001b[0;32m    167\u001b[0m \u001b[39m    :rtype: float\u001b[39;00m\n\u001b[0;32m    168\u001b[0m \n\u001b[0;32m    169\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m    170\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m \u001b[39m*\u001b[39m _mean(\n\u001b[1;32m--> 171\u001b[0m         [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlogscore(ngram[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m], ngram[:\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m]) \u001b[39mfor\u001b[39;49;00m ngram \u001b[39min\u001b[39;49;00m text_ngrams]\n\u001b[0;32m    172\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\shaem\\anaconda3\\Lib\\site-packages\\nltk\\lm\\api.py:171\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mentropy\u001b[39m(\u001b[39mself\u001b[39m, text_ngrams):\n\u001b[0;32m    164\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Calculate cross-entropy of model for given evaluation text.\u001b[39;00m\n\u001b[0;32m    165\u001b[0m \n\u001b[0;32m    166\u001b[0m \u001b[39m    :param Iterable(tuple(str)) text_ngrams: A sequence of ngram tuples.\u001b[39;00m\n\u001b[0;32m    167\u001b[0m \u001b[39m    :rtype: float\u001b[39;00m\n\u001b[0;32m    168\u001b[0m \n\u001b[0;32m    169\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m    170\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m \u001b[39m*\u001b[39m _mean(\n\u001b[1;32m--> 171\u001b[0m         [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlogscore(ngram[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m], ngram[:\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m]) \u001b[39mfor\u001b[39;00m ngram \u001b[39min\u001b[39;00m text_ngrams]\n\u001b[0;32m    172\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\shaem\\anaconda3\\Lib\\site-packages\\nltk\\lm\\api.py:150\u001b[0m, in \u001b[0;36mLanguageModel.logscore\u001b[1;34m(self, word, context)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlogscore\u001b[39m(\u001b[39mself\u001b[39m, word, context\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    145\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Evaluate the log score of this word in this context.\u001b[39;00m\n\u001b[0;32m    146\u001b[0m \n\u001b[0;32m    147\u001b[0m \u001b[39m    The arguments are the same as for `score` and `unmasked_score`.\u001b[39;00m\n\u001b[0;32m    148\u001b[0m \n\u001b[0;32m    149\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 150\u001b[0m     \u001b[39mreturn\u001b[39;00m log_base2(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscore(word, context))\n",
      "File \u001b[1;32mc:\\Users\\shaem\\anaconda3\\Lib\\site-packages\\nltk\\lm\\api.py:124\u001b[0m, in \u001b[0;36mLanguageModel.score\u001b[1;34m(self, word, context)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mscore\u001b[39m(\u001b[39mself\u001b[39m, word, context\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    119\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Masks out of vocab (OOV) words and computes their model score.\u001b[39;00m\n\u001b[0;32m    120\u001b[0m \n\u001b[0;32m    121\u001b[0m \u001b[39m    For model-specific logic of calculating scores, see the `unmasked_score`\u001b[39;00m\n\u001b[0;32m    122\u001b[0m \u001b[39m    method.\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 124\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49munmasked_score(\n\u001b[0;32m    125\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvocab\u001b[39m.\u001b[39;49mlookup(word), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvocab\u001b[39m.\u001b[39;49mlookup(context) \u001b[39mif\u001b[39;49;00m context \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m\n\u001b[0;32m    126\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\shaem\\anaconda3\\Lib\\site-packages\\nltk\\lm\\models.py:112\u001b[0m, in \u001b[0;36mInterpolatedLanguageModel.unmasked_score\u001b[1;34m(self, word, context)\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    111\u001b[0m     alpha, gamma \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimator\u001b[39m.\u001b[39malpha_gamma(word, context)\n\u001b[1;32m--> 112\u001b[0m \u001b[39mreturn\u001b[39;00m alpha \u001b[39m+\u001b[39m gamma \u001b[39m*\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49munmasked_score(word, context[\u001b[39m1\u001b[39;49m:])\n",
      "File \u001b[1;32mc:\\Users\\shaem\\anaconda3\\Lib\\site-packages\\nltk\\lm\\models.py:111\u001b[0m, in \u001b[0;36mInterpolatedLanguageModel.unmasked_score\u001b[1;34m(self, word, context)\u001b[0m\n\u001b[0;32m    109\u001b[0m     alpha, gamma \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m\n\u001b[0;32m    110\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 111\u001b[0m     alpha, gamma \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mestimator\u001b[39m.\u001b[39;49malpha_gamma(word, context)\n\u001b[0;32m    112\u001b[0m \u001b[39mreturn\u001b[39;00m alpha \u001b[39m+\u001b[39m gamma \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munmasked_score(word, context[\u001b[39m1\u001b[39m:])\n",
      "File \u001b[1;32mc:\\Users\\shaem\\anaconda3\\Lib\\site-packages\\nltk\\lm\\smoothing.py:105\u001b[0m, in \u001b[0;36mKneserNey.alpha_gamma\u001b[1;34m(self, word, context)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39malpha_gamma\u001b[39m(\u001b[39mself\u001b[39m, word, context):\n\u001b[0;32m    101\u001b[0m     prefix_counts \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcounts[context]\n\u001b[0;32m    102\u001b[0m     word_continuation_count, total_count \u001b[39m=\u001b[39m (\n\u001b[0;32m    103\u001b[0m         (prefix_counts[word], prefix_counts\u001b[39m.\u001b[39mN())\n\u001b[0;32m    104\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(context) \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_order\n\u001b[1;32m--> 105\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_continuation_counts(word, context)\n\u001b[0;32m    106\u001b[0m     )\n\u001b[0;32m    107\u001b[0m     alpha \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(word_continuation_count \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdiscount, \u001b[39m0.0\u001b[39m) \u001b[39m/\u001b[39m total_count\n\u001b[0;32m    108\u001b[0m     gamma \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdiscount \u001b[39m*\u001b[39m _count_values_gt_zero(prefix_counts) \u001b[39m/\u001b[39m total_count\n",
      "File \u001b[1;32mc:\\Users\\shaem\\anaconda3\\Lib\\site-packages\\nltk\\lm\\smoothing.py:124\u001b[0m, in \u001b[0;36mKneserNey._continuation_counts\u001b[1;34m(self, word, context)\u001b[0m\n\u001b[0;32m    118\u001b[0m higher_order_ngrams_with_context \u001b[39m=\u001b[39m (\n\u001b[0;32m    119\u001b[0m     counts\n\u001b[0;32m    120\u001b[0m     \u001b[39mfor\u001b[39;00m prefix_ngram, counts \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcounts[\u001b[39mlen\u001b[39m(context) \u001b[39m+\u001b[39m \u001b[39m2\u001b[39m]\u001b[39m.\u001b[39mitems()\n\u001b[0;32m    121\u001b[0m     \u001b[39mif\u001b[39;00m prefix_ngram[\u001b[39m1\u001b[39m:] \u001b[39m==\u001b[39m context\n\u001b[0;32m    122\u001b[0m )\n\u001b[0;32m    123\u001b[0m higher_order_ngrams_with_word_count, total \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m\n\u001b[1;32m--> 124\u001b[0m \u001b[39mfor\u001b[39;00m counts \u001b[39min\u001b[39;00m higher_order_ngrams_with_context:\n\u001b[0;32m    125\u001b[0m     higher_order_ngrams_with_word_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(counts[word] \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m)\n\u001b[0;32m    126\u001b[0m     total \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m _count_values_gt_zero(counts)\n",
      "File \u001b[1;32mc:\\Users\\shaem\\anaconda3\\Lib\\site-packages\\nltk\\lm\\smoothing.py:121\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_continuation_counts\u001b[39m(\u001b[39mself\u001b[39m, word, context\u001b[39m=\u001b[39m\u001b[39mtuple\u001b[39m()):\n\u001b[0;32m    112\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Count continuations that end with context and word.\u001b[39;00m\n\u001b[0;32m    113\u001b[0m \n\u001b[0;32m    114\u001b[0m \u001b[39m    Continuations track unique ngram \"types\", regardless of how many\u001b[39;00m\n\u001b[0;32m    115\u001b[0m \u001b[39m    instances were observed for each \"type\".\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[39m    This is different than raw ngram counts which track number of instances.\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m    118\u001b[0m     higher_order_ngrams_with_context \u001b[39m=\u001b[39m (\n\u001b[0;32m    119\u001b[0m         counts\n\u001b[0;32m    120\u001b[0m         \u001b[39mfor\u001b[39;00m prefix_ngram, counts \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcounts[\u001b[39mlen\u001b[39m(context) \u001b[39m+\u001b[39m \u001b[39m2\u001b[39m]\u001b[39m.\u001b[39mitems()\n\u001b[1;32m--> 121\u001b[0m         \u001b[39mif\u001b[39;00m prefix_ngram[\u001b[39m1\u001b[39m:] \u001b[39m==\u001b[39m context\n\u001b[0;32m    122\u001b[0m     )\n\u001b[0;32m    123\u001b[0m     higher_order_ngrams_with_word_count, total \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m\n\u001b[0;32m    124\u001b[0m     \u001b[39mfor\u001b[39;00m counts \u001b[39min\u001b[39;00m higher_order_ngrams_with_context:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def kn_median_perplexity(model, lines: list, ngram: int=NGRAM): \n",
    "    \"\"\"\n",
    "    Evaluates the given model by finding the median perplexity of the given test sequences. \n",
    "    \"\"\"\n",
    "    perplexities = []\n",
    "\n",
    "    for line in lines:\n",
    "        test_tokens = utils.tokenize_line(line, ngram)\n",
    "        test_ngrams = list(nltk.ngrams(test_tokens, n=ngram))\n",
    "        perplexities.append(model.perplexity(test_ngrams))\n",
    "\n",
    "    return np.median(perplexities)\n",
    "\n",
    "print(\"Median Validation Perplexity for Country Model:\", kn_median_perplexity(kneser_ney_country_model, country_val_lyrics))\n",
    "print(\"Median Validation Perplexity for Heavy Metal Model:\", kn_median_perplexity(kneser_ney_metal_model, metal_val_lyrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Own N-Gram Language Model with Laplace Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ngram_laplace_model(sequences: list, ngram: int = NGRAM, verbose: bool = True):\n",
    "\t\"\"\"\n",
    "\t Creates a trained n-gram language model using Laplace Smoothing. Model will be trained on songs in the given \n",
    "\t music genre. \n",
    "\n",
    "\t Args:\n",
    "\t\tsequences (list): a list of training sequence strings, not tokenized\n",
    "\t\tngram (int): the n-gram order of the language model to create\n",
    "\t\tverbose (bool): if True, prints information about the training data \n",
    "\n",
    "\tReturns:\n",
    "\t\tA trained NGramLaplaceLanguageModel\n",
    "\t\"\"\"\n",
    "\ttokens = utils.tokenize(sequences, ngram)\n",
    "\tmodel = lm.NGramLaplaceLanguageModel(ngram)\n",
    "\tmodel.train(tokens, verbose=verbose)\n",
    "\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country Laplace Model:\n",
      "Number of tokens: 2399485\n",
      "N-gram examples: [('<s>', '<s>', '<s>', '<s>', 'i'), ('<s>', '<s>', '<s>', 'i', \"'ve\"), ('<s>', '<s>', 'i', \"'ve\", 'seen'), ('<s>', 'i', \"'ve\", 'seen', 'how'), ('i', \"'ve\", 'seen', 'how', 'you')]\n",
      "Vocabulary Size: 11236\n"
     ]
    }
   ],
   "source": [
    "print(\"Country Laplace Model:\")\n",
    "laplace_country_model = create_ngram_laplace_model(country_train_lyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heavy Metal Laplace Model:\n",
      "Number of tokens: 2146433\n",
      "N-gram examples: [('<s>', '<s>', '<s>', '<s>', 'my'), ('<s>', '<s>', '<s>', 'my', 'journey'), ('<s>', '<s>', 'my', 'journey', 'began'), ('<s>', 'my', 'journey', 'began', 'after'), ('my', 'journey', 'began', 'after', 'the')]\n",
      "Vocabulary Size: 14350\n"
     ]
    }
   ],
   "source": [
    "print(\"Heavy Metal Laplace Model:\")\n",
    "laplace_metal_model = create_ngram_laplace_model(metal_train_lyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country Generated Lyrics:\n",
      "\n",
      "lord above me knows i love you\n",
      "a beautiful sight , weæš®e happy tonight\n",
      "i get along with you\n",
      "however you look at it , whatever you believe\n",
      "she can crawl it\n",
      "'cause it 's beer thirty , and it 's time to go out on a huntin ' spree\n",
      "since she up and walked away\n",
      "when he holds me i can feel he 's hard giving love 's true and real\n",
      "the sleeping child , you 're holding\n",
      "( <UNK> solo )\n"
     ]
    }
   ],
   "source": [
    "print(\"Country Generated Lyrics:\\n\")\n",
    "for i in range(NUM_SEQ_TO_GENERATE):\n",
    "    lyric_tokens = laplace_country_model.generate_sentence()\n",
    "    print(clean_lyric(lyric_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heavy Metal Generated Lyrics:\n",
      "\n",
      "and now i close the door\n",
      "you will forger the pain\n",
      "ca n't somebody tell me am i the top of the chain\n",
      "just be my human hand\n",
      "death from above\n",
      "before their <UNK>\n",
      "if you just forget\n",
      "you called me up the other day just when i thought you would\n",
      "ticket at the other , ‘ cos i 'm no <UNK> in distress\n",
      "with the mentor ? s anger\n"
     ]
    }
   ],
   "source": [
    "print(\"Heavy Metal Generated Lyrics:\\n\")\n",
    "for i in range(NUM_SEQ_TO_GENERATE):\n",
    "    lyric_tokens = laplace_metal_model.generate_sentence()\n",
    "    print(clean_lyric(lyric_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median Validation Perplexity for Country Model: 2531.502204342474\n",
      "Median Validation Perplexity for Heavy Metal Model: 3581.757352722282\n",
      "Median Training Perplexity for Country Model: 1346.5277568873\n",
      "Median Training Perplexity for Heavy Metal Model: 1904.5755312503975\n"
     ]
    }
   ],
   "source": [
    "# EVALUATE PERPLEXITY \n",
    "def laplace_median_perplexity(model, lines: list, ngram: int=NGRAM): \n",
    "    \"\"\"\n",
    "    Evaluates the given model by finding the median perplexity of the given test sequences. \n",
    "    \"\"\"\n",
    "    perplexities = []\n",
    "\n",
    "    for line in lines:\n",
    "        test_tokens = utils.tokenize_line(line, ngram)\n",
    "        perplexities.append(model.perplexity(test_tokens))\n",
    "\n",
    "    return np.median(perplexities)\n",
    "\n",
    "\n",
    "# seeing perplexity on training data as reference to compare against validation perplexity\n",
    "print(\"Median Training Perplexity for Country Model:\", laplace_median_perplexity(laplace_country_model, country_train_lyrics))\n",
    "print(\"Median Training Perplexity for Heavy Metal Model:\", laplace_median_perplexity(laplace_metal_model, metal_train_lyrics))\n",
    "\n",
    "print()\n",
    "\n",
    "# perplexity on data that the models have not seen yet \n",
    "print(\"Median Validation Perplexity for Country Model:\", laplace_median_perplexity(laplace_country_model, country_val_lyrics))\n",
    "print(\"Median Validation Perplexity for Heavy Metal Model:\", laplace_median_perplexity(laplace_metal_model, metal_val_lyrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimentation - Testing out Different NGRAM values\n",
    "\n",
    "Train models on train dataset, report perplexity on validation dataset, final model will be evaluated on test dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Country Laplace Model \n",
    "\n",
    "__ngram=1__\n",
    "1. perplexity on training set: 469.48482936007724\n",
    "1. perplexity on validation set: 458.24079917067286\n",
    "2. time to train: 15.7s\n",
    "3. time to generate 10 lines: 1m, 18.9s\n",
    "4. example lyrics:\\\n",
    "and by we wo this in undress that last of\\\n",
    "i loveable to of , we missin in sweet barney hello\\\n",
    "wake 'm from she honky those get babies '' that , you\\\n",
    "stranger to , just who ooh\\\n",
    "not all in worth house who i ] something out many it countryside i records just and [ i 's\n",
    "\n",
    "__ngram=2__\n",
    "1. perplexity on training set: 287.90516137012776\n",
    "1. perplexity on validation set: 337.8503322927694\n",
    "2. time to train: 16.5s\n",
    "3. time to generate 10 lines: 23.3s\n",
    "4. example lyrics:\\\n",
    "love , oh wipe each other one without wishin that 's the by\\\n",
    "except what to realize\\\n",
    "put you take you\\\n",
    "he loved her mother\\\n",
    "no chance\\\n",
    "on and who i was there\n",
    "\n",
    "\n",
    "__ngram=3__\n",
    "1. perplexity on training set: 880.498661231573\n",
    "1. perplexity on validation set: 1205.002805205404\n",
    "2. time to train: 16.2s\n",
    "3. time to generate 10 lines: 23.0s\n",
    "4. example lyrics:\\\n",
    "i said i will\\\n",
    "and little jeanie 's sake .\\\n",
    "technicolor , river queen , three on high\\\n",
    "when a road with my fiddle\\\n",
    "yes everything i have shown\n",
    "\n",
    "__ngram=4__\n",
    "1. perplexity on training set:  1139.1474901855402\n",
    "1. perplexity on validation set: 2046.0507074046266\n",
    "2. time to train: 17.7s\n",
    "3. time to generate 10 lines: 35.3s\n",
    "4. example lyrics:\\\n",
    "if heaven 's real\\\n",
    "i do n't know\\\n",
    "one night at a time\\\n",
    "well you nursed me through the valley filled with snow\\\n",
    "we always wear a great big world are we\\\n",
    "on a cloud nine ride\n",
    "\n",
    "__ngram=5__\n",
    "1. perplexity on training set: 1346.5277568873\n",
    "1. perplexity on validation set: 2531.502204342474\n",
    "2. time to train: 16.2s\n",
    "3. time to generate 10 lines: 39.2s\n",
    "4. example lyrics:\\\n",
    "lord above me knows i love you\\\n",
    "a beautiful sight , weæš®e happy tonight\\\n",
    "i get along with you\\\n",
    "however you look at it , whatever you believe\\\n",
    "she can crawl it\\\n",
    "'cause it 's beer thirty , and it 's time to go out on a huntin ' spree\\\n",
    "\n",
    "\n",
    "##### Heavy Metal Laplace Model\n",
    "\n",
    "__ngram=1__\n",
    "1. perplexity on training set: 550.9829570794548\n",
    "1. perplexity on validation set: 516.3249810238908\n",
    "2. time to train: 14.4s\n",
    "3. time to generate 10 lines: 2m, 2.4s\n",
    "4. example lyrics:\\\n",
    "are me be , melt times , black 've the\\\n",
    ", cause these rain disappear blue look diseased so for soul sinister fear do approach my dance\\\n",
    "inside everything all run i into\\\n",
    "loneliness gon the i turns and mind , i the love time it hands\\\n",
    "overmastered own love 's 're it\n",
    "\n",
    "__ngram=2__\n",
    "1. perplexity on training set: 512.2497432426746\n",
    "1. perplexity on validation set: 606.7330883081569\n",
    "2. time to train: 15.0s\n",
    "3. time to generate 10 lines: 49.5s\n",
    "4. example lyrics:\\\n",
    "out all you\\\n",
    "but i hide , hey , dokken ,\\\n",
    "lay you pain\\\n",
    "colder than you make me in fire\\\n",
    "learning life go\n",
    "\n",
    "\n",
    "__ngram=3__\n",
    "1. perplexity on training set: 1057.3627882221006\n",
    "1. perplexity on validation set: 1944.8382552263183\n",
    "2. time to train: 14.0s\n",
    "3. time to generate 10 lines: 45.3s\n",
    "4. example lyrics:\\\n",
    "oppressions wall they will learn\\\n",
    "mad magicians tinsel nightmares\\\n",
    "a thousand young\\\n",
    "wherever you are too much abuse of wasted human ... debris\\\n",
    "turning bottled water into wine\n",
    "\n",
    "__ngram=4__\n",
    "1. perplexity on training set: 1649.1497465256639\n",
    "1. perplexity on validation set: 2972.0462423487797\n",
    "2. time to train: 16.4s\n",
    "3. time to generate 10 lines: 53.5s\n",
    "4. example lyrics:\\\n",
    "but now we retaliate\\\n",
    "devoid the fake with full disdain\\\n",
    "boiling in rage - sophisticated cage\\\n",
    "well , i know\\\n",
    "you feel it\n",
    "\n",
    "__ngram=5__\n",
    "1. perplexity on training set: 1904.5755312503975\n",
    "1. perplexity on validation set: 3581.757352722282\n",
    "2. time to train: 15.4s\n",
    "3. time to generate 10 lines: 1m, 0.3s\n",
    "4. example lyrics:\\\n",
    "and now i close the door\\\n",
    "you will forger the pain\\\n",
    "ca n't somebody tell me am i the top of the chain\\\n",
    "just be my human hand\\\n",
    "death from above"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
