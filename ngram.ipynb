{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-Gram Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports \n",
    "import pandas as pd\n",
    "import ngram_laplace_lm_model as lm\n",
    "import numpy as np\n",
    "\n",
    "import utils "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants \n",
    "NGRAM = 3\n",
    "NUM_SEQ_TO_GENERATE = 10 # how many lines to generate with our models \n",
    "VERBOSE = True\n",
    "\n",
    "# special tokens \n",
    "SENTENCE_BEGIN = \"<s>\"\n",
    "SENTENCE_END = \"</s>\"\n",
    "\n",
    "# filepaths \n",
    "country_train_filepath = \"country_train.csv\"\n",
    "country_val_filepath = \"country_val.csv\"\n",
    "\n",
    "metal_train_filepath = \"metal_train.csv\"\n",
    "metal_val_filepath = \"metal_val.csv\"\n",
    "\n",
    "# savepaths "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training lines for Country: 149771\n",
      "Number of validation lines for Country: 18610\n",
      "\n",
      "Number of training lines for Heavy Metal: 149771\n",
      "Number of validation lines for Heavy Metal: 18610\n"
     ]
    }
   ],
   "source": [
    "# read in data\n",
    "country_train_lyrics = pd.read_csv(country_train_filepath, header=None)[0].to_list()\n",
    "country_val_lyrics = pd.read_csv(country_val_filepath, header=None)[0].to_list()\n",
    "print(\"Number of training lines for Country:\", len(country_train_lyrics))\n",
    "print(\"Number of validation lines for Country:\", len(country_val_lyrics))\n",
    "print()\n",
    "\n",
    "metal_train_lyrics = pd.read_csv(metal_train_filepath, header=None)[0].to_list()\n",
    "metal_val_lyrics = pd.read_csv(metal_val_filepath, header=None)[0].to_list()\n",
    "print(\"Number of training lines for Heavy Metal:\", len(metal_train_lyrics))\n",
    "print(\"Number of validation lines for Heavy Metal:\", len(metal_val_lyrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ngram_laplace_model(training_sequences: list, ngram: int = NGRAM, verbose: bool = True):\n",
    "\t\"\"\"\n",
    "\t Creates a trained n-gram language model using Laplace Smoothing using the given training data \n",
    "\n",
    "\t Args:\n",
    "\t\ttraining_sequences (list): a list of training sequence strings\n",
    "\t\tngram (int): the n-gram order of the language model to create\n",
    "\t\tverbose (bool): if True, prints information about the training data \n",
    "\n",
    "\tReturns:\n",
    "\t\tA trained NGramLaplaceLanguageModel\n",
    "\t\"\"\"\n",
    "\ttokens = utils.tokenize(training_sequences, ngram)\n",
    "\tmodel = lm.NGramLaplaceLanguageModel(ngram)\n",
    "\tmodel.train(tokens, verbose=verbose)\n",
    "\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country Laplace Model:\n",
      "Number of tokens: 1800401\n",
      "N-gram examples: [('<s>', '<s>', 'i'), ('<s>', 'i', \"'ve\"), ('i', \"'ve\", 'seen'), (\"'ve\", 'seen', 'how'), ('seen', 'how', 'you')]\n",
      "Vocabulary Size: 11236\n"
     ]
    }
   ],
   "source": [
    "print(\"Country Laplace Model:\")\n",
    "laplace_country_model = create_ngram_laplace_model(country_train_lyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heavy Metal Laplace Model:\n",
      "Number of tokens: 1547349\n",
      "N-gram examples: [('<s>', '<s>', 'my'), ('<s>', 'my', 'journey'), ('my', 'journey', 'began'), ('journey', 'began', 'after'), ('began', 'after', 'the')]\n",
      "Vocabulary Size: 14350\n"
     ]
    }
   ],
   "source": [
    "print(\"Heavy Metal Laplace Model:\")\n",
    "laplace_metal_model = create_ngram_laplace_model(metal_train_lyrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate New Sequences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to help format generated sentences \n",
    "def clean_lyric(lyric_tokens: list) -> str:\n",
    "    \"\"\"\n",
    "    Return the given sequence of tokens as a single string without special tokens \n",
    "    Args:\n",
    "        lyric_tokens (list): list of tokens for the generated sequence\n",
    "\n",
    "    Returns:\n",
    "        The tokens joined in a single string without special characters \n",
    "\n",
    "    \"\"\"\n",
    "    lyric_str = ' '.join(lyric_tokens)\n",
    "    return lyric_str.replace(SENTENCE_BEGIN, '').replace(SENTENCE_END, '').strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country Generated Lyrics:\n",
      "\n",
      "oh no\n",
      "i wo n't get to where i belong\n",
      "if jesus walked the world\n",
      "<UNK> never thought id be obliged to any one woman\n",
      "come on , got a single reindeer and his rope 's pulled way to show your affection\n",
      "the way it made\n",
      "on my ear\n",
      "why do n't feel like this town\n",
      "stand still , can i stand in line\n",
      "no all around me and you 're out in the wind started to rain\n"
     ]
    }
   ],
   "source": [
    "print(\"Country Generated Lyrics:\\n\")\n",
    "for i in range(NUM_SEQ_TO_GENERATE):\n",
    "    lyric_tokens = laplace_country_model.generate_sentence()\n",
    "    print(clean_lyric(lyric_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heavy Metal Generated Lyrics:\n",
      "\n",
      "carniwar\n",
      "i do\n",
      "purgatory !\n",
      "i 'm alive\n",
      "i 'm watching you\n",
      "take a look at the ground , crused immortal\n",
      "epidemic of addiction , my sweet revenge for the first floor of your life\n",
      "every minute a shattered silence\n",
      "that 's haunting me\n",
      "mysteries of time\n"
     ]
    }
   ],
   "source": [
    "print(\"Heavy Metal Generated Lyrics:\\n\")\n",
    "for i in range(NUM_SEQ_TO_GENERATE):\n",
    "    lyric_tokens = laplace_metal_model.generate_sentence()\n",
    "    print(clean_lyric(lyric_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median Validation Perplexity for Country Model: 1205.002805205404\n",
      "Median Validation Perplexity for Heavy Metal Model: 1944.8382552263183\n"
     ]
    }
   ],
   "source": [
    "def median_perplexity(model, lines: list, ngram: int=NGRAM, should_truncate: bool=False): \n",
    "    \"\"\"\n",
    "    Evaluates the given model by finding the median perplexity of the given test sequences. \n",
    "\n",
    "    Args:\n",
    "        model : the  N-gram Language Model \n",
    "        lines (list): a list of strings of data to evaluate perplexity on \n",
    "        ngram (int): the n-gram order used by the model\n",
    "        should_truncate (bool): an optional truncation parameter that shortens the sequences to the same length used by RNNs\n",
    "                                (to make perplexity more comparable between models)\n",
    "\n",
    "    Returns: median perplexity over the given sequences \n",
    "    \"\"\"\n",
    "    perplexities = []\n",
    "    for line in lines:\n",
    "        if should_truncate:\n",
    "            line = line[:10]\n",
    "\n",
    "        test_tokens = utils.tokenize_line(line, ngram)\n",
    "        perplexities.append(model.perplexity(test_tokens))\n",
    "\n",
    "    return np.median(perplexities)\n",
    "\n",
    "\n",
    "# perplexity on data that the models have not seen yet \n",
    "print(\"Median Validation Perplexity for Country Model:\", median_perplexity(laplace_country_model, country_val_lyrics))\n",
    "print(\"Median Validation Perplexity for Heavy Metal Model:\", median_perplexity(laplace_metal_model, metal_val_lyrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimentation - Testing out Different NGRAM values\n",
    "\n",
    "Training perplexity listed simply for comparison. Validation perplexity should be used when choosing an NGRAM value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Country Model \n",
    "\n",
    "__ngram=2__\n",
    "1. perplexity on training set: 287.90516137012776\n",
    "1. perplexity on validation set: 337.8503322927694\n",
    "4. example lyrics:\\\n",
    "love , oh wipe each other one without wishin that 's the by\\\n",
    "except what to realize\\\n",
    "put you take you\\\n",
    "he loved her mother\\\n",
    "no chance\\\n",
    "on and who i was there\n",
    "\n",
    "\n",
    "__ngram=3__\n",
    "1. perplexity on training set: 880.498661231573\n",
    "1. perplexity on validation set: 1205.002805205404\n",
    "4. example lyrics:\\\n",
    "i said i will\\\n",
    "and little jeanie 's sake .\\\n",
    "technicolor , river queen , three on high\\\n",
    "when a road with my fiddle\\\n",
    "yes everything i have shown\n",
    "\n",
    "__ngram=4__\n",
    "1. perplexity on training set:  1139.1474901855402\n",
    "1. perplexity on validation set: 2046.0507074046266\n",
    "4. example lyrics:\\\n",
    "if heaven 's real\\\n",
    "i do n't know\\\n",
    "one night at a time\\\n",
    "well you nursed me through the valley filled with snow\\\n",
    "we always wear a great big world are we\\\n",
    "on a cloud nine ride\n",
    "\n",
    "__ngram=5__\n",
    "1. perplexity on training set: 1346.5277568873\n",
    "1. perplexity on validation set: 2531.502204342474\n",
    "4. example lyrics:\\\n",
    "lord above me knows i love you\\\n",
    "a beautiful sight , weæš®e happy tonight\\\n",
    "i get along with you\\\n",
    "however you look at it , whatever you believe\\\n",
    "she can crawl it\\\n",
    "'cause it 's beer thirty , and it 's time to go out on a huntin ' spree\\\n",
    "\n",
    "\n",
    "##### Heavy Metal Model\n",
    "\n",
    "__ngram=2__\n",
    "1. perplexity on training set: 512.2497432426746\n",
    "1. perplexity on validation set: 606.7330883081569\n",
    "4. example lyrics:\n",
    "out all you\\\n",
    "but i hide , hey , dokken ,\\\n",
    "lay you pain\\\n",
    "colder than you make me in fire\\\n",
    "learning life go\n",
    "\n",
    "\n",
    "__ngram=3__\n",
    "1. perplexity on training set: 1057.3627882221006\n",
    "1. perplexity on validation set: 1944.8382552263183\n",
    "4. example lyrics:\\\n",
    "oppressions wall they will learn\\\n",
    "mad magicians tinsel nightmares\\\n",
    "a thousand young\\\n",
    "wherever you are too much abuse of wasted human ... debris\\\n",
    "turning bottled water into wine\n",
    "\n",
    "__ngram=4__\n",
    "1. perplexity on training set: 1649.1497465256639\n",
    "1. perplexity on validation set: 2972.0462423487797\n",
    "4. example lyrics:\\\n",
    "but now we retaliate\\\n",
    "devoid the fake with full disdain\\\n",
    "boiling in rage - sophisticated cage\\\n",
    "well , i know\\\n",
    "you feel it\n",
    "\n",
    "__ngram=5__\n",
    "1. perplexity on training set: 1904.5755312503975\n",
    "1. perplexity on validation set: 3581.757352722282\n",
    "4. example lyrics:\\\n",
    "and now i close the door\\\n",
    "you will forger the pain\\\n",
    "ca n't somebody tell me am i the top of the chain\\\n",
    "just be my human hand\\\n",
    "death from above"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
