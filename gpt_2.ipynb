{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import transformers\n",
    "import torch \n",
    "import math\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "GENRE = 'Rap'\n",
    "# use all songs\n",
    "N_SONGS = 1000000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in  data\n",
    "df = pd.read_csv('clean_data.csv')\n",
    "# filter data by genre\n",
    "genre_df = df[df.genres.apply(lambda x: GENRE in x)]\n",
    "\n",
    "# save lyrics\n",
    "lyrics = genre_df.lyrics.values\n",
    "# split lyrics by line\n",
    "lines = [song.split('\\n') for song in lyrics[0:N_SONGS]]\n",
    "lines = list(chain.from_iterable(lines))\n",
    "# remove empty lines\n",
    "lines = [line for line in lines if len(line)>0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name model\n",
    "MODEL_NAME = 'gpt2'\n",
    "pipe = transformers.pipeline(task='text-generation', model=MODEL_NAME, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set model \n",
    "model = transformers.GPT2LMHeadModel.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# set model configurations\n",
    "config = transformers.GPT2Config.from_pretrained(MODEL_NAME)\n",
    "config.do_sample = True\n",
    "config.max_length = 100\n",
    "model = transformers.GPT2LMHeadModel.from_pretrained(MODEL_NAME,\n",
    "                                                      config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set tokenizer\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode training data\n",
    "enc_tokens = [tokenizer(line, return_tensors='pt') for line in lines]\n",
    "enc_tokens = [enc['input_ids'].tolist()[0] for enc in enc_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDset(torch.utils.data.Dataset):\n",
    "     \"\"\"A custom dataset\"\"\"\n",
    "     def __init__(self, data: list[list[int]]):\n",
    "         self.data = []\n",
    "         for d in data:\n",
    "             input_ids = torch.tensor(d, dtype=torch.int64)\n",
    "             attention_mask = torch.ones(len(d), dtype=torch.int64)\n",
    "             self.data.append({'input_ids': input_ids,\n",
    "                  'attention_mask': attention_mask, 'labels': input_ids})\n",
    " \n",
    "     def __len__(self):\n",
    "         return len(self.data)\n",
    " \n",
    "     def __getitem__(self, idx: int):\n",
    "         return self.data[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = transformers.TrainingArguments(\n",
    "     output_dir=\"idiot_save/\",\n",
    "     learning_rate=1e-3,\n",
    "     per_device_train_batch_size=1,\n",
    "     per_device_eval_batch_size=1,\n",
    "     num_train_epochs=1,\n",
    "     evaluation_strategy='epoch',\n",
    "     save_strategy='no',\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training, valdation, and testing data intervals\n",
    "END1 = math.ceil(len(enc_tokens)*0.8)\n",
    "END2 = END1 + math.ceil(len(enc_tokens)*0.1)\n",
    "\n",
    "# create training, valdation, and testing data\n",
    "dset_train = MyDset(enc_tokens[0:END1])\n",
    "dset_val = MyDset(enc_tokens[END1:END2])\n",
    "dset_test = MyDset(enc_tokens[END2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      "100%|██████████| 38/38 [01:16<00:00,  2.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 5.124989986419678, 'eval_runtime': 1.054, 'eval_samples_per_second': 8.539, 'eval_steps_per_second': 8.539, 'epoch': 1.0}\n",
      "{'train_runtime': 76.2414, 'train_samples_per_second': 0.498, 'train_steps_per_second': 0.498, 'train_loss': 6.826578240645559, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=38, training_loss=6.826578240645559, metrics={'train_runtime': 76.2414, 'train_samples_per_second': 0.498, 'train_steps_per_second': 0.498, 'train_loss': 6.826578240645559, 'epoch': 1.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train model\n",
    "trainer = transformers.Trainer(\n",
    "     model=model,\n",
    "     args=training_args,\n",
    "     train_dataset=dset_train,\n",
    "     eval_dataset=dset_val,\n",
    " )\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\" \", return_tensors=\"pt\")\n",
    "generation_output = model.generate(**inputs, pad_token_id=50256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I am vernacular-minded that our culture has an inherent moral dimension to it.\\n\\n\\nEven before we could talk about this, the question of morality and morality should have been completely central to the thinking of the Western civilization. We certainly know that morality is a product of human nature, and the very qualities of moral behavior are important parts of it. It is up to us to explain it and determine the course of action toward it.\\n\\n\\nBut what should the moral character of a man']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(generation_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Go, go, go, go',\n",
       " 'Go, go, go shawty',\n",
       " \"It's your birthday\",\n",
       " \"We gon' party like it's your birthday\",\n",
       " \"We gon' sip Bacardi like it's yo birthday\",\n",
       " \"And you know we don't give a fuck\",\n",
       " \"It's not your birthday!\",\n",
       " '[Chorus (2x)]',\n",
       " 'You can find me in the club,',\n",
       " 'bottle full of Bud',\n",
       " 'Look mami I got the X',\n",
       " 'if you into taking drugs',\n",
       " \"I'm into having sex, I ain't into making love\",\n",
       " 'So come give me a hug if you into getting rubbed',\n",
       " 'When I pull out up front, you see the Benz on dubs',\n",
       " \"When I roll 20 deep, it's 20 knives in the club\",\n",
       " 'Niggas heard I fuck with Dre,',\n",
       " 'now they wanna show me love',\n",
       " 'When you sell like Eminem,',\n",
       " 'and the hoes they wanna fuck',\n",
       " \"But homie ain't nothing change ho's down, G's up\",\n",
       " 'I see Xzibit in the Cut',\n",
       " 'that nigga roll that weed up',\n",
       " 'If you watch how I move',\n",
       " \"you'll mistake me for a playa or pimp\",\n",
       " 'Been hit wit a few shells but I dont walk wit a limp',\n",
       " 'In the hood, in L.A. they saying \"50 you hot\"',\n",
       " 'They like me,',\n",
       " \"I want them to love me like they love 'Pac\",\n",
       " 'But holla in New York',\n",
       " \"them niggas'll tell ya im loco\",\n",
       " 'And the plan is to put the rap game in a choke hold',\n",
       " \"I'm feelin' focused man, my money on my mind\",\n",
       " \"I got a mill out the deal and I'm still on the grind\",\n",
       " 'Now shawty said she feeling my style,',\n",
       " 'she feeling my flow',\n",
       " 'Her girlfriend wanna get bi',\n",
       " 'and they ready to go',\n",
       " '[Chorus (2x)]',\n",
       " 'You can find me in the club,',\n",
       " 'bottle full of Bud',\n",
       " 'Look mami I got the X,',\n",
       " \"if you into takin' drugs\",\n",
       " \"I'm into having sex, I ain't into making love\",\n",
       " 'So come give me a hug if you into getting rubbed',\n",
       " 'My flow, my show brought me the dough',\n",
       " 'That bought me all my fancy things',\n",
       " 'My crib, my cars, my clothes, my jewels',\n",
       " \"Look nigga I done came up and I ain't change.\",\n",
       " 'And you should love it, way more then you hate it',\n",
       " 'Nigga you mad?',\n",
       " \"I thought that you'd be happy I made it\",\n",
       " \"I'm that cat by the bar toasting to the good life\",\n",
       " 'You that faggot ass nigga trying to pull me back right?',\n",
       " \"When my junk get to pumpin in the club it's on\",\n",
       " 'I wink my eye at ya bitch,',\n",
       " 'if she smiles she gone',\n",
       " 'If the roof on fire, let the motherfucker burn',\n",
       " 'If you talking bout money homie,',\n",
       " \"I ain't concerned\",\n",
       " \"I'm a tell you what Banks told me\",\n",
       " \"cause go 'head switch the style up\",\n",
       " \"If the niggas hate then let 'em hate\",\n",
       " 'Watch the money pile up',\n",
       " 'Or we go upside their head',\n",
       " 'wit a bottle of bud',\n",
       " 'You know where we fucking be',\n",
       " '[Chorus (2x)]',\n",
       " 'You can find me in the club,',\n",
       " 'bottle full of Bud',\n",
       " 'Look mami I got the X,',\n",
       " \"if you into takin' drugs\",\n",
       " \"I'm into having sex, I ain't into making love\",\n",
       " 'So come give me a hug if you into getting rubbed',\n",
       " '[Talking]',\n",
       " \"Don't try to act like you ain't know where we been either nigga\",\n",
       " 'In the club all the time nigga, its about to pop off nigga',\n",
       " 'G-Unit']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   40,   716,   220,   933, 12754,    12, 14543,   326,   674,  3968,\n",
       "           468,   281, 11519,  6573, 15793,   284,   340,    13,   628,   198,\n",
       "          6104,   878,   356,   714,  1561,   546,   428,    11,   262,  1808,\n",
       "           286, 18016,   290, 18016,   815,   423,   587,  3190,  4318,   284,\n",
       "           262,  3612,   286,   262,  4885, 14355,    13,   775,  3729,   760,\n",
       "           326, 18016,   318,   257,  1720,   286,  1692,  3450,    11,   290,\n",
       "           262,   845, 14482,   286,  6573,  4069,   389,  1593,  3354,   286,\n",
       "           340,    13,   632,   318,   510,   284,   514,   284,  4727,   340,\n",
       "           290,  5004,   262,  1781,   286,  2223,  3812,   340,    13,   628,\n",
       "           198,  1537,   644,   815,   262,  6573,  2095,   286,   257,   582]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation_output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
