{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine Tuning GPT-2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "from gpt2_utils import Dset \n",
    "from gpt2_utils import get_model_tokenizer, train_model, generate_texts, compute_perplexity, load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set notebook variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants \n",
    "MAX_SEQ_LEN = 10\n",
    "DEVICE = 'cpu'\n",
    "VERBOSE = True\n",
    "\n",
    "GENRE = 'metal'\n",
    "\n",
    "# Name of this trained model, will be used for filename when saving the model\n",
    "MODEL_INSTANCE_NAME = 'test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in train, vallidation, and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in cleaned data\n",
    "if GENRE == 'country':\n",
    "    train_lines = pd.read_csv('data/country_train.csv', header=None).values.tolist()\n",
    "    val_lines = pd.read_csv('data/country_val.csv', header=None).values.tolist()\n",
    "    test_lines = pd.read_csv('data/country_test.csv', header=None).values.tolist()\n",
    "\n",
    "elif GENRE == 'metal':\n",
    "    train_lines = pd.read_csv('data/metal_train.csv', header=None).values.tolist()\n",
    "    val_lines = pd.read_csv('data/metal_val.csv', header=None).values.tolist()\n",
    "    test_lines = pd.read_csv('data/metal_test.csv', header=None).values.tolist()\n",
    "\n",
    "else:\n",
    "    raise ValueError('Incorrect genre given.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train lines : 149771\n",
      "val lines :  18610\n",
      "test lines :  19108\n"
     ]
    }
   ],
   "source": [
    "print('train lines :', len(train_lines))\n",
    "print('val lines : ', len(val_lines))\n",
    "print('test lines : ', len(test_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_end = math.ceil(len(train_lines)/10)\n",
    "train_lines = train_lines[0:train_end]\n",
    "\n",
    "val_end = math.ceil(len(val_lines)/10)\n",
    "val_lines = val_lines[0:val_end]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine Tuning GPT-2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at ashiqabdulkhader/GPT2-Poet.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
      "All TF 2.0 model weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "Some weights of GPT2LMHeadModel were not initialized from the TF 2.0 model and are newly initialized: ['lm_head.weight', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# get model and tokenizer\n",
    "model, tokenizer = get_model_tokenizer(MAX_SEQ_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode data\n",
    "train_encodings = [tokenizer(text=x, return_tensors='pt', padding='max_length', max_length=MAX_SEQ_LEN, truncation=True) for x in train_lines]\n",
    "train_encodings = [enc['input_ids'].tolist()[0] for enc in train_encodings]\n",
    "\n",
    "val_encodings = [tokenizer(text=x, return_tensors='pt', padding='max_length', max_length=MAX_SEQ_LEN, truncation=True) for x in val_lines]\n",
    "val_encodings = [enc['input_ids'].tolist()[0] for enc in val_encodings]\n",
    "\n",
    "test_encodings = [tokenizer(text=x, return_tensors='pt', padding='max_length', max_length=MAX_SEQ_LEN, truncation=True) for x in test_lines]\n",
    "test_encodings = [enc['input_ids'].tolist()[0] for enc in test_encodings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training, valdation, and testing datasets\n",
    "dset_train = Dset(train_encodings)\n",
    "dset_val = Dset(val_encodings)\n",
    "dset_test = Dset(test_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cad77419483f4683b15c4579b479f63a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "343caf375a694f1ab918618edb9873da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.284275531768799, 'eval_runtime': 20.2864, 'eval_samples_per_second': 91.736, 'eval_steps_per_second': 0.493, 'epoch': 1.0}\n",
      "{'train_runtime': 521.1179, 'train_samples_per_second': 28.742, 'train_steps_per_second': 0.144, 'train_loss': 3.492087809244792, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# fine tune the model\n",
    "model = train_model(model, dset_train, dset_val, GENRE, MODEL_INSTANCE_NAME, batches=200, epochs=1, lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('gpt2_tokenizers/metal/test/tokenizer_config.json',\n",
       " 'gpt2_tokenizers/metal/test/special_tokens_map.json',\n",
       " 'gpt2_tokenizers/metal/test/vocab.json',\n",
       " 'gpt2_tokenizers/metal/test/merges.txt',\n",
       " 'gpt2_tokenizers/metal/test/added_tokens.json')"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_file_path = \"gpt2_tokenizers/\"+ GENRE.lower()+'/'+MODEL_INSTANCE_NAME+\"/\"\n",
    "tokenizer.save_pretrained(tokenizer_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "solo:\n",
      "took me, he cried\n",
      "s on and when the dead begins by\n",
      "\n",
      "takes - we're real running cold\n",
      " now and then\n",
      "takes and fro the twisted stones\n",
      " your hate\n",
      " now, is to be dead again\n",
      "and\n",
      " now\n",
      "in' yeah, yeah!\n",
      "till to see that\n",
      " ooh...\n",
      "till all the guardians fade away\n"
     ]
    }
   ],
   "source": [
    "# generate lyrics\n",
    "gen_texts = generate_texts(model, tokenizer, 15)\n",
    "for text in gen_texts:\n",
    "    print(''.join(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "loaded_model = load_model(\"gpt2_trained_models/metal/gpt2_final_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      " (while highway kings who seem to join)\n",
      " after was over in his way of light.\n",
      "o' twinkle. wave your beak\n",
      ".\n",
      ".\n",
      "pop-pearl chair,when company goes\n",
      "perhaps i could never leave\n",
      " (ooh)\n",
      ".com. harper has missed the doors\n",
      "˜ king roll. bones of gold was ch\n",
      ".''—i was dressed like a bird\n",
      ".\n",
      ".\n",
      "™s where you donâ€™t\n"
     ]
    }
   ],
   "source": [
    "gen_texts = generate_texts(loaded_model, tokenizer, 15)\n",
    "for text in gen_texts:\n",
    "    print(''.join(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "till this is a kingdom in the air\n",
      " they make me beautiful\n",
      " us for love - yeah - yeah\n",
      " us for more\n",
      "\n",
      " v Volume v Landtakes a rumb\n",
      " now and now\n",
      " of a world\n",
      "s down\n",
      "\n",
      "\n",
      "s and burns again\n",
      "ing out to the battle of pain\n",
      "takes me,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#other_loaded_model = load_model(\"gpt2_trained_models/metal/test\")\n",
    "gen_texts = generate_texts(other_loaded_model, tokenizer, 15)\n",
    "for text in gen_texts:\n",
    "    print(''.join(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (177610 > 1024). Running this sequence through the model will result in indexing errors\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "94.27391815185547"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute perplexity of generated lyrics\n",
    "import numpy as np\n",
    "test_lines_flt = np.array(test_lines).flatten().tolist()\n",
    "ppl = compute_perplexity(model, tokenizer, test_lines_flt, MAX_SEQ_LEN, DEVICE)\n",
    "ppl"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
