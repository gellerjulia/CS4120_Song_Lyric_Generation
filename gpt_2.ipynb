{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine Tuning GPT-2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\J-Dog\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import transformers\n",
    "import torch \n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants \n",
    "MAX_SEQ_LEN = 50\n",
    "DEVICE = 'cpu'\n",
    "VERBOSE = True\n",
    "\n",
    "GENRE = 'Rock'\n",
    "SONG_LIMIT = 5\n",
    "\n",
    "\n",
    "# True to train by groups of lines, False to train by single lines \n",
    "BY_VERSE = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(171855, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>song_name</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>language</th>\n",
       "      <th>genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ivete sangalo</td>\n",
       "      <td>Careless Whisper</td>\n",
       "      <td>I feel so unsure\\nAs I take your hand and lead...</td>\n",
       "      <td>en</td>\n",
       "      <td>['Pop', ' Axé', ' Romântico']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ivete sangalo</td>\n",
       "      <td>Could You Be Loved / Citação Musical do Rap: S...</td>\n",
       "      <td>Don't let them fool, ya\\nOr even try to school...</td>\n",
       "      <td>en</td>\n",
       "      <td>['Pop', ' Axé', ' Romântico']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ivete sangalo</td>\n",
       "      <td>Cruisin' (Part. Saulo)</td>\n",
       "      <td>Baby, let's cruise, away from here\\nDon't be c...</td>\n",
       "      <td>en</td>\n",
       "      <td>['Pop', ' Axé', ' Romântico']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ivete sangalo</td>\n",
       "      <td>Easy</td>\n",
       "      <td>Know it sounds funny\\nBut, I just can't stand ...</td>\n",
       "      <td>en</td>\n",
       "      <td>['Pop', ' Axé', ' Romântico']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ivete sangalo</td>\n",
       "      <td>For Your Babies (The Voice cover)</td>\n",
       "      <td>You've got that look again\\nThe one I hoped I ...</td>\n",
       "      <td>en</td>\n",
       "      <td>['Pop', ' Axé', ' Romântico']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          artist                                          song_name  \\\n",
       "0  ivete sangalo                                   Careless Whisper   \n",
       "1  ivete sangalo  Could You Be Loved / Citação Musical do Rap: S...   \n",
       "2  ivete sangalo                             Cruisin' (Part. Saulo)   \n",
       "3  ivete sangalo                                               Easy   \n",
       "4  ivete sangalo                  For Your Babies (The Voice cover)   \n",
       "\n",
       "                                              lyrics language  \\\n",
       "0  I feel so unsure\\nAs I take your hand and lead...       en   \n",
       "1  Don't let them fool, ya\\nOr even try to school...       en   \n",
       "2  Baby, let's cruise, away from here\\nDon't be c...       en   \n",
       "3  Know it sounds funny\\nBut, I just can't stand ...       en   \n",
       "4  You've got that look again\\nThe one I hoped I ...       en   \n",
       "\n",
       "                          genres  \n",
       "0  ['Pop', ' Axé', ' Romântico']  \n",
       "1  ['Pop', ' Axé', ' Romântico']  \n",
       "2  ['Pop', ' Axé', ' Romântico']  \n",
       "3  ['Pop', ' Axé', ' Romântico']  \n",
       "4  ['Pop', ' Axé', ' Romântico']  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in cleaned data\n",
    "song_df = pd.read_csv('clean_data.csv')\n",
    "print(song_df.shape)\n",
    "song_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 5 / 86991 in the genre Rock\n",
      "Total sequences: 197\n"
     ]
    }
   ],
   "source": [
    "# get song lyrics of the given genre \n",
    "song_lyrics = utils.get_lyrics_in_genre(song_df, GENRE, verbose=VERBOSE, by_verse=BY_VERSE, song_limit=SONG_LIMIT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine Tuning GPT-2 Model Using Song Lyrics Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining Dataset Class\n",
    "class Dset(torch.utils.data.Dataset):\n",
    "     \"\"\"A custom dataset\"\"\"\n",
    "     def __init__(self, data: list[list[int]]):\n",
    "         self.data = []\n",
    "         for d in data:\n",
    "             input_ids = torch.tensor(d, dtype=torch.int64)\n",
    "             attention_mask = torch.ones(len(d), dtype=torch.int64)\n",
    "             self.data.append({'input_ids': input_ids,\n",
    "                  'attention_mask': attention_mask, 'labels': input_ids})\n",
    " \n",
    "     def __len__(self):\n",
    "         return len(self.data)\n",
    " \n",
    "     def __getitem__(self, idx: int):\n",
    "         return self.data[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_tokenizer():\n",
    "    # get model\n",
    "    # pipe = transformers.pipeline(task='text-generation', model='gpt2', device=DEVICE)\n",
    "    model = transformers.GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "    # set model configurations\n",
    "    config = transformers.GPT2Config.from_pretrained('gpt2')\n",
    "    config.do_sample = True\n",
    "    config.max_length = MAX_SEQ_LEN\n",
    "\n",
    "    # get tokenizer\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained('gpt2')\n",
    "\n",
    "    # add padding token to tokenizer\n",
    "    tokenizer.add_special_tokens({'pad_token': \"[PAD]\"})\n",
    "\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    model = transformers.GPT2LMHeadModel.from_pretrained('gpt2',\n",
    "                                                        config=model.config)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dset_train, dset_val):\n",
    "    \n",
    "    # set training arguments\n",
    "    training_args = transformers.TrainingArguments(\n",
    "     output_dir=\"gpt2-lyrics-model_save/training_args\",\n",
    "     learning_rate=1e-3,\n",
    "     per_device_train_batch_size=4, #TODO try changing to 20\n",
    "     per_device_eval_batch_size=4, #TODO try changing to 20\n",
    "     num_train_epochs=1,\n",
    "     evaluation_strategy='epoch',\n",
    "     save_strategy='no',\n",
    "    )\n",
    "\n",
    "    # train model\n",
    "    trainer = transformers.Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=dset_train,\n",
    "        eval_dataset=dset_val,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_texts(model, tokenizer, n_texts):\n",
    "    encoded_texts = []\n",
    "    texts = []\n",
    "\n",
    "    for _ in range(n_texts):\n",
    "        input_str = np.random.choice(list(tokenizer.get_vocab().keys()))\n",
    "        print(input_str)\n",
    "\n",
    "        input_ids = tokenizer(input_str, return_tensors=\"pt\").input_ids\n",
    "        encoded_output = model.generate(input_ids, remove_invalid_values=True, num_return_sequences=1).numpy().tolist()[0]\n",
    "        print(encoded_output)\n",
    "        text_output = tokenizer.decode(encoded_output, skip_special_tokens=True)\n",
    "        print(text_output)\n",
    "\n",
    "        encoded_texts.append(encoded_output)\n",
    "        texts.append(text_output)\n",
    "    \n",
    "    return encoded_texts, texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "\n",
    "tokenizer2 = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# add the EOS token as PAD token to avoid warnings\n",
    "model2 = AutoModelForCausalLM.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with my dog. I'm not sure if I'll ever be able to walk with my dog.\n",
      "\n",
      "I'm not sure\n"
     ]
    }
   ],
   "source": [
    "model_inputs = tokenizer('I enjoy walking with my cute dog', return_tensors='pt').to(DEVICE)\n",
    "\n",
    "# generate 40 new tokens\n",
    "greedy_output = model.generate(**model_inputs, max_new_tokens=40)\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(greedy_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Config {\n",
       "  \"_name_or_path\": \"gpt2\",\n",
       "  \"activation_function\": \"gelu_new\",\n",
       "  \"architectures\": [\n",
       "    \"GPT2LMHeadModel\"\n",
       "  ],\n",
       "  \"attn_pdrop\": 0.1,\n",
       "  \"bos_token_id\": 50256,\n",
       "  \"embd_pdrop\": 0.1,\n",
       "  \"eos_token_id\": 50256,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"layer_norm_epsilon\": 1e-05,\n",
       "  \"model_type\": \"gpt2\",\n",
       "  \"n_ctx\": 1024,\n",
       "  \"n_embd\": 768,\n",
       "  \"n_head\": 12,\n",
       "  \"n_inner\": null,\n",
       "  \"n_layer\": 12,\n",
       "  \"n_positions\": 1024,\n",
       "  \"pad_token_id\": 50257,\n",
       "  \"reorder_and_upcast_attn\": false,\n",
       "  \"resid_pdrop\": 0.1,\n",
       "  \"scale_attn_by_inverse_layer_idx\": false,\n",
       "  \"scale_attn_weights\": true,\n",
       "  \"summary_activation\": null,\n",
       "  \"summary_first_dropout\": 0.1,\n",
       "  \"summary_proj_to_labels\": true,\n",
       "  \"summary_type\": \"cls_index\",\n",
       "  \"summary_use_proj\": true,\n",
       "  \"task_specific_params\": {\n",
       "    \"text-generation\": {\n",
       "      \"do_sample\": true,\n",
       "      \"max_length\": 50\n",
       "    }\n",
       "  },\n",
       "  \"transformers_version\": \"4.35.2\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 50258\n",
       "}"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Config {\n",
       "  \"_name_or_path\": \"gpt2\",\n",
       "  \"activation_function\": \"gelu_new\",\n",
       "  \"architectures\": [\n",
       "    \"GPT2LMHeadModel\"\n",
       "  ],\n",
       "  \"attn_pdrop\": 0.1,\n",
       "  \"bos_token_id\": 50256,\n",
       "  \"embd_pdrop\": 0.1,\n",
       "  \"eos_token_id\": 50256,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"layer_norm_epsilon\": 1e-05,\n",
       "  \"model_type\": \"gpt2\",\n",
       "  \"n_ctx\": 1024,\n",
       "  \"n_embd\": 768,\n",
       "  \"n_head\": 12,\n",
       "  \"n_inner\": null,\n",
       "  \"n_layer\": 12,\n",
       "  \"n_positions\": 1024,\n",
       "  \"pad_token_id\": 50256,\n",
       "  \"reorder_and_upcast_attn\": false,\n",
       "  \"resid_pdrop\": 0.1,\n",
       "  \"scale_attn_by_inverse_layer_idx\": false,\n",
       "  \"scale_attn_weights\": true,\n",
       "  \"summary_activation\": null,\n",
       "  \"summary_first_dropout\": 0.1,\n",
       "  \"summary_proj_to_labels\": true,\n",
       "  \"summary_type\": \"cls_index\",\n",
       "  \"summary_use_proj\": true,\n",
       "  \"task_specific_params\": {\n",
       "    \"text-generation\": {\n",
       "      \"do_sample\": true,\n",
       "      \"max_length\": 50\n",
       "    }\n",
       "  },\n",
       "  \"transformers_version\": \"4.35.2\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 50257\n",
       "}"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bodar.n@northeastern.edu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vote\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "argument 'ids': 'list' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\J-Dog\\23FALL\\CS4120_Song_Lyric_Generation\\gpt_2.ipynb Cell 11\u001b[0m line \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/J-Dog/23FALL/CS4120_Song_Lyric_Generation/gpt_2.ipynb#Y141sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m encoded_output \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mgenerate(input_ids, remove_invalid_values\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, num_return_sequences\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mtolist()[\u001b[39m0\u001b[39m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/J-Dog/23FALL/CS4120_Song_Lyric_Generation/gpt_2.ipynb#Y141sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m encoded_output \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(encoded_output)\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mtolist()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/J-Dog/23FALL/CS4120_Song_Lyric_Generation/gpt_2.ipynb#Y141sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m text_output \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39;49mdecode(encoded_output, skip_special_tokens\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/J-Dog/23FALL/CS4120_Song_Lyric_Generation/gpt_2.ipynb#Y141sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mprint\u001b[39m(text_output)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/J-Dog/23FALL/CS4120_Song_Lyric_Generation/gpt_2.ipynb#Y141sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m encoded_texts\u001b[39m.\u001b[39mappend(encoded_output)\n",
      "File \u001b[1;32mc:\\Users\\J-Dog\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3746\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.decode\u001b[1;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[0;32m   3743\u001b[0m \u001b[39m# Convert inputs to python lists\u001b[39;00m\n\u001b[0;32m   3744\u001b[0m token_ids \u001b[39m=\u001b[39m to_py_obj(token_ids)\n\u001b[1;32m-> 3746\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decode(\n\u001b[0;32m   3747\u001b[0m     token_ids\u001b[39m=\u001b[39mtoken_ids,\n\u001b[0;32m   3748\u001b[0m     skip_special_tokens\u001b[39m=\u001b[39mskip_special_tokens,\n\u001b[0;32m   3749\u001b[0m     clean_up_tokenization_spaces\u001b[39m=\u001b[39mclean_up_tokenization_spaces,\n\u001b[0;32m   3750\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   3751\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\J-Dog\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\tokenization_utils_fast.py:625\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._decode\u001b[1;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[0;32m    623\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(token_ids, \u001b[39mint\u001b[39m):\n\u001b[0;32m    624\u001b[0m     token_ids \u001b[39m=\u001b[39m [token_ids]\n\u001b[1;32m--> 625\u001b[0m text \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tokenizer\u001b[39m.\u001b[39;49mdecode(token_ids, skip_special_tokens\u001b[39m=\u001b[39;49mskip_special_tokens)\n\u001b[0;32m    627\u001b[0m clean_up_tokenization_spaces \u001b[39m=\u001b[39m (\n\u001b[0;32m    628\u001b[0m     clean_up_tokenization_spaces\n\u001b[0;32m    629\u001b[0m     \u001b[39mif\u001b[39;00m clean_up_tokenization_spaces \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclean_up_tokenization_spaces\n\u001b[0;32m    631\u001b[0m )\n\u001b[0;32m    632\u001b[0m \u001b[39mif\u001b[39;00m clean_up_tokenization_spaces:\n",
      "\u001b[1;31mTypeError\u001b[0m: argument 'ids': 'list' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "input_str = np.random.choice(list(tokenizer.get_vocab().keys()))\n",
    "print(input_str)\n",
    "\n",
    "input_ids = tokenizer(input_str, return_tensors=\"pt\").input_ids\n",
    "encoded_output = model.generate(input_ids, remove_invalid_values=True, num_return_sequences=1).numpy().tolist()[0]\n",
    "encoded_output = np.array(encoded_output).reshape(-1,1).tolist()\n",
    "text_output = tokenizer.decode(encoded_output, skip_special_tokens=True)\n",
    "print(text_output)\n",
    "\n",
    "encoded_texts.append(encoded_output)\n",
    "texts.append(text_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'keeper': 13884, 'ł': 254, 'goal': 35231}"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{k:v for k,v in zip(tokenizer.get_vocab().keys(), tokenizer.get_vocab().values()) if v in [ 254, 35231, 13884]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ġdestroys\n",
      "[128, 254, 16520, 305, 893, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257]\n",
      "Ġdestroys[PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([[128,\n",
       "   254,\n",
       "   16520,\n",
       "   305,\n",
       "   893,\n",
       "   50257,\n",
       "   50257,\n",
       "   50257,\n",
       "   50257,\n",
       "   50257,\n",
       "   50257,\n",
       "   50257,\n",
       "   50257,\n",
       "   50257,\n",
       "   50257,\n",
       "   50257,\n",
       "   50257,\n",
       "   50257,\n",
       "   50257,\n",
       "   50257]],\n",
       " ['Ġdestroys[PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]'])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_texts(model, tokenizer, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get model and tokenizer\n",
    "model, tokenizer = get_model_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training, valdation, and testing data \n",
    "train_data, val_data = train_test_split(song_lyrics, train_size=0.8)\n",
    "\n",
    "# encode data\n",
    "train_encodings = [tokenizer(text=x, return_tensors='pt', padding='max_length', max_length=MAX_SEQ_LEN, truncation=True) for x in train_data]\n",
    "train_encodings = [enc['input_ids'].tolist()[0] for enc in train_encodings]\n",
    "\n",
    "val_encodings = [tokenizer(text=x, return_tensors='pt', padding='max_length', max_length=MAX_SEQ_LEN, truncation=True) for x in val_data]\n",
    "val_encodings = [enc['input_ids'].tolist()[0] for enc in val_encodings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training and valdation datasets\n",
    "dset_train = Dset(train_encodings)\n",
    "dset_val = Dset(val_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      "100%|██████████| 40/40 [03:57<00:00,  5.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7281273007392883, 'eval_runtime': 7.6393, 'eval_samples_per_second': 5.236, 'eval_steps_per_second': 1.309, 'epoch': 1.0}\n",
      "{'train_runtime': 237.0778, 'train_samples_per_second': 0.662, 'train_steps_per_second': 0.169, 'train_loss': 2.236800765991211, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# fine tune the model\n",
    "model = train_model(model, dset_train, dset_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\J-Dog\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\generation\\utils.py:1473: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n",
      "c:\\Users\\J-Dog\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['H', 'H', 'H']\n"
     ]
    }
   ],
   "source": [
    "# generate 3 lyrics\n",
    "encoded_gen_texts, gen_texts = generate_texts(model, tokenizer, 3)\n",
    "print(gen_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perplexity(model, tokenizer, test_text):\n",
    "    \n",
    "    encodings = tokenizer(\"\\n\\n\".join(test_text), return_tensors=\"pt\")\n",
    "\n",
    "    max_length = model.config.n_positions\n",
    "    stride = 512\n",
    "    seq_len = MAX_SEQ_LEN\n",
    "\n",
    "    nlls = []\n",
    "    prev_end_loc = 0\n",
    "    for begin_loc in tqdm(range(0, seq_len, stride)):\n",
    "        end_loc = min(begin_loc + max_length, seq_len)\n",
    "        trg_len = end_loc - prev_end_loc  # may be different from stride on last loop\n",
    "        input_ids = encodings.input_ids[:, begin_loc:end_loc].to(DEVICE)\n",
    "        target_ids = input_ids.clone()\n",
    "        target_ids[:, :-trg_len] = -100\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, labels=target_ids)\n",
    "\n",
    "            # loss is calculated using CrossEntropyLoss which averages over valid labels\n",
    "            # N.B. the model only calculates loss over trg_len - 1 labels, because it internally shifts the labels\n",
    "            # to the left by 1.\n",
    "            neg_log_likelihood = outputs.loss\n",
    "\n",
    "        nlls.append(neg_log_likelihood)\n",
    "\n",
    "        prev_end_loc = end_loc\n",
    "        if end_loc == seq_len:\n",
    "            break\n",
    "\n",
    "    ppl = torch.exp(torch.stack(nlls).mean())\n",
    "    # convert to scalar\n",
    "    ppl = ppl.numpy().tolist()\n",
    "\n",
    "    return ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20.20557975769043"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute perplexity of generated lyrics\n",
    "ppl = compute_perplexity(model, tokenizer, gen_texts)\n",
    "ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
