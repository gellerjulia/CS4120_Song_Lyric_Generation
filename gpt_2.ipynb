{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine Tuning GPT-2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "from gpt2_utils import Dset \n",
    "from gpt2_utils import get_model_tokenizer, train_model, generate_texts, compute_perplexity, load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set notebook variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants \n",
    "MAX_SEQ_LEN = 10\n",
    "DEVICE = 'cpu'\n",
    "VERBOSE = True\n",
    "\n",
    "GENRE = 'metal'\n",
    "\n",
    "# Name of this trained model, will be used for filename when saving the model\n",
    "MODEL_INSTANCE_NAME = 'foo'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in train, vallidation, and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in cleaned data\n",
    "if GENRE == 'country':\n",
    "    train_lines = pd.read_csv('data/country_train.csv', header=None).values.tolist()\n",
    "    val_lines = pd.read_csv('data/country_val.csv', header=None).values.tolist()\n",
    "    test_lines = pd.read_csv('data/country_test.csv', header=None).values.tolist()\n",
    "\n",
    "elif GENRE == 'metal':\n",
    "    train_lines = pd.read_csv('data/metal_train.csv', header=None).values.tolist()\n",
    "    val_lines = pd.read_csv('data/metal_val.csv', header=None).values.tolist()\n",
    "    test_lines = pd.read_csv('data/metal_test.csv', header=None).values.tolist()\n",
    "\n",
    "else:\n",
    "    raise ValueError('Incorrect genre given.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train lines : 149771\n",
      "val lines :  18610\n",
      "test lines :  19108\n"
     ]
    }
   ],
   "source": [
    "print('train lines :', len(train_lines))\n",
    "print('val lines : ', len(val_lines))\n",
    "print('test lines : ', len(test_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_end = math.ceil(len(train_lines)/4)\n",
    "train_lines = train_lines[0:train_end]\n",
    "\n",
    "#val_end = math.ceil(len(val_lines)/4)\n",
    "#val_lines = val_lines[0:val_end]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine Tuning GPT-2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All TF 2.0 model weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "Some weights of GPT2LMHeadModel were not initialized from the TF 2.0 model and are newly initialized: ['lm_head.weight', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# get model and tokenizer\n",
    "model, tokenizer = get_model_tokenizer(MAX_SEQ_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode data\n",
    "train_encodings = [tokenizer(text=x, return_tensors='tf', padding='max_length', max_length=MAX_SEQ_LEN, truncation=True) for x in train_lines]\n",
    "train_encodings = [enc['input_ids'].numpy().tolist()[0] for enc in train_encodings]\n",
    "\n",
    "val_encodings = [tokenizer(text=x, return_tensors='tf', padding='max_length', max_length=MAX_SEQ_LEN, truncation=True) for x in val_lines]\n",
    "val_encodings = [enc['input_ids'].numpy().tolist()[0] for enc in val_encodings]\n",
    "\n",
    "test_encodings = [tokenizer(text=x, return_tensors='tf', padding='max_length', max_length=MAX_SEQ_LEN, truncation=True) for x in test_lines]\n",
    "test_encodings = [enc['input_ids'].numpy().tolist()[0] for enc in test_encodings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training, valdation, and testing datasets\n",
    "dset_train = Dset(train_encodings)\n",
    "dset_val = Dset(val_encodings)\n",
    "dset_test = Dset(test_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f45a1ca323746e491cb340aebe762c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/375 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86a6f69f40564ce7ba5b1123dbdb1458",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.997827529907227, 'eval_runtime': 72.765, 'eval_samples_per_second': 63.946, 'eval_steps_per_second': 0.646, 'epoch': 1.0}\n",
      "{'train_runtime': 1491.3113, 'train_samples_per_second': 25.107, 'train_steps_per_second': 0.251, 'train_loss': 4.483689453125, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# fine tune the model\n",
    "model = train_model(model, dset_train, dset_val, GENRE, MODEL_INSTANCE_NAME, batches=100, epochs=1, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shaem\\anaconda3\\Lib\\site-packages\\transformers\\generation\\utils.py:1473: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and we've gonna see\n",
      " it's in one\n",
      " like a little woman\n",
      " me than we're walking over my lips\n",
      " but all in heaven you're walking in the\n",
      " to lose me more i've been good on\n",
      " what you're all the sun\n",
      " the little to make your life.\n",
      " to buy you\n",
      " in fire like you can me\n",
      ", just where they might make you hold me\n",
      " for you? not a few more i've\n",
      "\n",
      " in town and just all in home\n",
      " in the door\n"
     ]
    }
   ],
   "source": [
    "# generate lyrics\n",
    "gen_texts = generate_texts(model, tokenizer, 15)\n",
    "for text in gen_texts:\n",
    "    print(''.join(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " no remorse, they want to make me\n",
      "\n",
      " now i cannot hide my mind.\n",
      "\n",
      "\n",
      ", my way and\n",
      " you with misery\n",
      ", you in the one is not a world\n",
      ", oh, all your world,\n",
      ", i've been alone, i am the\n",
      "\n",
      ", in pain and the night in fire\n",
      " you and you\n",
      " than the sun\n",
      " to the sun in the world\n"
     ]
    }
   ],
   "source": [
    "loaded_model = load_model(\"gpt2_trained_models/metal/gpt2_final_model_epoch1_quarter\")\n",
    "gen_texts = generate_texts(loaded_model, tokenizer, 15)\n",
    "for text in gen_texts:\n",
    "    print(''.join(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 162/162 [01:18<00:00,  2.06it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2844.3903038997314"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "test_data = np.array(val_lines).flatten().tolist()\n",
    "model = loaded_model\n",
    "\n",
    "# being func\n",
    "encodings = tokenizer(\"\\n\\n\".join(test_data), return_tensors=\"tf\")\n",
    "\n",
    "max_length = 10\n",
    "stride = 1024\n",
    "seq_len = encodings.input_ids.shape[1]\n",
    "\n",
    "nlls = []\n",
    "prev_end_loc = 0\n",
    "for begin_loc in tqdm(range(0, seq_len, stride)):\n",
    "    end_loc = min(begin_loc + max_length, seq_len)\n",
    "    trg_len = end_loc - prev_end_loc  # may be different from stride on last loop\n",
    "    input_ids = encodings.input_ids[:, begin_loc:end_loc]\n",
    "    target_ids_np = tf.identity(input_ids).numpy()\n",
    "    target_ids_np[:, :-trg_len] = -100 \n",
    "    target_ids = tf.convert_to_tensor(np.array(target_ids_np))\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=target_ids)\n",
    "\n",
    "        # loss is calculated using CrossEntropyLoss which averages over valid labels\n",
    "        # N.B. the model only calculates loss over trg_len - 1 labels, because it internally shifts the labels\n",
    "        # to the left by 1.\n",
    "        neg_log_likelihood = outputs.loss\n",
    "\n",
    "    nlls.append(neg_log_likelihood)\n",
    "\n",
    "    prev_end_loc = end_loc\n",
    "    if end_loc == seq_len:\n",
    "        break\n",
    "\n",
    "ppl = math.exp(np.mean(nlls))\n",
    "\n",
    "ppl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 10), dtype=int32, numpy=\n",
       "array([[40252, 11752,  5156,  1560,   262,   582,   379,   262,  7846,\n",
       "         1302]])>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids[:, [-1,0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute perplexity of generated lyrics\n",
    "import numpy as np\n",
    "#test_lines_first = test_lines[:math.ceil(len(test_lines)/4)]\n",
    "test_lines_flt = np.array(test_lines).flatten().tolist()\n",
    "ppl = compute_perplexity(loaded_model, tokenizer, test_lines_flt, MAX_SEQ_LEN, DEVICE)\n",
    "ppl"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
