{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import transformers\n",
    "import torch \n",
    "import math\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "GENRE = 'Rock'\n",
    "# use all songs\n",
    "N_SONGS = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in  data\n",
    "df = pd.read_csv('clean_data.csv')\n",
    "# filter data by genre\n",
    "genre_df = df[df.genres.apply(lambda x: GENRE in x)]\n",
    "\n",
    "# save lyrics\n",
    "lyrics = genre_df.lyrics.values\n",
    "# split lyrics by line\n",
    "lines = [song.split('\\n') for song in lyrics[0:N_SONGS]]\n",
    "lines = list(chain.from_iterable(lines))\n",
    "# remove empty lines\n",
    "lines = [line for line in lines if len(line)>0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name model\n",
    "MODEL_NAME = 'gpt2'\n",
    "pipe = transformers.pipeline(task='text-generation', model=MODEL_NAME, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set model \n",
    "model = transformers.GPT2LMHeadModel.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# set model configurations\n",
    "config = transformers.GPT2Config.from_pretrained(MODEL_NAME)\n",
    "config.do_sample = True\n",
    "config.max_length = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set tokenizer\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# add padding token to tokenizer\n",
    "tokenizer.add_special_tokens({'pad_token': \"[PAD]\"})\n",
    "\n",
    "# encode training data\n",
    "enc_tokens = [tokenizer(text=line, return_tensors='pt', padding='max_length', max_length=50, truncation=True) for line in lines]\n",
    "enc_tokens = [enc['input_ids'].tolist()[0] for enc in enc_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = transformers.GPT2LMHeadModel.from_pretrained(MODEL_NAME,\n",
    "                                                      config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDset(torch.utils.data.Dataset):\n",
    "     \"\"\"A custom dataset\"\"\"\n",
    "     def __init__(self, data: list[list[int]]):\n",
    "         self.data = []\n",
    "         for d in data:\n",
    "             input_ids = torch.tensor(d, dtype=torch.int64)\n",
    "             attention_mask = torch.ones(len(d), dtype=torch.int64)\n",
    "             self.data.append({'input_ids': input_ids,\n",
    "                  'attention_mask': attention_mask, 'labels': input_ids})\n",
    " \n",
    "     def __len__(self):\n",
    "         return len(self.data)\n",
    " \n",
    "     def __getitem__(self, idx: int):\n",
    "         return self.data[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = transformers.TrainingArguments(\n",
    "     output_dir=\"idiot_save/\",\n",
    "     learning_rate=1e-3,\n",
    "     per_device_train_batch_size=4, #TODO try changing to 20\n",
    "     per_device_eval_batch_size=4, #TODO try changing to 20\n",
    "     num_train_epochs=1,\n",
    "     evaluation_strategy='epoch',\n",
    "     save_strategy='no',\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training, valdation, and testing data intervals\n",
    "END1 = math.ceil(len(enc_tokens)*0.8)\n",
    "END2 = END1 + math.ceil(len(enc_tokens)*0.1)\n",
    "\n",
    "# create training, valdation, and testing data\n",
    "dset_train = MyDset(enc_tokens[0:END1])\n",
    "dset_val = MyDset(enc_tokens[END1:END2])\n",
    "dset_test = MyDset(enc_tokens[END2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30f183a5012e414d85c5b2bced6e44ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/88 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\shaem\\NLP\\Project\\CS4120_Song_Lyric_Generation\\gpt_2.ipynb Cell 12\u001b[0m line \u001b[0;36m9\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/shaem/NLP/Project/CS4120_Song_Lyric_Generation/gpt_2.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# train model\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/shaem/NLP/Project/CS4120_Song_Lyric_Generation/gpt_2.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m trainer \u001b[39m=\u001b[39m transformers\u001b[39m.\u001b[39mTrainer(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/shaem/NLP/Project/CS4120_Song_Lyric_Generation/gpt_2.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m      model\u001b[39m=\u001b[39mmodel,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/shaem/NLP/Project/CS4120_Song_Lyric_Generation/gpt_2.ipynb#X12sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m      args\u001b[39m=\u001b[39mtraining_args,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/shaem/NLP/Project/CS4120_Song_Lyric_Generation/gpt_2.ipynb#X12sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m      train_dataset\u001b[39m=\u001b[39mdset_train,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/shaem/NLP/Project/CS4120_Song_Lyric_Generation/gpt_2.ipynb#X12sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m      eval_dataset\u001b[39m=\u001b[39mdset_val,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/shaem/NLP/Project/CS4120_Song_Lyric_Generation/gpt_2.ipynb#X12sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m  )\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/shaem/NLP/Project/CS4120_Song_Lyric_Generation/gpt_2.ipynb#X12sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[1;32mc:\\Users\\shaem\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:1555\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1553\u001b[0m         hf_hub_utils\u001b[39m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1554\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1555\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   1556\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m   1557\u001b[0m         resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[0;32m   1558\u001b[0m         trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[0;32m   1559\u001b[0m         ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[0;32m   1560\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\shaem\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:1860\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1857\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_step_begin(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n\u001b[0;32m   1859\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39maccumulate(model):\n\u001b[1;32m-> 1860\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[0;32m   1862\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   1863\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   1864\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m   1865\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   1866\u001b[0m ):\n\u001b[0;32m   1867\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   1868\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\shaem\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:2725\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   2722\u001b[0m     \u001b[39mreturn\u001b[39;00m loss_mb\u001b[39m.\u001b[39mreduce_mean()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m   2724\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss_context_manager():\n\u001b[1;32m-> 2725\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_loss(model, inputs)\n\u001b[0;32m   2727\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mn_gpu \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   2728\u001b[0m     loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mmean()  \u001b[39m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\shaem\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:2748\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[0;32m   2746\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2747\u001b[0m     labels \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 2748\u001b[0m outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs)\n\u001b[0;32m   2749\u001b[0m \u001b[39m# Save past state if it exists\u001b[39;00m\n\u001b[0;32m   2750\u001b[0m \u001b[39m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[0;32m   2751\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mpast_index \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\shaem\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\shaem\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\shaem\\anaconda3\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:1074\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1066\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1067\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1068\u001b[0m \u001b[39m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[0;32m   1069\u001b[0m \u001b[39m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[0;32m   1070\u001b[0m \u001b[39m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[0;32m   1071\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1072\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1074\u001b[0m transformer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(\n\u001b[0;32m   1075\u001b[0m     input_ids,\n\u001b[0;32m   1076\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[0;32m   1077\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m   1078\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[0;32m   1079\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[0;32m   1080\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m   1081\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[0;32m   1082\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[0;32m   1083\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[0;32m   1084\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m   1085\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1086\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m   1087\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m   1088\u001b[0m )\n\u001b[0;32m   1089\u001b[0m hidden_states \u001b[39m=\u001b[39m transformer_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m   1091\u001b[0m \u001b[39m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\shaem\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\shaem\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\shaem\\anaconda3\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:837\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    834\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mn_layer)\n\u001b[0;32m    836\u001b[0m \u001b[39mif\u001b[39;00m inputs_embeds \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 837\u001b[0m     inputs_embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwte(input_ids)\n\u001b[0;32m    838\u001b[0m position_embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwpe(position_ids)\n\u001b[0;32m    839\u001b[0m hidden_states \u001b[39m=\u001b[39m inputs_embeds \u001b[39m+\u001b[39m position_embeds\n",
      "File \u001b[1;32mc:\\Users\\shaem\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\shaem\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\shaem\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 162\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49membedding(\n\u001b[0;32m    163\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_idx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_norm,\n\u001b[0;32m    164\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm_type, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_grad_by_freq, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse)\n",
      "File \u001b[1;32mc:\\Users\\shaem\\anaconda3\\Lib\\site-packages\\torch\\nn\\functional.py:2233\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2227\u001b[0m     \u001b[39m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2228\u001b[0m     \u001b[39m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2229\u001b[0m     \u001b[39m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2230\u001b[0m     \u001b[39m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2231\u001b[0m     \u001b[39m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2232\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[39minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2233\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49membedding(weight, \u001b[39minput\u001b[39;49m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[1;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "trainer = transformers.Trainer(\n",
    "     model=model,\n",
    "     args=training_args,\n",
    "     train_dataset=dset_train,\n",
    "     eval_dataset=dset_val,\n",
    " )\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index -1 is out of bounds for dimension 1 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\shaem\\NLP\\Project\\CS4120_Song_Lyric_Generation\\gpt_2.ipynb Cell 13\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/shaem/NLP/Project/CS4120_Song_Lyric_Generation/gpt_2.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m inputs \u001b[39m=\u001b[39m tokenizer(\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/shaem/NLP/Project/CS4120_Song_Lyric_Generation/gpt_2.ipynb#X13sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m generation_output \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs, pad_token_id\u001b[39m=\u001b[39;49m\u001b[39m50256\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\shaem\\anaconda3\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\shaem\\anaconda3\\Lib\\site-packages\\transformers\\generation\\utils.py:1538\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   1531\u001b[0m \u001b[39m# decoder-only models should use left-padding for generation\u001b[39;00m\n\u001b[0;32m   1532\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder:\n\u001b[0;32m   1533\u001b[0m     \u001b[39m# If `input_ids` was given, check if the last id in any sequence is `pad_token_id`\u001b[39;00m\n\u001b[0;32m   1534\u001b[0m     \u001b[39m# Note: If using, `inputs_embeds` this check does not work, because we want to be more hands-off.\u001b[39;00m\n\u001b[0;32m   1535\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   1536\u001b[0m         generation_config\u001b[39m.\u001b[39mpad_token_id \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(inputs_tensor\u001b[39m.\u001b[39mshape) \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m\n\u001b[1;32m-> 1538\u001b[0m         \u001b[39mand\u001b[39;00m torch\u001b[39m.\u001b[39msum(inputs_tensor[:, \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m] \u001b[39m==\u001b[39m generation_config\u001b[39m.\u001b[39mpad_token_id) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m   1539\u001b[0m     ):\n\u001b[0;32m   1540\u001b[0m         logger\u001b[39m.\u001b[39mwarning(\n\u001b[0;32m   1541\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mA decoder-only architecture is being used, but right-padding was detected! For correct \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1542\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mgeneration results, please set `padding_side=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mleft\u001b[39m\u001b[39m'\u001b[39m\u001b[39m` when initializing the tokenizer.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1543\u001b[0m         )\n\u001b[0;32m   1545\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mencoder_outputs\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m model_kwargs:\n\u001b[0;32m   1546\u001b[0m     \u001b[39m# if model is encoder decoder encoder_outputs are created\u001b[39;00m\n\u001b[0;32m   1547\u001b[0m     \u001b[39m# and added to `model_kwargs`\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: index -1 is out of bounds for dimension 1 with size 0"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\"\", return_tensors=\"pt\")\n",
    "generation_output = model.generate(**inputs, pad_token_id=50256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generation_output' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\shaem\\NLP\\Project\\CS4120_Song_Lyric_Generation\\gpt_2.ipynb Cell 14\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/shaem/NLP/Project/CS4120_Song_Lyric_Generation/gpt_2.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m tokenizer\u001b[39m.\u001b[39mbatch_decode(generation_output)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'generation_output' is not defined"
     ]
    }
   ],
   "source": [
    "tokenizer.batch_decode(generation_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Giant steps are what you take',\n",
       " 'Walking on the moon',\n",
       " \"I hope my legs don't break\",\n",
       " 'Walking on the moon',\n",
       " 'We could walk forever',\n",
       " 'Walking on the moon',\n",
       " 'We could live together',\n",
       " 'Walking on, walking on the moon',\n",
       " 'Walking back from your house',\n",
       " 'Walking on the moon',\n",
       " 'Walking back from your house',\n",
       " 'Walking on the moon',\n",
       " 'Feet they hardly touch the ground',\n",
       " 'Walking on the moon',\n",
       " \"My feet don't hardly make no sound\",\n",
       " 'Walking on, walking on the moon',\n",
       " 'Some may say',\n",
       " \"I'm wishing my days away\",\n",
       " 'No way',\n",
       " \"And if it's the price I pay\",\n",
       " 'Some say',\n",
       " \"Tomorrow's another day\",\n",
       " 'You stay',\n",
       " 'I may as well play',\n",
       " 'Giant steps are what you take',\n",
       " 'Walking on the moon',\n",
       " \"I hope my legs don't break\",\n",
       " 'Walking on the moon',\n",
       " 'We could walk forever',\n",
       " 'Walking on the moon',\n",
       " 'We could be together',\n",
       " 'Walking on, walking on the moon',\n",
       " 'Some may say',\n",
       " \"I'm wishing my days away\",\n",
       " 'No way',\n",
       " \"And if it's the price I pay\",\n",
       " 'Some say',\n",
       " \"Tomorrow's another day\",\n",
       " 'You stay',\n",
       " 'I may as well play',\n",
       " 'Keep it up, keep it up',\n",
       " 'Wise men say, only fools rush in',\n",
       " \"But I can't help, falling in love with you\",\n",
       " 'Shall I stay? Would it be a sin',\n",
       " \"If I can't help, falling in love with you?\",\n",
       " 'Like a river flows, surely to the sea',\n",
       " 'Darling, so it goes somethings are meant to be',\n",
       " 'Take my hand, take my whole life too',\n",
       " \"For I can't help, Falling in love with you\",\n",
       " 'Like a river flows, surely to the sea',\n",
       " 'Darling so it goes, somethings are meant to be',\n",
       " 'Take my hand, take my whole life too',\n",
       " \"For I can't help falling in love with you\",\n",
       " \"For I can't help falling in love with you\",\n",
       " \"We're caught in a trap\",\n",
       " \"I can't walk out\",\n",
       " 'Because I love you too much baby',\n",
       " \"Why can't you see\",\n",
       " \"What you're doing to me\",\n",
       " \"When you don't believe a word I say?\",\n",
       " \"We can't go on together\",\n",
       " 'With suspicious minds',\n",
       " \"And we can't build our dreams\",\n",
       " 'On suspicious minds',\n",
       " 'So, if an old friend I know',\n",
       " 'Drops by to say hello',\n",
       " 'Would I still see suspicion in your eyes?',\n",
       " 'Here we go again',\n",
       " \"Asking where I've been\",\n",
       " \"You can't see these tears are real\",\n",
       " \"I'm crying\",\n",
       " \"We can't go on together\",\n",
       " 'With suspicious minds',\n",
       " \"And we can't build our dreams\",\n",
       " 'On suspicious minds',\n",
       " 'Oh let our love survive',\n",
       " 'Or dry the tears from your eyes',\n",
       " \"Let's not let a good thing die\",\n",
       " 'When honey, you know',\n",
       " \"I've never lied to you\",\n",
       " \"(we can't go on together)\",\n",
       " \"We can't go on together\",\n",
       " 'With suspicious minds',\n",
       " \"And we can't build our dreams\",\n",
       " 'On suspicious minds',\n",
       " \"We're caught in a trap\",\n",
       " \"I can't walk out\",\n",
       " 'Because I love you too much baby',\n",
       " \"Why can't you see\",\n",
       " \"What you're doing to me\",\n",
       " \"When you don't believe a word I say?\",\n",
       " \"We're caught in a trap\",\n",
       " \"I can't walk out\",\n",
       " 'Because I love you too much baby',\n",
       " \"Maybe I didn't treat you,\",\n",
       " 'Quite as good as I should have.',\n",
       " \"Maybe I didn't love you,\",\n",
       " 'Quite as often as I could have.',\n",
       " 'Little things I should have said and done,',\n",
       " 'I just never took the time.',\n",
       " 'You were always on my mind.',\n",
       " 'You were always on my mind.',\n",
       " \"Maybe I didn't hold you,\",\n",
       " 'All those lonely, lonely times.',\n",
       " 'And I guess I never told you,',\n",
       " \"I'm so happy that you're mine.\",\n",
       " 'If I made you feel second best,',\n",
       " \"Girl I'm so sorry, I was blind.\",\n",
       " 'You were always on my mind.',\n",
       " 'You were always on my mind.',\n",
       " 'Tell me.',\n",
       " \"Tell me that your sweet love hasn't died.\",\n",
       " 'Give me.',\n",
       " 'Give me one more chance to keep you satisfied.',\n",
       " 'Satisfied.',\n",
       " 'Little things I should have said and done,',\n",
       " 'I just never took the time.',\n",
       " 'You were always on my mind.',\n",
       " 'You were always on my mind.',\n",
       " 'You were always on my mind.',\n",
       " 'You were always on my mind',\n",
       " 'Love me tender, love me sweet,',\n",
       " 'Never let me go.',\n",
       " 'You have made my life complete,',\n",
       " 'And I love you so.',\n",
       " 'Love me tender, love me true,',\n",
       " 'All my dreams fulfill.',\n",
       " \"For my darlin' I love you,\",\n",
       " 'And I always will.',\n",
       " 'Love me tender, love me long,',\n",
       " 'Take me to your heart.',\n",
       " \"For it's there that I belong,\",\n",
       " \"And we'll never part.\",\n",
       " 'Love me tender, love me true,',\n",
       " 'All my dreams fulfill.',\n",
       " \"For my darlin' I love you,\",\n",
       " 'And I always will.',\n",
       " 'Love me tender, love me dear,',\n",
       " 'Tell me you are mine.',\n",
       " \"I'll be yours through all the years,\",\n",
       " 'Till the end of time.',\n",
       " 'Love me tender, love me true,',\n",
       " 'All my dreams fulfill.',\n",
       " \"For my darlin' I love you,\",\n",
       " 'And I always will.',\n",
       " 'And now the end is near',\n",
       " 'And so I face the final curtain',\n",
       " \"My friend, I'll say it clear\",\n",
       " \"I'll state my case of which I'm certain\",\n",
       " \"I've lived a life that's full\",\n",
       " \"I've traveled each and every highway\",\n",
       " 'And more, much more than this',\n",
       " 'I did it my way',\n",
       " \"Regrets, I've had a few\",\n",
       " 'But then again, too few to mention',\n",
       " 'I did what I had to do',\n",
       " 'And saw it through without exemption',\n",
       " 'I planned each charted course',\n",
       " 'Each careful step along the byway',\n",
       " 'Oh, and more, much more than this',\n",
       " 'I did it my way',\n",
       " \"Yes, there were times, I'm sure you knew\",\n",
       " 'When I bit off more than I could chew',\n",
       " 'But through it all when there was doubt',\n",
       " 'I ate it up and spit it out',\n",
       " 'I faced it all and I stood tall',\n",
       " 'And did it my way',\n",
       " \"I've loved, I've laughed and cried\",\n",
       " \"I've had my fails, my share of losing\",\n",
       " 'And now as tears subside',\n",
       " 'I find it all so amusing',\n",
       " 'To think I did all that',\n",
       " 'And may I say, not in a shy way',\n",
       " 'Oh, no, no not me',\n",
       " 'I did it my way',\n",
       " 'For what is a man, what has he got',\n",
       " 'If not himself, then he has not',\n",
       " 'To say the words he truly feels',\n",
       " 'And not the words from one who kneels',\n",
       " 'The record shows I took the blows',\n",
       " 'And did it my way',\n",
       " 'The record shows I took the blows',\n",
       " 'And did it my way',\n",
       " \"When you're weary, feeling small\",\n",
       " 'When tears are in your eyes I will dry them all',\n",
       " \"I'm on your side, oh, When times get rough\",\n",
       " \"And friends just can't be found\",\n",
       " 'Like a bridge over troubled water,',\n",
       " 'I will lay me down,',\n",
       " 'Oh, like a bridge over troubled water',\n",
       " 'I will lay me down',\n",
       " \"When you're down and out, when you're on the street\",\n",
       " 'When evening falls so hard, I will comfort you',\n",
       " \"I'll take your part oh! when darkness falls and pain is all around\",\n",
       " 'Yes, like a bridge over troubled water',\n",
       " 'I will lay me down',\n",
       " 'Oh, like a bridge over troubled water',\n",
       " 'I will lay me down',\n",
       " 'Sailing on, silver girl, sailing on by',\n",
       " 'Your time has come to shine',\n",
       " 'All your dreams are on their way',\n",
       " 'See how they shine',\n",
       " 'Oh, if you need a friend,',\n",
       " \"I'm sailing right behind\",\n",
       " 'Yes, like a bridge over troubled water',\n",
       " 'I, I will ease your mind',\n",
       " 'Like a bridge over troubled water',\n",
       " 'I will ease your mind',\n",
       " \"It's now or never,\",\n",
       " 'come hold me tight',\n",
       " 'Kiss me my darling,',\n",
       " 'be mine tonight',\n",
       " 'Tomorrow will be too late,',\n",
       " \"it's now or never\",\n",
       " \"My love won't wait.\",\n",
       " 'When I first saw you',\n",
       " 'with your smile so tender',\n",
       " 'My heart was captured,',\n",
       " 'my soul surrendered',\n",
       " \"I'd spend a lifetime\",\n",
       " 'waiting for the right time',\n",
       " 'Now that your near',\n",
       " 'the time is here at last.',\n",
       " \"It's now or never,\",\n",
       " 'come hold me tight',\n",
       " 'Kiss me my darling,',\n",
       " 'be mine tonight',\n",
       " 'Tomorrow will be too late,',\n",
       " \"it's now or never\",\n",
       " \"My love won't wait.\",\n",
       " 'Just like a willow,',\n",
       " 'we would cry an ocean',\n",
       " 'If we lost true love',\n",
       " 'and sweet devotion',\n",
       " 'Your lips excite me,',\n",
       " 'let your arms invite me',\n",
       " 'For who knows when',\n",
       " \"we'll meet again this way\",\n",
       " \"It's now or never,\",\n",
       " 'come hold me tight',\n",
       " 'Kiss me my darling,',\n",
       " 'be mine tonight',\n",
       " 'Tomorrow will be too late,',\n",
       " \"it's now or never\",\n",
       " \"My love won't wait.\",\n",
       " 'Oh, my love, my darling',\n",
       " \"I've hungered for your touch\",\n",
       " 'A long lonely time',\n",
       " 'And time goes by, so slowly',\n",
       " 'And time can do so much',\n",
       " 'Are you, still mine?',\n",
       " 'I need your love',\n",
       " 'I need your love',\n",
       " 'God, spread your love to me',\n",
       " 'Lonely rivers',\n",
       " 'Flow to the sea, to the sea',\n",
       " 'To the open arms of the sea',\n",
       " 'Lonely rivers cry',\n",
       " 'Wait for me, wait for me',\n",
       " \"I'm coming home\",\n",
       " 'Wait for me',\n",
       " 'My love, my darling',\n",
       " \"I've hungered for your kiss\",\n",
       " 'Oh lonely, Lonely time',\n",
       " 'I need your love',\n",
       " 'I need your love',\n",
       " 'Amazing grace, oh how sweet the sound',\n",
       " 'That saved a wreck like me',\n",
       " \"I once was lost, though now I'm found\",\n",
       " 'I was blind, but now I see',\n",
       " \"When we've been there ten thousand years\",\n",
       " 'Bright shining as the sun',\n",
       " \"We've no less days to sing God's praise\",\n",
       " 'Then when, when we first begun',\n",
       " 'Through many dangers, toils and snares,',\n",
       " 'I have already come',\n",
       " \"It's grace that brought me safe thus far,\",\n",
       " 'and grace will lead me home.',\n",
       " 'Amazing grace, oh how sweet the sound',\n",
       " 'To save a wreck like me',\n",
       " \"I once was lost, but now I'm found\",\n",
       " 'I was blind, but now I see',\n",
       " 'The warden threw a party in the county jail.',\n",
       " 'The prison band was there and they began to wail.',\n",
       " \"The band was jumpin' and the joint began to swing.\",\n",
       " \"You should've heard those knocked out jailbirds sing.\",\n",
       " \"Let's rock, everybody, let's rock.\",\n",
       " 'Everybody in the whole cell block',\n",
       " \"Was dancin' to the jailhouse rock.\",\n",
       " 'Spider murphy played the tenor saxophone,',\n",
       " \"Little joe was blowin' on the slide trombone.\",\n",
       " 'The drummer boy from illinois went crash, boom, bang,',\n",
       " 'The whole rhythm section was the purple gang.',\n",
       " \"Let's rock, everybody, let's rock.\",\n",
       " 'Everybody in the whole cell block',\n",
       " \"Was dancin' to the jailhouse rock.\",\n",
       " 'Number forty-seven said to number three:',\n",
       " '\"you\\'re the cutest jailbird I ever did see.',\n",
       " 'I sure would be delighted with your company,',\n",
       " 'Come on and do the jailhouse rock with me.\"',\n",
       " \"Let's rock, everybody, let's rock.\",\n",
       " 'Everybody in the whole cell block',\n",
       " \"Was dancin' to the jailhouse rock.\",\n",
       " \"The sad sack was a sittin' on a block of stone\",\n",
       " \"Way over in the corner weepin' all alone.\",\n",
       " 'The warden said, \"hey, buddy, don\\'t you be no square.',\n",
       " \"If you can't find a partner\",\n",
       " 'Use a wooden chair.\"',\n",
       " \"Let's rock, everybody, let's rock.\",\n",
       " 'Everybody in the whole cell block',\n",
       " \"Was dancin' to the jailhouse rock.\",\n",
       " 'Shifty henry said to bugs, \"for heaven\\'s sake,',\n",
       " 'No one\\'s lookin\\', now\\'s our chance to make a break.\"',\n",
       " 'Bugsy turned to shifty and he said, \"nix nix,',\n",
       " 'I wanna stick around a while and get my kicks.\"',\n",
       " \"Let's rock, everybody, let's rock.\",\n",
       " 'Everybody in the whole cell block',\n",
       " \"Was dancin' to the jailhouse rock.\",\n",
       " 'Lord Almighty,',\n",
       " 'I feel my temperature rising',\n",
       " 'Higher higher',\n",
       " \"It's burning through to my soul\",\n",
       " 'Girl, girl, girl, girl',\n",
       " 'You gonna set me on fire',\n",
       " 'My brain is flaming',\n",
       " \"I don't know which way to go\",\n",
       " 'Your kisses lift me higher',\n",
       " 'Like the sweet song of a choir',\n",
       " 'You light my morning sky',\n",
       " 'With burning love',\n",
       " 'Ooh, ooh, ooh, ooh,',\n",
       " 'I feel my temperature rising',\n",
       " \"Help me, I'm flaming\",\n",
       " 'I must be a hundred and nine',\n",
       " 'Burning, burning, burning',\n",
       " 'And nothing can cool me',\n",
       " 'I just might turn into smoke',\n",
       " 'But I feel fine',\n",
       " 'Cause your kisses lift me higher',\n",
       " 'Like a sweet song of a choir',\n",
       " 'And you light my morning sky',\n",
       " 'With burning love',\n",
       " \"It's coming closer\",\n",
       " 'The flames are now licking my body',\n",
       " \"Won't you help me\",\n",
       " \"I feel like I'm slipping away\",\n",
       " \"It's hard to breathe\",\n",
       " 'And my chest is a-heaving',\n",
       " 'Lord have Mercy',\n",
       " \"I'm burning a whole where I lay\",\n",
       " 'Your kisses lift me higher',\n",
       " 'Like the sweet song of a choir',\n",
       " 'You light my morning sky',\n",
       " 'With burning love',\n",
       " 'With burning love',\n",
       " 'Ah, ah, burning love',\n",
       " \"I'm just a hunk, a hunk of burning love\",\n",
       " 'Just a hunk, a hunk of burning love',\n",
       " 'Just a hunk, a hunk of burning love',\n",
       " 'Just a hunk, a hunk of burning love',\n",
       " 'Just a hunk, a hunk of burning love',\n",
       " 'Just a hunk, a hunk of burning love',\n",
       " 'Kiss me quick, while we still have this feeling',\n",
       " 'Hold me close and never let me go',\n",
       " \"'Cause tomorrows can be so uncertain\",\n",
       " 'Love can fly and leave just hurting',\n",
       " 'Kiss me quick because I love you so',\n",
       " 'Kiss me quick and make my heart go crazy',\n",
       " 'Sigh that sigh and whisper oh so low',\n",
       " 'Tell me that tonight will last forever',\n",
       " 'Say that you will leave me never',\n",
       " 'Kiss me quick because I love you so',\n",
       " 'Let the band keep playing while we are swaying',\n",
       " \"Let's keep on praying that we'll never stop\",\n",
       " \"Kiss me quick I just can't stand this waiting\",\n",
       " \"'Cause your lips are lips I long to know\",\n",
       " \"For that kiss will open heaven's door\",\n",
       " \"And we'll stay there forever more\",\n",
       " 'Kiss me quick because I love you so',\n",
       " 'You know I can be found,',\n",
       " 'sitting home all alone,',\n",
       " \"If you can't come around,\",\n",
       " 'at least please telephone.',\n",
       " \"Don't be cruel to a heart that's true.\",\n",
       " 'Baby, if I made you mad',\n",
       " 'for something I might have said,',\n",
       " \"Please, let's forget the past,\",\n",
       " 'the future looks bright ahead,',\n",
       " \"Don't be cruel to a heart that's true.\",\n",
       " \"I don't want no other love,\",\n",
       " \"Baby it's just you I'm thinking of.\",\n",
       " \"Don't stop thinking of me,\",\n",
       " \"don't make me feel this way,\",\n",
       " 'Come on over here and love me,',\n",
       " 'you know what I want you to say.',\n",
       " \"Don't be cruel to a heart that's true.\",\n",
       " 'Why should we be apart?',\n",
       " 'I really love you baby, cross my heart.',\n",
       " \"Let's walk up to the preacher\",\n",
       " 'and let us say I do,',\n",
       " \"Then you'll know you'll have me,\",\n",
       " \"and I'll know that I'll have you,\",\n",
       " \"Don't be cruel to a heart that's true.\",\n",
       " \"I don't want no other love,\",\n",
       " \"Baby it's just you I'm thinking of.\",\n",
       " \"Don't be cruel to a heart that's true.\",\n",
       " \"Don't be cruel to a heart that's true.\",\n",
       " \"I don't want no other love,\",\n",
       " \"Baby it's just you I'm thinking of.\",\n",
       " 'You never close your eyes',\n",
       " 'Anymore when I kiss your lips',\n",
       " \"There's no tenderness\",\n",
       " 'Like before in you fingertips',\n",
       " \"You're trying hard not to show it\",\n",
       " 'But baby, baby I know it',\n",
       " \"You've lost that lovin' feelin',\",\n",
       " \"oh that lovin' feeelin'\",\n",
       " \"You've lost that lovin' feelin'\",\n",
       " \"now it's gone, gone, gone\",\n",
       " \"There's no tenderness in your eyes\",\n",
       " 'When I reach out for you',\n",
       " \"Girl, you're starting to criticize\",\n",
       " 'Every little thing I do',\n",
       " 'It make me just feel like crying',\n",
       " \"'Cause baby, something beautiful's dying\",\n",
       " \"You've lost that lovin' feelin',\",\n",
       " \"oh that lovin' feelin'\",\n",
       " \"You've lost that lovin' feelin'\",\n",
       " \"now it's gone, gone, gone\",\n",
       " 'Baby, baby, I get down on my knees for you',\n",
       " 'If you would only love me like you used to do',\n",
       " 'We had a love, love, love,',\n",
       " \"A love you don't find every day\",\n",
       " \"Oh don't, don't, don't, don't take it away\",\n",
       " \"Listen to me, talkin' to you\",\n",
       " \"Bring back that lovin' feelin,\",\n",
       " \"oh, that lovin' feelin'\",\n",
       " \"Bring back that lovin' feelin,\",\n",
       " \"now it's gone, gone, gone\",\n",
       " \"And I can't go on, woh ... woh ... woh\"]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  220,   460,   470,  1037,    13, 50256]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation_output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
