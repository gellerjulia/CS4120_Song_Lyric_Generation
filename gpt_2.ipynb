{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine Tuning GPT-2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "from gpt2_utils import Dset \n",
    "from gpt2_utils import get_model_tokenizer, train_model, generate_texts, compute_perplexity, load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set notebook variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants \n",
    "MAX_SEQ_LEN = 10\n",
    "DEVICE = 'cpu'\n",
    "VERBOSE = True\n",
    "\n",
    "GENRE = 'country'\n",
    "\n",
    "# Name of this trained model, will be used for filename when saving the model\n",
    "MODEL_INSTANCE_NAME = 'foo'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in train, vallidation, and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in cleaned data\n",
    "if GENRE == 'country':\n",
    "    train_lines = pd.read_csv('data/country_train.csv', header=None).values.tolist()\n",
    "    val_lines = pd.read_csv('data/country_val.csv', header=None).values.tolist()\n",
    "    test_lines = pd.read_csv('data/country_test.csv', header=None).values.tolist()\n",
    "\n",
    "elif GENRE == 'metal':\n",
    "    train_lines = pd.read_csv('data/metal_train.csv', header=None).values.tolist()\n",
    "    val_lines = pd.read_csv('data/metal_val.csv', header=None).values.tolist()\n",
    "    test_lines = pd.read_csv('data/metal_test.csv', header=None).values.tolist()\n",
    "\n",
    "else:\n",
    "    raise ValueError('Incorrect genre given.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train lines : 149771\n",
      "val lines :  18610\n",
      "test lines :  19108\n"
     ]
    }
   ],
   "source": [
    "print('train lines :', len(train_lines))\n",
    "print('val lines : ', len(val_lines))\n",
    "print('test lines : ', len(test_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_end = math.ceil(len(train_lines)/4)\n",
    "train_lines = train_lines[0:train_end]\n",
    "\n",
    "#val_end = math.ceil(len(val_lines)/4)\n",
    "#val_lines = val_lines[0:val_end]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine Tuning GPT-2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All TF 2.0 model weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "Some weights of GPT2LMHeadModel were not initialized from the TF 2.0 model and are newly initialized: ['lm_head.weight', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# get model and tokenizer\n",
    "model, tokenizer = get_model_tokenizer(MAX_SEQ_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode data\n",
    "train_encodings = [tokenizer(text=x, return_tensors='tf', padding='max_length', max_length=MAX_SEQ_LEN, truncation=True) for x in train_lines]\n",
    "train_encodings = [enc['input_ids'].numpy().tolist()[0] for enc in train_encodings]\n",
    "\n",
    "val_encodings = [tokenizer(text=x, return_tensors='tf', padding='max_length', max_length=MAX_SEQ_LEN, truncation=True) for x in val_lines]\n",
    "val_encodings = [enc['input_ids'].numpy().tolist()[0] for enc in val_encodings]\n",
    "\n",
    "test_encodings = [tokenizer(text=x, return_tensors='tf', padding='max_length', max_length=MAX_SEQ_LEN, truncation=True) for x in test_lines]\n",
    "test_encodings = [enc['input_ids'].numpy().tolist()[0] for enc in test_encodings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training, valdation, and testing datasets\n",
    "dset_train = Dset(train_encodings)\n",
    "dset_val = Dset(val_encodings)\n",
    "dset_test = Dset(test_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f45a1ca323746e491cb340aebe762c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/375 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86a6f69f40564ce7ba5b1123dbdb1458",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.997827529907227, 'eval_runtime': 72.765, 'eval_samples_per_second': 63.946, 'eval_steps_per_second': 0.646, 'epoch': 1.0}\n",
      "{'train_runtime': 1491.3113, 'train_samples_per_second': 25.107, 'train_steps_per_second': 0.251, 'train_loss': 4.483689453125, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# fine tune the model\n",
    "model = train_model(model, dset_train, dset_val, GENRE, MODEL_INSTANCE_NAME, batches=100, epochs=1, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shaem\\anaconda3\\Lib\\site-packages\\transformers\\generation\\utils.py:1473: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and we've gonna see\n",
      " it's in one\n",
      " like a little woman\n",
      " me than we're walking over my lips\n",
      " but all in heaven you're walking in the\n",
      " to lose me more i've been good on\n",
      " what you're all the sun\n",
      " the little to make your life.\n",
      " to buy you\n",
      " in fire like you can me\n",
      ", just where they might make you hold me\n",
      " for you? not a few more i've\n",
      "\n",
      " in town and just all in home\n",
      " in the door\n"
     ]
    }
   ],
   "source": [
    "# generate lyrics\n",
    "gen_texts = generate_texts(model, tokenizer, 15)\n",
    "for text in gen_texts:\n",
    "    print(''.join(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " you been just a different\n",
      "\n",
      ", the man that i could get back\n",
      " to make it together, take a lot of\n",
      " us the things that said, we're right\n",
      ": the music\n",
      " just where the way in the day\n",
      ", the love is gone\n",
      " the angels can make a big wind\n",
      "\n",
      ", not,\n",
      ", and things about the whole sound of day\n",
      " in a hobo\n",
      " were we were one\n",
      "\n",
      "'s up\n",
      " the love are the things we're a shame\n",
      " the greatest folks the party is one and some\n",
      " all the moon and all he loves me\n",
      "'s gone on the wall\n",
      " to lose her\n",
      " and friends so hard i'm in my arms\n",
      " by\n",
      " to me too lonely\n",
      " him to be in my hands\n",
      " the suns, they're a good girl\n",
      " about us to be free\n",
      " to be a-home song,\n",
      "\n",
      " and it still takes your eyes\n",
      "\n",
      "\n",
      ", if she'll be the same\n",
      " for once\n",
      "\n",
      ", it out the storm.\n",
      " for a little and his song\n",
      " your lips\n",
      " the rest, for a perfect man\n",
      "\n",
      " my pride\n",
      " in our pride out for\n",
      "\n",
      " are all the way we go together\n",
      "\n",
      "\n",
      ", as much happy day in his life\n",
      " with people to love you\n",
      "s just just been the whole thing\n",
      ", you still love them, i was a\n",
      " and the people ever left to me\n",
      " is another thing in a long way of me\n",
      "\n",
      " in the old men he's gonna see\n",
      " all my memories as i do is wrong than\n",
      " a man is just a man that took to\n",
      " are a chance to take a hand\n",
      " me we don't get a little bit of\n",
      "\n",
      " in the mountain\n",
      "s and mighty?\n",
      " not an' old cheard\n",
      "\n",
      " for you've seen her\n",
      " me a little, a dream in the sky\n",
      "\n",
      " as i'll be so better when i see\n",
      "\n",
      "\n",
      " from the way that i've gone, i\n",
      " just two, the same\n",
      ", so fine\n",
      " you were happy\n",
      " me like i've gone.\n",
      "in' my love i really know who i\n",
      " for him,\n",
      "\n",
      " about that\n",
      " the stars of me\n",
      " a time for somebody old man\n",
      " in the rain, we're over me in\n",
      "\n",
      ", no one but this, this is one\n",
      " on the end of the ground\n",
      " just like a pair of dreams are over\n",
      "\n",
      "in' i'll be your\n",
      " to make a friend\n",
      " in that life like a small tree\n",
      ", i'll not know how\n",
      " to lose the wind\n",
      " on me out to your way\n",
      ", when our minds dies.\n",
      " the way i would leave my home\n",
      "\n",
      " on the night of a small tree\n",
      " him all the love we'll burn\n"
     ]
    }
   ],
   "source": [
    "loaded_model = load_model(\"gpt2_trained_models/country/gpt2_final_model_epoch1_half\")\n",
    "#generate_texts(model: transformers.GPT2LMHeadModel, tokenizer: transformers.GPT2Tokenizer, n_texts: int, file_path=None) -> List[List[str]]:\n",
    "gen_texts = generate_texts(loaded_model, tokenizer, 100, \"generated_txts/gpt2_country.txt\")\n",
    "for text in gen_texts:\n",
    "    print(''.join(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (196059 > 1024). Running this sequence through the model will result in indexing errors\n",
      "  7%|â–‹         | 1355/19606 [06:10<1:31:39,  3.32it/s]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "test_data = np.array(test_lines).flatten().tolist()\n",
    "model = loaded_model\n",
    "\n",
    "# being func\n",
    "encodings = tokenizer(\"\\n\\n\".join(test_data), return_tensors=\"tf\")\n",
    "\n",
    "max_length = 10\n",
    "stride=10\n",
    "seq_len = encodings.input_ids.shape[1]\n",
    "\n",
    "nlls = []\n",
    "prev_end_loc = 0\n",
    "for begin_loc in tqdm(range(0, seq_len, stride)):\n",
    "    end_loc = min(begin_loc + max_length, seq_len)\n",
    "    trg_len = end_loc - prev_end_loc  # may be different from stride on last loop\n",
    "    input_ids = encodings.input_ids[:, begin_loc:end_loc]\n",
    "    target_ids_np = tf.identity(input_ids).numpy()\n",
    "    target_ids_np[:, :-trg_len] = -100 \n",
    "    target_ids = tf.convert_to_tensor(np.array(target_ids_np))\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=target_ids)\n",
    "\n",
    "        # loss is calculated using CrossEntropyLoss which averages over valid labels\n",
    "        # N.B. the model only calculates loss over trg_len - 1 labels, because it internally shifts the labels\n",
    "        # to the left by 1.\n",
    "        neg_log_likelihood = outputs.loss\n",
    "\n",
    "    nlls.append(neg_log_likelihood)\n",
    "\n",
    "    prev_end_loc = end_loc\n",
    "    if end_loc == seq_len:\n",
    "        break\n",
    "\n",
    "ppl = math.exp(np.mean(nlls))\n",
    "\n",
    "ppl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 10), dtype=int32, numpy=\n",
       "array([[40252, 11752,  5156,  1560,   262,   582,   379,   262,  7846,\n",
       "         1302]])>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids[:, [-1,0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute perplexity of generated lyrics\n",
    "import numpy as np\n",
    "#test_lines_first = test_lines[:math.ceil(len(test_lines)/4)]\n",
    "test_lines_flt = np.array(test_lines).flatten().tolist()\n",
    "ppl = compute_perplexity(loaded_model, tokenizer, test_lines_flt, MAX_SEQ_LEN, DEVICE)\n",
    "ppl"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
